---
title: "Maximum likelihood"
author: "Robin Elahi"
subtitle: "Topics in Scientific and Statistical Computing"
format: revealjs
embed-resources: true
filters:
  - openlinksinnewpage
---

```{r setup, include=FALSE}
# Change general options
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, 
                      message = FALSE, warning = FALSE, 
                      cache = FALSE, par = FALSE)

# Set up figure options
knitr::opts_chunk$set(fig.align = 'center', fig.show = 'asis', 
                      fig.width = 4, fig.height = 3)

# Set the web address where R will look for files from this repository
repo_url <- "https://raw.githubusercontent.com/elahi/qk2e/master/data"

# Define the directory where images generated by knit will be saved
# knitr::opts_chunk$set(fig.path = "/images/", 
#                       cache.path = "/cache/")

# Packages
library(tidyverse)
library(knitr)
library(broom)
theme_set(theme_bw(base_size = 12) + theme(panel.grid = element_blank()))

# Set working directory
knitr::opts_knit$set(root.dir = "../")
```

## What is likelihood?

\
\

. . .

Likelihood: (relative) measure of how well our observations fit a specific hypothesis / model

## Probability distribution functions vs likelihood functions

-   parameters
-   data
-   summation / integration of probabilities

## Probability distribution function

**known parameter, variable data**

. . .

We **know** that the oceans cover 70% of the earth's surface. If we were to take 10 random GPS coordinates of the earth, what's the probability of getting 5 ocean points?

```{r}
p <- 0.7
```

What probability distribution should we use? Which function in R?

. . .

```{r}
dbinom(x = 5, size = 10, p = p)
```

## 

If we repeat this calculation for every possible outcome of ocean points (out of 10), we can visualize the probability distribution, for a *known* parameter ($p$).

```{r}
ocean_grid <- seq(0, 10, by = 1)
ocean_p <- dbinom(x = ocean_grid, size = 10, p = p)
```

. . .

If we sum the `ocean_p` vector, what value should we get?\
Reminder: this is a probability mass function.

. . .

```{r}
sum(ocean_p)
```

## 

```{r}
#| echo: false
plot(ocean_grid, ocean_p, type = "b", 
     xlab = "# of ocean points (out of 10)",
     ylab = "Pr(x | p = 0.7)", main = "Binomial PMF, p = 0.7")
```

## Likelihood function

**unknown parameter, fixed data**

(the usual case)

. . .

Imagine you are martian observing Earth for the first time and you want to estimate the proportion of ocean on the planet using a bunch of random landings with your space probe.

```{r}
#| echo: false
set.seed(11)
d <- rbinom(n = 1, size = 20, prob = 0.55)
```

Out of 20 trials, you made `r d` ocean landings.

## 

What is $\text{Pr}(p = 0.7 | \text{ocean} = 12, \text{trials} = 20)$?

. . .

```{r}
dbinom(x = 12, size = 20, prob = 0.7)
```

. . .

   

Repeat for many values of $p$:

```{r}
#| output: false
p_grid <- seq(0, 1, by = 0.01)
p_lik <- dbinom(x = 12, size = 20, prob = p_grid)
plot(p_grid, p_lik, type = "b", 
     xlab = "Proportion of ocean (hypothesis)", 
     ylab = "Pr(data | hypothesis)", 
     main = "Likelihood function")
```

## 

```{r}
#| echo: false
plot(p_grid, p_lik, type = "b", 
     xlab = "Proportion of ocean (hypothesis)", 
     ylab = "Pr(data | hypothesis)", 
     main = "Likelihood function")
```

Will these probabilities sum to 1? 

. . .

```{r}
sum(p_lik)
```

## 

Which value of $p$ corresponds to the maximum likelihood?

```{r}
df <- tibble(p_grid, p_lik)
summary(df)
df[which.max(df$p_lik), ]
```

## Maximum likelihood

Following slides are based on a [lab](https://github.com/mcgoodman/quant-eco-course/tree/master/Part%201%20-%20Maximum%20Likelihood/lab%201%20-%20linear%20regression) by Maurice Goodman.

## Outline

-   simulate data from a linear model using a known intercept and slope
-   **write a function** to fit a linear regression using **maximum-likelihood estimation**,
-   compare estimated intercept and slope compare to the known values

## Linear regression

$$
\begin{aligned}
Y &= \alpha + \beta X + \epsilon \\
\epsilon &\sim \text{Normal}(0, \sigma^2)
\end{aligned}
$$

$$
\begin{aligned}
 Y &\sim \text{Normal}(\mu, \sigma^2) \\
 \mu &= \alpha + \beta X
\end{aligned}
$$

**Three** parameters:

-   $\alpha$, the intercept
-   $\beta$, the slope
-   $\sigma$, the standard deviation of the residuals

## Simulate data

```{r}
a <- 1 ## intercept
b <- 2 ## slope
sigma <- 1 ## error standard deviation
n <- 50 ## sample size
## simulate data
set.seed(13)
sim_data <- data.frame(x = runif(n, min = -3, max = 3))
sim_data$y <- rnorm(n, mean = a + b*sim_data$x, sd = sigma)
## Look at the data structure
head(sim_data)
```

## 

```{r}
#| echo: false
sim_data %>% # start with the data
  ggplot(aes(x, y)) + # tell ggplot which aesthetics to use
  geom_point() + # add points
  geom_abline(intercept = a, slope = b) # show the known regression line
```

## 

**Likelihood function**:

$$
\mathcal{L}(\theta) = f_X(x|\theta) = \prod_{i = 1}^n f_X(x_i )
$$

. . . 

**Log-likelihood function**:

$$
l(\theta) = \ln (\mathcal{L}(\theta)) = \sum_{i = 1}^n \ln [f_X(x_i)]
$$

We want to maximize the log-likelihood of this function, which is equivalent to **minimizing the negative log-likelihood**.

## 

$$
\begin{aligned}
Y &= \alpha + \beta X + \epsilon \\
\epsilon &\sim \text{Normal}(0, \sigma^2)
\end{aligned}
$$

becomes:

$$
Y \sim \text{Normal}(\underbrace{\alpha + \beta X}_{\text{mean}}, \underbrace{\sigma^2}_{\text{variance}})
$$

We can express the negative log-likelihood function in R as:

`-sum(dnorm(y, mean = ..., sd = ..., log = TRUE))`

## R function

```{r lm_nll}
## negative log-likelihood of y-values given 
## intercept, slope, and error sd
## we pass the parameters as a vector in the order a, b, sigma
lm_nll <- function(par, x, y) {
  a <- par["a"]; b <- par["b"]; sigma <- exp(par["log_sigma"])
  -sum(dnorm(y, mean = a + b*x, sd = sigma, log = TRUE))
}
```

## `optim`ize the function

```{r MLE}
fit <- optim(
  par = c(a = 0, b = 0, log_sigma = 1), 
  fn = lm_nll, 
  x = sim_data$x, 
  y = sim_data$y, 
  hessian = TRUE
)
```

## 

```{r}
fit
# Compare with lm
coef(lm(y ~ x, data = sim_data))
```

## 

```{r plot_fit}
sim_data %>% ggplot(aes(x, y)) + geom_point() + 
  geom_abline(intercept = a, slope = b, 
              color = "red") + # truth
  geom_abline(intercept = fit$par[1], slope = fit$par[2], 
              color = "blue") # fit
```

## Likelihood slice

```{r likelihood_slice}
a_seq <- seq(-2, 4, length.out = 100)
b_seq <- seq(-1, 5, length.out = 100)

### data frame containing all combinations of the intercept and slope
par_grid <- expand.grid("a" = a_seq, "b" = b_seq)

### loop over data frame, store negative log likelihood
for (i in 1:nrow(par_grid)) {
  par_grid$loglik[i] <- lm_nll(
    c(a = par_grid$a[i], b = par_grid$b[i], fit$par[3]), 
    x = sim_data$x, 
    y = sim_data$y
  )
}
```

## 

```{r contour_plot}
#| echo: false
#| fig-height: 4
#| fig-width: 6
par_grid %>% 
  ggplot(aes(a, b, z = loglik)) + 
  geom_contour_filled() + 
  labs(x = expression(alpha), y = expression(beta), fill = "negative log-likelihood") + 
  geom_point(aes(x = fit$par[1], y = fit$par[2]), size = 2, color = "white")
```

## Likelihood profile for the slope

Modify our `lm_log_lik` function so that the slope is a separate argument.

```{r slope_profile}
slope_proflik <- function(pars, b, x, y) {
  a <- pars["a"]; sigma <- exp(pars["log_sigma"]) ## only two pars in this function
  -sum(dnorm(y, mean = a + b*x, sd = sigma, log = TRUE))
}

b_profile <- data.frame(
  b = seq(-1, 5, length.out = 100)  ## values of slope to loop over
)
```

## 

We'll pass this to `optim` to just find the values of the intercept and sd

```{r}
## loop over values of the slope, store negative log-likelihood
for (i in 1:nrow(b_profile)) {
  b_fit <- optim(
    par = c(a = 0, log_sigma = log(1)), # initial values for intercept and sd
    fn = slope_proflik,  # objective function
    b = b_profile$b[i], # fixed slope value
    x = sim_data$x,   # x values
    y = sim_data$y   # y values
  )
  b_profile$nll[i] <- b_fit$value
}
```

## 

```{r}
## plot slope profile
b_profile %>% 
  ggplot(aes(b, nll)) + 
  geom_line() + 
  labs(x = expression(beta), y = "negative log-likelihood")
```

## Standard errors: Fisher information

$$
\begin{aligned}
\mathcal{J}(\hat{\theta}) &= - \frac{\partial^2}{\partial \theta^2} \ln [\mathcal{L}(\hat{\theta})] \\
\text{Var}(\hat{\theta}) &= 1 / \mathcal{J}(\hat{\theta})
\end{aligned}
$$

## 

```{r fisher_var}
var_cov <- solve(fit$hessian)
var_cov
```

The variances are on the diagonal, and we can get the standard errors by taking their square root:

## 

```{r fisher_se}
fisher_se <- sqrt(diag(var_cov))
fisher_se
```

\

Let's compare these to the standard errors reported by `lm`:

```{r lm_se}
#| echo: false
kable(tidy(summary(lm(y ~ x, data = sim_data))))
```

## Confidence intervals: normal approximation

$$
\text{CI} = (\hat{\theta} - \phi^{-1}(\gamma / 2) \times \text{SE}(\hat{\theta}), \; \hat{\theta} + \phi^{-1}(1 - \gamma / 2) \times \text{SE}(\hat{\theta}))
$$

Where $\phi^{-1}$ is the inverse cumulative distribution function of the standard Normal distribution.

## 

Equivalently:

$$
\text{CI} = (\phi^{-1}_{\hat{\theta}, \text{SE}(\theta)}(\gamma / 2), \; \phi^{-1}_{\hat{\theta}, \text{SE}(\theta)} (1-\gamma/2))
$$

Where $\phi^{-1}_{\hat{\theta}, \text{SE}(\theta)}$ is the inverse cumulative distribution function for a Normal distribution with mean $\hat{\theta}$ and standard deviation $\text{SE}(\theta)$.

## 

The Normal inverse CDF is returned by the `qnorm` function. We can obtain a confidence interval for our intercept $\alpha$ and slope $\beta$:

```{r normal_CI_fisher}
gamma <- 0.05 ## 95% confidence interval
alpha_CI <- qnorm(c(gamma/2, 1 - gamma/2), mean = fit$par[1], 
                  sd = fisher_se[1])
beta_CI <- qnorm(c(gamma/2, 1 - gamma/2), mean = fit$par[2], 
                 sd = fisher_se[2])
cbind(alpha_CI, beta_CI)
```

## Making it a function

```{r lm_function}
lm_fit <- function(x, y, SE = "fisher", init = c("a" = 0, "b" = 0, "log_sigma" = 0), n_boot = 1000) {

  if (!(SE %in% c("bootstrap", "fisher"))) stop("SE options are bootstrap or fisher")
  if (length(x) != length(y)) stop("x and y lengths differ")

  ## Maximum-likelihood estimate
  MLE <- optim(
    par = init, # initial values
    fn = lm_nll,  # our negative log-likelihood function
    x = x,   # x values
    y = y,   # y values
    hessian = (SE == "fisher") # only return Hessian if Fisher information used
  )

  ## Standard error using either fisher information or bootstrapping
  if (SE == "fisher") {
    var_cov <- solve(MLE$hessian)
    fit_se <- sqrt(diag(var_cov))
  } else {
    n_obs <- length(x) ## number of observations

    ## initialize an empty matrix to store parameter estimates for each sample
    par_samples <- matrix(nrow = n_boot, ncol = 3)

    ## recalculate MLE for each sample
    for (i in 1:n_boot) {
      samp <- sample(1:n_obs, size = n_obs, replace = TRUE) # sampled observations
      new_x <- x[samp] # subset x to sampled observations
      new_y <- y[samp] # subset y to sampled observations

      sample_fit <- optim(
        par = init, # initial values
        fn = lm_nll, # log likelihood function
        x = new_x, # sampled x-values
        y = new_y # sampled y-values
      )
  
    par_samples[i,] <- sample_fit$par # store sample parameters
  }
  fit_se <- apply(par_samples, MARGIN = 2, FUN = sd) # calculate column SDs
 }

 ## nicely format output
 data.frame(
   coef = c("intercept", "slope"), 
   estimate = MLE$par[1:2], 
   SE = fit_se[1:2]
 )

}
```

## Lab exercise 1

```{r}
a <- 1 ## intercept
b <- 2 ## slope
sigma <- 1 ## error standard deviation
n <- 50 ## sample size

## simulate data
sim_data <- tibble(
  x = runif(n, min = -3, max = 3), ## x values
  y = rnorm(n, mean = a + b*x, sd = sigma) ## regression equation
)

## plot data
sim_data %>% ggplot(aes(x, y)) + geom_point() + geom_smooth(method = "lm")

## run regression
lm_fit(sim_data$x, sim_data$y, SE = "fisher")
```
