[
  {
    "objectID": "slides/slides.html",
    "href": "slides/slides.html",
    "title": "Slides",
    "section": "",
    "text": "Week 1: Intro to Quinn and Keough, 2nd edition"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "qk2e",
    "section": "",
    "text": "This website is a space for working through Quinn and Keough’s 2nd edition textbook on data analysis for biologists. The Chapter notes are intended for review and discussion in a seminar (Oceans 200). The Slides are intended as a short primer on the topic by the discussion leader. The Examples are reproduced and lightly modified from the original worked examples.\nOceans 200 schedule\n\n\n\n\n\n\n\n\n\n\n\nWeek\nDate\nTopic\nQK chapter\nPre-class box example\nIn-class box example\n\n\n\n\n1\nJan. 17th 2023\nIntro to seminar and review of intro stats\n2\n2.2\n\n\n\n2\nJan. 10th 2023\nSampling and experimental design\n3\n\n\n\n\n3\nJan. 24th 2023\nOne-way designs\n6\n6.7\n6.11\n\n\n4\nJan. 31st 2023\nFactorial designs\n7\n7.1\n7.3\n\n\n5\nFeb. 7th 2023\nNested designs\n10\n10.6\n10.7\n\n\n6\nFeb 14th 2023\nSplit-plot designs\n11\n11.1\n11.2\n\n\n7\nFeb. 21st 2023\nRepeated measures designs\n12\n12.1\n12.2\n\n\n8\nFeb. 28th 2023\nStudents’ choice / presentations\ntbd\n\n\n\n\n9\nMar. 6th 2023\nStudents’ choice / presentations\ntbd\n\n\n\n\n10\nMar. 13th 2023\nStudents’ choice / presentations\ntbd"
  },
  {
    "objectID": "examples/lowbayes.html",
    "href": "examples/lowbayes.html",
    "title": "QK Box 2.5",
    "section": "",
    "text": "This box continues with the Low et al. example starting in Box 2.2\n\nPreliminaries\nUse rstanarm and BayesFactor packages; also needs bayestestR.\nAdd bayesplot for control over plot\nLoad graphics packages (if ggplot version of figures wanted)\nNote that iso is reference group so diff between means is -ve\n\n\nUninformative priors\n\nlow &lt;- read_csv(\"../data/lowco2.csv\")\nlow1 &lt;- stan_glm(co2~anesth,family = gaussian(link = \"identity\"),data=low)\nposteriors1 &lt;- describe_posterior(low1)\n\n\nprint_md(posteriors1, digits = 2)\n\n\nSummary of Posterior Distribution\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nMedian\n95% CI\npd\nROPE\n% in ROPE\nRhat\nESS\n\n\n\n\n(Intercept)\n70.89\n[ 61.17, 81.00]\n100%\n[-1.91, 1.91]\n0%\n1.000\n3887.00\n\n\nanesthiso\n-20.83\n[-34.65, -7.01]\n99.72%\n[-1.91, 1.91]\n0%\n1.000\n3795.00\n\n\n\n\n# plot posterior distribution for all three parameters (intercept, mean diff, sigma)\nplot(low1,plotfun=\"mcmc_hist\")\n## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n# get Bayes factor for mean diff\nlowx &lt;- as.data.frame(low)\nlmBF(co2~anesth, data=lowx,posterior=FALSE)\n## Bayes factor analysis\n## --------------\n## [1] anesth : 8.039109 ±0%\n## \n## Against denominator:\n##   Intercept only \n## ---\n## Bayes factor type: BFlinearModel, JZS\n\n\n\nInformative priors\nRun three options, mean difference with high and low precision, and a bigger mean difference with high precision\n\n#for mean difference with high precision\nlow2 &lt;- stan_glm(co2~anesth,family = gaussian(link = \"identity\"),prior=normal(-25,5),data=low)\n## \n## SAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\n## Chain 1: \n## Chain 1: Gradient evaluation took 1e-05 seconds\n## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.1 seconds.\n## Chain 1: Adjust your expectations accordingly!\n## Chain 1: \n## Chain 1: \n## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\n## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\n## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\n## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\n## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\n## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\n## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\n## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\n## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\n## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\n## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\n## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\n## Chain 1: \n## Chain 1:  Elapsed Time: 0.015 seconds (Warm-up)\n## Chain 1:                0.013 seconds (Sampling)\n## Chain 1:                0.028 seconds (Total)\n## Chain 1: \n## \n## SAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\n## Chain 2: \n## Chain 2: Gradient evaluation took 3e-06 seconds\n## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds.\n## Chain 2: Adjust your expectations accordingly!\n## Chain 2: \n## Chain 2: \n## Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\n## Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\n## Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\n## Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\n## Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\n## Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\n## Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\n## Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\n## Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\n## Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\n## Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\n## Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\n## Chain 2: \n## Chain 2:  Elapsed Time: 0.016 seconds (Warm-up)\n## Chain 2:                0.013 seconds (Sampling)\n## Chain 2:                0.029 seconds (Total)\n## Chain 2: \n## \n## SAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\n## Chain 3: \n## Chain 3: Gradient evaluation took 2e-06 seconds\n## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds.\n## Chain 3: Adjust your expectations accordingly!\n## Chain 3: \n## Chain 3: \n## Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\n## Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\n## Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\n## Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\n## Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\n## Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\n## Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\n## Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\n## Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\n## Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\n## Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\n## Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\n## Chain 3: \n## Chain 3:  Elapsed Time: 0.019 seconds (Warm-up)\n## Chain 3:                0.012 seconds (Sampling)\n## Chain 3:                0.031 seconds (Total)\n## Chain 3: \n## \n## SAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\n## Chain 4: \n## Chain 4: Gradient evaluation took 2e-06 seconds\n## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds.\n## Chain 4: Adjust your expectations accordingly!\n## Chain 4: \n## Chain 4: \n## Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\n## Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\n## Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\n## Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\n## Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\n## Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\n## Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\n## Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\n## Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\n## Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\n## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\n## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\n## Chain 4: \n## Chain 4:  Elapsed Time: 0.015 seconds (Warm-up)\n## Chain 4:                0.012 seconds (Sampling)\n## Chain 4:                0.027 seconds (Total)\n## Chain 4:\nposteriors2 &lt;- describe_posterior(low2)\nprint_md(posteriors2, digits = 2)\n\n\nSummary of Posterior Distribution\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nMedian\n95% CI\npd\nROPE\n% in ROPE\nRhat\nESS\n\n\n\n\n(Intercept)\n72.30\n[ 64.37, 80.50]\n100%\n[-1.91, 1.91]\n0%\n1.002\n3239.00\n\n\nanesthiso\n-23.53\n[-31.62, -15.78]\n100%\n[-1.91, 1.91]\n0%\n1.000\n3925.00\n\n\n\n\n# informative prior for mean difference with low precision\nlow3 &lt;- stan_glm(co2~anesth,family = gaussian(link = \"identity\"),prior=normal(-25,20),data=low)\n## \n## SAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\n## Chain 1: \n## Chain 1: Gradient evaluation took 8e-06 seconds\n## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds.\n## Chain 1: Adjust your expectations accordingly!\n## Chain 1: \n## Chain 1: \n## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\n## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\n## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\n## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\n## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\n## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\n## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\n## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\n## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\n## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\n## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\n## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\n## Chain 1: \n## Chain 1:  Elapsed Time: 0.013 seconds (Warm-up)\n## Chain 1:                0.012 seconds (Sampling)\n## Chain 1:                0.025 seconds (Total)\n## Chain 1: \n## \n## SAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\n## Chain 2: \n## Chain 2: Gradient evaluation took 3e-06 seconds\n## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds.\n## Chain 2: Adjust your expectations accordingly!\n## Chain 2: \n## Chain 2: \n## Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\n## Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\n## Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\n## Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\n## Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\n## Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\n## Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\n## Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\n## Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\n## Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\n## Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\n## Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\n## Chain 2: \n## Chain 2:  Elapsed Time: 0.014 seconds (Warm-up)\n## Chain 2:                0.012 seconds (Sampling)\n## Chain 2:                0.026 seconds (Total)\n## Chain 2: \n## \n## SAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\n## Chain 3: \n## Chain 3: Gradient evaluation took 2e-06 seconds\n## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds.\n## Chain 3: Adjust your expectations accordingly!\n## Chain 3: \n## Chain 3: \n## Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\n## Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\n## Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\n## Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\n## Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\n## Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\n## Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\n## Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\n## Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\n## Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\n## Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\n## Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\n## Chain 3: \n## Chain 3:  Elapsed Time: 0.013 seconds (Warm-up)\n## Chain 3:                0.012 seconds (Sampling)\n## Chain 3:                0.025 seconds (Total)\n## Chain 3: \n## \n## SAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\n## Chain 4: \n## Chain 4: Gradient evaluation took 2e-06 seconds\n## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds.\n## Chain 4: Adjust your expectations accordingly!\n## Chain 4: \n## Chain 4: \n## Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\n## Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\n## Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\n## Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\n## Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\n## Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\n## Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\n## Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\n## Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\n## Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\n## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\n## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\n## Chain 4: \n## Chain 4:  Elapsed Time: 0.014 seconds (Warm-up)\n## Chain 4:                0.014 seconds (Sampling)\n## Chain 4:                0.028 seconds (Total)\n## Chain 4:\nposteriors3 &lt;- describe_posterior(low3)\nprint_md(posteriors3, digits = 2)\n\n\nSummary of Posterior Distribution\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nMedian\n95% CI\npd\nROPE\n% in ROPE\nRhat\nESS\n\n\n\n\n(Intercept)\n71.22\n[ 61.15, 80.74]\n100%\n[-1.91, 1.91]\n0%\n1.001\n2991.00\n\n\nanesthiso\n-21.26\n[-34.56, -8.84]\n99.98%\n[-1.91, 1.91]\n0%\n1.000\n3137.00\n\n\n\n\n# informative prior for bigger mean difference with high precision\nlow4 &lt;- stan_glm(co2~anesth,family = gaussian(link = \"identity\"),prior=normal(-50,5),data=low)\n## \n## SAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\n## Chain 1: \n## Chain 1: Gradient evaluation took 9e-06 seconds\n## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.09 seconds.\n## Chain 1: Adjust your expectations accordingly!\n## Chain 1: \n## Chain 1: \n## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\n## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\n## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\n## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\n## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\n## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\n## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\n## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\n## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\n## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\n## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\n## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\n## Chain 1: \n## Chain 1:  Elapsed Time: 0.017 seconds (Warm-up)\n## Chain 1:                0.014 seconds (Sampling)\n## Chain 1:                0.031 seconds (Total)\n## Chain 1: \n## \n## SAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\n## Chain 2: \n## Chain 2: Gradient evaluation took 3e-06 seconds\n## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds.\n## Chain 2: Adjust your expectations accordingly!\n## Chain 2: \n## Chain 2: \n## Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\n## Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\n## Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\n## Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\n## Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\n## Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\n## Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\n## Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\n## Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\n## Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\n## Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\n## Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\n## Chain 2: \n## Chain 2:  Elapsed Time: 0.018 seconds (Warm-up)\n## Chain 2:                0.012 seconds (Sampling)\n## Chain 2:                0.03 seconds (Total)\n## Chain 2: \n## \n## SAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\n## Chain 3: \n## Chain 3: Gradient evaluation took 2e-06 seconds\n## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds.\n## Chain 3: Adjust your expectations accordingly!\n## Chain 3: \n## Chain 3: \n## Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\n## Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\n## Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\n## Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\n## Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\n## Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\n## Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\n## Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\n## Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\n## Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\n## Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\n## Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\n## Chain 3: \n## Chain 3:  Elapsed Time: 0.013 seconds (Warm-up)\n## Chain 3:                0.012 seconds (Sampling)\n## Chain 3:                0.025 seconds (Total)\n## Chain 3: \n## \n## SAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\n## Chain 4: \n## Chain 4: Gradient evaluation took 2e-06 seconds\n## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds.\n## Chain 4: Adjust your expectations accordingly!\n## Chain 4: \n## Chain 4: \n## Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\n## Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\n## Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\n## Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\n## Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\n## Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\n## Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\n## Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\n## Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\n## Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\n## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\n## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\n## Chain 4: \n## Chain 4:  Elapsed Time: 0.016 seconds (Warm-up)\n## Chain 4:                0.012 seconds (Sampling)\n## Chain 4:                0.028 seconds (Total)\n## Chain 4:\nposteriors4 &lt;- describe_posterior(low4)\nprint_md(posteriors4, digits = 2)\n\n\nSummary of Posterior Distribution\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nMedian\n95% CI\npd\nROPE\n% in ROPE\nRhat\nESS\n\n\n\n\n(Intercept)\n81.90\n[ 72.65, 91.97]\n100%\n[-1.91, 1.91]\n0%\n1.001\n2681.00\n\n\nanesthiso\n-42.05\n[-51.37, -33.29]\n100%\n[-1.91, 1.91]\n0%\n1.000\n3075.00\n\n\n\n\n\n\n\nGenerate ggplot-compatible figure for mean difference posterior distribution\n\nposterior&lt;-as.array(low1)\ncolor_scheme_set(\"gray\")\np&lt;-mcmc_hist(posterior, pars = c(\"anesthiso\"))+\n  xlab(\"Mean difference\")\np\n## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n# ggsave (\"QK F2_07.pdf\", plot = p, height = ph, width = pw, units='cm')"
  },
  {
    "objectID": "chapter_notes/chapter3.html",
    "href": "chapter_notes/chapter3.html",
    "title": "Chapter 3",
    "section": "",
    "text": "Questions for review and discussion, based on chapter 3 from Quinn and Keough 2023."
  },
  {
    "objectID": "chapter_notes/chapter2.html",
    "href": "chapter_notes/chapter2.html",
    "title": "Chapter 2",
    "section": "",
    "text": "Questions for review and discussion, based on chapter 2 from Quinn and Keough 2023."
  },
  {
    "objectID": "chapter_notes/chapter2.html#section",
    "href": "chapter_notes/chapter2.html#section",
    "title": "Chapter 2",
    "section": "2.1",
    "text": "2.1\nWhat is a sample?\nWhat is the difference between a statistic and a parameter?\nWhat is the difference between process and observation uncertainty?"
  },
  {
    "objectID": "chapter_notes/chapter2.html#section-1",
    "href": "chapter_notes/chapter2.html#section-1",
    "title": "Chapter 2",
    "section": "2.2",
    "text": "2.2\nDefine the following terms:\n\nprobability\nsample space\nconditional probability\n\nDraw a Venn diagram representing the probability of three outcomes (A, B) in a sample space. Let C be mutually exclusive of A and B, but allow A and B to overlap. Use this diagram to visualize the idea of conditional probability, and relate it to the the mathematical equation for conditional probability."
  },
  {
    "objectID": "chapter_notes/chapter2.html#section-2",
    "href": "chapter_notes/chapter2.html#section-2",
    "title": "Chapter 2",
    "section": "2.3",
    "text": "2.3\nDefine the following terms:\n\nrandom variable\ndiscrete variable\ncontinuous variable\nprobability distribution\nprobability mass function\nprobability density function\n\nImagine you have counted rockfish along transects in the kelp forest. You are an avid diver, so you completed 1000 transects. Because these are count data, we will use a Poisson distribution (the support for the Poisson is non-negative integers). The average number of fish is 2 per transect. Note that in the Poisson, one parameter (\\(\\lambda\\)), governs the central tendency and the spread of the distribution (unlike, e.g., the Normal). This means the single parameter of the Poisson distribution is \\(\\lambda = 2\\).\nHere, we generate data according to our data story:\n\nset.seed(101)\nx &lt;- rpois(n = 1000, lambda = 2)\nhist(x, xlab = \"Rockfish per transect\",\n           main = expression(paste(\"1000 samples from a Poisson(\", lambda, \" = 2)\")))\n\n\n\n\nNow run these two lines of code. Why do these two expressions give you the same answer? (To force you to dig into the help files for these functions, I am not writing explicit code).\n\ndpois(0, 2) + dpois(1, 2) + dpois(2, 2)\nppois(2, 2)\n\nThe above code tells us that ~67% of the probability mass of a Poisson(2) distribution lies at or below 2. Returning to our random samples, let’s use quantile to figure out which of our values is at the 67th percentile in our data:\n\nquantile(x = x, probs = 0.67)\n\nNow let’s use qpois to identify the value at which we have 67% of the observations:\n\nqpois(p = 0.67, lambda = 2)\n\nFinally, let’s just tabulate the data to see if this all makes sense:\n\ntable(x)\nsum(table(x)[1:3]) / 1000\n\nPonder all of this until your understanding of the inter-relationships between d, p, q, r - pois is solid.\n\nPlotting distributions\nLet’s plot a Normal distribution centered at 0, with different standard deviations:\n\nx_grid &lt;- seq(-4, 4, 0.01)\n\nplot(x_grid, dnorm(x_grid, mean = 0, sd = 0.5), type = \"l\", \n     col = \"red\", xlab = \"x\", ylab = \"probability density\")\n\nlines(x_grid, dnorm(x_grid, mean = 0, sd = 1), type = \"l\", \n     col = \"blue\")\n\nlines(x_grid, dnorm(x_grid, mean = 0, sd = 1.5), type = \"l\", \n     col = \"black\")\n\n\n\n\nLet’s compare with a Student t-distribution:\n\nx_grid &lt;- seq(-3, 3, 0.01)\n\nplot(x_grid, dt(x_grid, df = 40), type = \"l\", \n     col = \"red\", xlab = \"x\", ylab = \"probability density\")\n\nlines(x_grid, dt(x_grid, df = 20), type = \"l\", \n     col = \"blue\")\n\nlines(x_grid, dt(x_grid, df = 10), type = \"l\", \n     col = \"black\")\n\nlines(x_grid, dnorm(x_grid, mean = 0, sd = 1), type = \"l\", \n     col = \"gray\", lty = 2)\n\n\n\n\nThe difference appears to be small. But the ‘thicker tails’ of the t-distribution dramatically increases the probability of extreme events, so-called black swans (e.g., in animal populations).\n\n\nMore practice\n\nFind the mean, variance, and 95% quantiles (i.e., 2.5% and 97.5% quantiles) of 1000 random draws from a Poisson distribution with \\(\\lambda=33\\).\nWhat is the probability \\(\\text P (X \\leq 6)\\) that a random draw from a Poisson distribution with \\(\\lambda = 4\\) will be less than or equal to 6?\nWhat is the probability \\(\\text P(X = 3)\\) of obtaining a value of 3 from a Binomial distribution with \\(p = 0.3\\) and \\(n = 5\\)?\nWhat is the probability \\(\\text P(-1.5 \\leq X \\leq 1.5)\\) that a value drawn from a standard normal distribution will be between -1.5 and 1.5? It may help to approach this visually.\nFind the value \\(x\\) that satisfies to \\(\\text P(X \\leq x) = 0.8\\), if \\(X\\) is a Gamma random variable with \\(k=2\\) and \\(\\theta = 1\\)."
  },
  {
    "objectID": "chapter_notes/chapter2.html#section-3",
    "href": "chapter_notes/chapter2.html#section-3",
    "title": "Chapter 2",
    "section": "2.4",
    "text": "2.4\nWhat is estimation?\nWhat makes a good estimator?\nCompare and contrast these frequentist estimation methods:\n\nordinary least squares\nmaximum likelihood\nresampling (bootstrap)\n\nPopulation parameters and sample statistics\n\nBox 2.2\nWhat is a sampling distribution?\nWhat does the central limit theorem tell us about the shape of a sampling distribution (e.g., a distribution of sample means)?\nWhat happens to the standard error of the mean as you increase sample size?\nWhen calculating the confidence interval for \\(\\mu\\), should you use a Normal distribution or a \\(t\\) distribution? Why? How do these two approaches compare?\nIn the frequentist world, parameters are fixed (but unknowable). In this context, how do you interpret a frequentist confidence interval?\nBootstrap methods and Box 2.3"
  },
  {
    "objectID": "chapter_notes/chapter2.html#section-4",
    "href": "chapter_notes/chapter2.html#section-4",
    "title": "Chapter 2",
    "section": "2.5",
    "text": "2.5\nHypothesis testing"
  },
  {
    "objectID": "chapter_notes/chapter2.html#section-5",
    "href": "chapter_notes/chapter2.html#section-5",
    "title": "Chapter 2",
    "section": "2.6",
    "text": "2.6\nComments on frequentist inference"
  },
  {
    "objectID": "chapter_notes/chapter2.html#section-6",
    "href": "chapter_notes/chapter2.html#section-6",
    "title": "Chapter 2",
    "section": "2.7",
    "text": "2.7\nBayesian inference"
  },
  {
    "objectID": "examples/low.html",
    "href": "examples/low.html",
    "title": "QK Box 2.2",
    "section": "",
    "text": "Low et al (2016) examined the effects of two different anesthetics on aspects of the physiology of the mouse. Twelve mice were anesthetized with isoflurane and eleven mice were anesthetized with alpha chloralose and blood CO2 levels were recorded after 120 minutes. The H0 was that there is no difference between the anesthetics in the mean blood CO2 level. This is an independent comparison because individual mice were only given one of the two anesthetics."
  },
  {
    "objectID": "examples/low.html#preliminaries",
    "href": "examples/low.html#preliminaries",
    "title": "QK Box 2.2",
    "section": "Preliminaries",
    "text": "Preliminaries\nFirst, load the required packages (tidyverse, RMisc, MKinfer, car, emmeans)\nImport low data file\n\nlow &lt;- read.csv(\"data/lowco2.csv\")\nlow\n\n   anesth co2\n1     iso  43\n2     iso  35\n3     iso  50\n4     iso  39\n5     iso  56\n6     iso  54\n7     iso  39\n8     iso  51\n9     iso  49\n10    iso  54\n11    iso  51\n12    iso  79\n13     ac  60\n14     ac  53\n15     ac  54\n16     ac  73\n17     ac  64\n18     ac  95\n19     ac  57\n20     ac  80\n21     ac 115\n22     ac  79\n23     ac  50"
  },
  {
    "objectID": "examples/low.html#get-summary-statistics-by-anesthetic",
    "href": "examples/low.html#get-summary-statistics-by-anesthetic",
    "title": "QK Box 2.2",
    "section": "Get summary statistics by anesthetic",
    "text": "Get summary statistics by anesthetic\n\nlow_stats &lt;- summarySE(data=low,measurevar=\"co2\", groupvars=\"anesth\")\nlow_stats\n\n  anesth  N      co2       sd       se        ci\n1     ac 11 70.90909 20.20126 6.090909 13.571391\n2    iso 12 50.00000 11.39378 3.289100  7.239261\n\nlow %&gt;% dplyr::count(anesth)\n\n  anesth  n\n1     ac 11\n2    iso 12\n\nlow %&gt;%  \n  group_by(anesth) %&gt;% \n  dplyr::summarise(n = n(), \n            mean = mean(co2),\n            median = median(co2),\n            sd = sd(co2), \n            variance = var(co2), \n            se = sd / sqrt(n), \n            CI_upper = mean + se * qt(p = 0.975, df = n-1), \n            CI_lower = mean + se * qt(p = 0.025, df = n-1), \n            CI = se * qt(p = 0.975, df = n-1), \n            upper = mean + CI, \n            lower = mean - CI\n            )\n\n# A tibble: 2 × 12\n  anesth     n  mean median    sd variance    se CI_upper CI_lower    CI upper\n  &lt;chr&gt;  &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 ac        11  70.9   64    20.2     408.  6.09     84.5     57.3 13.6   84.5\n2 iso       12  50     50.5  11.4     130.  3.29     57.2     42.8  7.24  57.2\n# ℹ 1 more variable: lower &lt;dbl&gt;\n\n\nPlay around with df to see how the z-multiplier changes when using the T-distribution to calculate the 95% confidence interval.\n\n# Standard normal distribution\nqnorm(p = 0.025)\n\n[1] -1.959964\n\nqnorm(p = 0.975)\n\n[1] 1.959964\n\n# Student-t distribution\nqt(p = 0.025, df = 100)\n\n[1] -1.983972\n\nqt(p = 0.975, df = 100)\n\n[1] 1.983972"
  },
  {
    "objectID": "examples/low.html#plot-data",
    "href": "examples/low.html#plot-data",
    "title": "QK Box 2.2",
    "section": "Plot data",
    "text": "Plot data\n\nlow %&gt;% \n  ggplot(aes(anesth, co2)) + \n  geom_point(alpha = 0.5) + \n  theme_qk()"
  },
  {
    "objectID": "examples/low.html#fit-model-and-get-effect-size",
    "href": "examples/low.html#fit-model-and-get-effect-size",
    "title": "QK Box 2.2",
    "section": "Fit model and get effect size",
    "text": "Fit model and get effect size\n\nlow.aov &lt;- aov(co2~anesth,data=low)\ntidy(low.aov, conf.int=TRUE)\n\n# A tibble: 2 × 6\n  term         df sumsq meansq statistic  p.value\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 anesth        1 2509.  2509.      9.56  0.00552\n2 Residuals    21 5509.   262.     NA    NA      \n\nlow.emm &lt;- emmeans(low.aov,\"anesth\")\neff_size(low.emm, sigma=sigma(low.aov), edf=df.residual(low.aov))\n\n contrast effect.size    SE df lower.CL upper.CL\n ac - iso        1.29 0.463 21    0.329     2.25\n\nsigma used for effect sizes: 16.2 \nConfidence level used: 0.95 \n\n\nNote that we’ve chosen to show a standardized effect size, using the pooled variance from the analysis of variance - Residual MS = 262.44, and √262.44 = 16.2"
  },
  {
    "objectID": "examples/low.html#test-variances",
    "href": "examples/low.html#test-variances",
    "title": "QK Box 2.2",
    "section": "Test variances",
    "text": "Test variances\n\nleveneTest(co2 ~ anesth, low)\n\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(&gt;F)\ngroup  1   2.604 0.1215\n      21               \n\n\n\nt-test for equal variances\n\nt.test(co2~anesth,var.equal=TRUE, data=low)\n\n\n    Two Sample t-test\n\ndata:  co2 by anesth\nt = 3.0927, df = 21, p-value = 0.005515\nalternative hypothesis: true difference in means between group ac and group iso is not equal to 0\n95 percent confidence interval:\n  6.849172 34.969010\nsample estimates:\n mean in group ac mean in group iso \n         70.90909          50.00000 \n\n\n\n\nt-test for separate variances\n\nt.test(co2~anesth,data=low)\n\n\n    Welch Two Sample t-test\n\ndata:  co2 by anesth\nt = 3.0206, df = 15.485, p-value = 0.008362\nalternative hypothesis: true difference in means between group ac and group iso is not equal to 0\n95 percent confidence interval:\n  6.194866 35.623316\nsample estimates:\n mean in group ac mean in group iso \n         70.90909          50.00000"
  },
  {
    "objectID": "examples/low.html#wilcoxon-mann-whitney",
    "href": "examples/low.html#wilcoxon-mann-whitney",
    "title": "QK Box 2.2",
    "section": "Wilcoxon-Mann-Whitney",
    "text": "Wilcoxon-Mann-Whitney\n\nwilcox.test(co2~anesth,data=low)\n\nWarning in wilcox.test.default(x = DATA[[1L]], y = DATA[[2L]], ...): cannot\ncompute exact p-value with ties\n\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  co2 by anesth\nW = 114, p-value = 0.003398\nalternative hypothesis: true location shift is not equal to 0\n\nsum(rank(low$co2)[low$anesth==\"ac\"])\n\n[1] 180\n\nsum(rank(low$co2)[low$anesth==\"iso\"])\n\n[1] 96"
  },
  {
    "objectID": "examples/lowboot.html",
    "href": "examples/lowboot.html",
    "title": "QK Box 2.3",
    "section": "",
    "text": "This box continues with the Low et al. anesthetic example from Box 2.2\n\nPreliminaries\nPackages: MKinfer, resample\n\nlibrary(MKinfer)\nlibrary(resample)\nlibrary(tidyverse)\n\nUse low data:\n\nlow &lt;- read_csv(\"data/lowco2.csv\")\n\n\n\nGet jackknife SE for two groups\n\nlow1 &lt;- subset(low,anesth==\"iso\")\njackknife(low1$co2,mean)\n## Call:\n## jackknife(data = low1$co2, statistic = mean)\n## Replications: 12\n## \n## Summary Statistics:\n##      Observed     SE Mean Bias\n## mean       50 3.2891   50    0\nlow2 &lt;- subset(low,anesth==\"ac\")\njackknife(low2$co2,mean)\n## Call:\n## jackknife(data = low2$co2, statistic = mean)\n## Replications: 11\n## \n## Summary Statistics:\n##      Observed       SE     Mean         Bias\n## mean 70.90909 6.090909 70.90909 1.421085e-13\n\n\n\nGet bootstrap SE and 95%CI\n\nlow1boot &lt;- bootstrap(low1$co2,mean,R=9999)\nlow1boot\n## Call:\n## bootstrap(data = low1$co2, statistic = mean, R = 9999)\n## Replications: 9999\n## \n## Summary Statistics:\n##      Observed       SE    Mean       Bias\n## mean       50 3.188776 50.0104 0.01040104\nCI.percentile(low1boot, probs=c(0.025,0.975))\n##      2.5% 97.5%\n## mean 43.5    58\nCI.bca(low1boot, probs=c(0.025,0.975))\n##          2.5%    97.5%\n## mean 44.33333 60.16667\n\nlow2boot &lt;- bootstrap(low2$co2,mean,R=9999)\nlow2boot\n## Call:\n## bootstrap(data = low2$co2, statistic = mean, R = 9999)\n## Replications: 9999\n## \n## Summary Statistics:\n##      Observed       SE     Mean        Bias\n## mean 70.90909 5.860237 70.89403 -0.01505605\nCI.percentile(low2boot, probs=c(0.025,0.975))\n##          2.5%    97.5%\n## mean 58.72727 86.16145\nCI.bca(low2boot, probs=c(0.025,0.975))\n##      2.5%    97.5%\n## mean   60 88.36364\n\n\n\nGet bootstrap SE and CI on difference\n\nlowboot &lt;- bootstrap2(low$co2,mean,treatment=low$anesth,R=9999,ratio=FALSE)\nlowboot\n## Call:\n## bootstrap2(data = low$co2, statistic = mean, treatment = low$anesth, \n##     R = 9999, ratio = FALSE)\n## Replications: 9999\n## Two samples, sample sizes are 11 12\n## \n## Summary Statistics for the difference between samples 1 and 2:\n##              Observed       SE     Mean       Bias\n## mean: ac-iso 20.90909 6.492549 20.91318 0.00408753\nCI.percentile(lowboot, probs=c(0.025,0.975))\n##                  2.5%    97.5%\n## mean: ac-iso 6.212121 37.08751\n\n\n\nRandomization test\n\nperm.t.test(co2~anesth, data=low, R=9999, paired= FALSE)\n## \n##  Permutation Welch Two Sample t-test\n## \n## data:  co2 by anesth\n## (Monte-Carlo) permutation p-value = 0.0038 \n## permutation difference of means (SE) = 20.9089 (7.966646) \n## 95 percent (Monte-Carlo) permutation percentile confidence interval:\n##   5.924242 36.251136\n## \n## Results without permutation:\n## t = 3.0206, df = 15.485, p-value = 0.008362\n## alternative hypothesis: true difference in means is not equal to 0\n## 95 percent confidence interval:\n##   6.194866 35.623316\n## sample estimates:\n##  mean in group ac mean in group iso \n##          70.90909          50.00000"
  },
  {
    "objectID": "slides/intro_qk_slides.html#course-overview",
    "href": "slides/intro_qk_slides.html#course-overview",
    "title": "Intro to QK2E",
    "section": "Course overview",
    "text": "Course overview\n1 or 2 presenters per week\nExpectations (everyone)\n\nRead the chapter\nWork through the suggested examples in R"
  },
  {
    "objectID": "slides/intro_qk_slides.html#expectations-presenter",
    "href": "slides/intro_qk_slides.html#expectations-presenter",
    "title": "Intro to QK2E",
    "section": "Expectations (presenter)",
    "text": "Expectations (presenter)\n\nlead a discussion of the reading\nprepare a group / break-out activity\nbe creative and focus on what you want, in the context of the chapter\nupload relevant materials to Canvas / Gdrive"
  },
  {
    "objectID": "slides/intro_qk_slides.html#qk2e",
    "href": "slides/intro_qk_slides.html#qk2e",
    "title": "Intro to QK2E",
    "section": "QK2E",
    "text": "QK2E"
  },
  {
    "objectID": "slides/intro_qk_slides.html#sign-up-to-lead-a-week",
    "href": "slides/intro_qk_slides.html#sign-up-to-lead-a-week",
    "title": "Intro to QK2E",
    "section": "Sign up to lead a week",
    "text": "Sign up to lead a week\nLink to Google sign up is on Canvas\n\n\n\nProgramming challenge (should you choose to accept it):\n\nIf you are going to use slides, create them in Quarto (.qmd) or Rmarkdown (.Rmd)\n\nTotally optional. In any case, please share your ppt slides, R code, other relevant materials with the group in the Gdrive."
  },
  {
    "objectID": "slides/intro_qk_slides.html#prerequisites",
    "href": "slides/intro_qk_slides.html#prerequisites",
    "title": "Intro to QK2E",
    "section": "Prerequisites",
    "text": "Prerequisites\nYou have some familiarity with R computing and statistics.\n\nDo you know what all this means?\n\nx &lt;- c(2, 4, 3, 6)\ny &lt;- c(5, 12, 4, 10, 2)\nt.test(x, y)\n\n\n    Welch Two Sample t-test\n\ndata:  x and y\nt = -1.3761, df = 5.4988, p-value = 0.2222\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -8.031728  2.331728\nsample estimates:\nmean of x mean of y \n     3.75      6.60"
  },
  {
    "objectID": "slides/intro_qk_slides.html#statistics-vs-parameters",
    "href": "slides/intro_qk_slides.html#statistics-vs-parameters",
    "title": "Intro to QK2E",
    "section": "Statistics vs parameters",
    "text": "Statistics vs parameters\n\n\nA statistic is\n\na numerical description of a sample\n\n\n\n\n\nA parameter is\n\na numerical attribute of a population\n\n\n\n\nOften, statistics are used to estimate parameters."
  },
  {
    "objectID": "slides/intro_qk_slides.html#the-two-heads-of-classical-statistics",
    "href": "slides/intro_qk_slides.html#the-two-heads-of-classical-statistics",
    "title": "Intro to QK2E",
    "section": "The two heads of classical statistics",
    "text": "The two heads of classical statistics\n\nestimating parameters, with uncertainty (confidence intervals)\nevaluating (in-)consistency with a particular situation (\\(p\\)-values)\n\n\n\nWhat do these data tell us about the world?\nHow strongly do we believe it?"
  },
  {
    "objectID": "slides/intro_qk_slides.html#lurking-behind-everything",
    "href": "slides/intro_qk_slides.html#lurking-behind-everything",
    "title": "Intro to QK2E",
    "section": "Lurking, behind everything:",
    "text": "Lurking, behind everything:\nis uncertainty, thanks to:\n\n\nactual differences of biological interest (process uncertainty)\n\n\n\n\nuninteresting differences due to sampling variation and measurement error (observation uncertainty)\n\n\n\nHow do we understand uncertainty, concretely and quantitatively?\n\n\n\nwith models."
  },
  {
    "objectID": "slides/intro_qk_slides.html#break",
    "href": "slides/intro_qk_slides.html#break",
    "title": "Intro to QK2E",
    "section": "Break",
    "text": "Break\nStand up! Stretch! Get a drink, use the restroom.\nThen, with a partner(s), go to a board and discuss the following:\n\nWhat is hypothesis testing?\nWhat is a p-value?"
  },
  {
    "objectID": "slides/intro_qk_slides.html#data-story",
    "href": "slides/intro_qk_slides.html#data-story",
    "title": "Intro to QK2E",
    "section": "Data story",
    "text": "Data story\nLow et al (2016) examined the effects of two different anesthetics on aspects of the physiology of the mouse. Twelve mice were anesthetized with isoflurane and eleven mice were anesthetized with alpha chloralose and blood CO2 levels were recorded after 120 minutes. The H0 was that there was no difference between the anesthetics in the mean blood CO2 level. This is an independent comparison because individual mice were only given one of the two anesthetics."
  },
  {
    "objectID": "slides/intro_qk_slides.html#r",
    "href": "slides/intro_qk_slides.html#r",
    "title": "Intro to QK2E",
    "section": "R",
    "text": "R\n\nlibrary(tidyverse)\nlibrary(car)\n\ntheme_set(theme_bw(base_size = 16) + \n            theme(panel.grid.minor = element_blank(), \n                  strip.background = element_blank()))"
  },
  {
    "objectID": "slides/intro_qk_slides.html#the-data",
    "href": "slides/intro_qk_slides.html#the-data",
    "title": "Intro to QK2E",
    "section": "The data",
    "text": "The data\nDescribe what is happening in these lines of code.\n\nlow &lt;- read.csv(\"data/lowco2.csv\")\n\n \n\n\nnames(low)\n\n[1] \"anesth\" \"co2\"   \n\n\n\n\n\n\n\ndim(low)\n\n[1] 23  2\n\n\n\n\n\n\n\nstr(low)\n\n'data.frame':   23 obs. of  2 variables:\n $ anesth: chr  \"iso\" \"iso\" \"iso\" \"iso\" ...\n $ co2   : int  43 35 50 39 56 54 39 51 49 54 ..."
  },
  {
    "objectID": "slides/intro_qk_slides.html#visualize-data",
    "href": "slides/intro_qk_slides.html#visualize-data",
    "title": "Intro to QK2E",
    "section": "Visualize data",
    "text": "Visualize data\n\nlow %&gt;% \n  ggplot(aes(anesth, co2)) + \n  geom_point(alpha = 0.5, size = 5) + \n  labs(x = \"Anesthetic\", y = \"CO2\") + \n  theme_bw(base_size = 24)"
  },
  {
    "objectID": "slides/intro_qk_slides.html#summarizing-data-point-estimates-and-variability",
    "href": "slides/intro_qk_slides.html#summarizing-data-point-estimates-and-variability",
    "title": "Intro to QK2E",
    "section": "Summarizing data: point estimates and variability",
    "text": "Summarizing data: point estimates and variability\n\nlow %&gt;%  \n  group_by(anesth) %&gt;% \n  summarise(n = n(), \n            mean = mean(co2),\n            median = median(co2),\n            sd = sd(co2), \n            variance = var(co2), \n            se = sd / sqrt(n)\n            )\n\n# A tibble: 2 × 7\n  anesth     n  mean median    sd variance    se\n  &lt;chr&gt;  &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n1 ac        11  70.9   64    20.2     408.  6.09\n2 iso       12  50     50.5  11.4     130.  3.29"
  },
  {
    "objectID": "slides/intro_qk_slides.html#confidence-intervals",
    "href": "slides/intro_qk_slides.html#confidence-intervals",
    "title": "Intro to QK2E",
    "section": "Confidence intervals",
    "text": "Confidence intervals\n\nlow %&gt;%  \n  group_by(anesth) %&gt;% \n  summarise(n = n(), \n            mean = mean(co2),\n            sd = sd(co2), \n            se = sd / sqrt(n), \n            CI_upper = mean + se * qt(p = 0.975, df = n-1), \n            CI_lower = mean + se * qt(p = 0.025, df = n-1), \n            CI = se * qt(p = 0.975, df = n-1), \n            upper = mean + CI, \n            lower = mean - CI\n            )\n\n# A tibble: 2 × 10\n  anesth     n  mean    sd    se CI_upper CI_lower    CI upper lower\n  &lt;chr&gt;  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 ac        11  70.9  20.2  6.09     84.5     57.3 13.6   84.5  57.3\n2 iso       12  50    11.4  3.29     57.2     42.8  7.24  57.2  42.8\n\n\nIn a frequentist world, parameters are fixed (but unknowable). Interpret the CI in this context."
  },
  {
    "objectID": "slides/intro_qk_slides.html#hypothesis-testing",
    "href": "slides/intro_qk_slides.html#hypothesis-testing",
    "title": "Intro to QK2E",
    "section": "Hypothesis testing",
    "text": "Hypothesis testing\nA. Construct a null hypothesis (HO)\nB. Derive a test statistic from the data\nC. Compare the obtained test statistic to one derived from values obtained under the HO."
  },
  {
    "objectID": "slides/intro_qk_slides.html#a-p-value-is",
    "href": "slides/intro_qk_slides.html#a-p-value-is",
    "title": "Intro to QK2E",
    "section": "A \\(p\\)-value is",
    "text": "A \\(p\\)-value is\n\n\nthe probability of seeing a result at least as surprising as what was observed in the data, if the null hypothesis is true.\n\n\n\nUsually, this means\n\na result - numerical value of a statistic\nsurprising - big\nnull hypothesis - the model we use to calculate the \\(p\\)-value\n\nwhich can all be defined to suit the situation."
  },
  {
    "objectID": "slides/intro_qk_slides.html#what-does-a-small-p-value-mean",
    "href": "slides/intro_qk_slides.html#what-does-a-small-p-value-mean",
    "title": "Intro to QK2E",
    "section": "What does a small \\(p\\)-value mean?",
    "text": "What does a small \\(p\\)-value mean?\n\nIf the null hypothesis was true, then you’d be really unlikely to see something like what you actually did.\n\n\n\n\nSo, either the “null hypothesis” is not a good description of reality or something surprising happened.\n\n\n\n\nHow useful this is depends on the null hypothesis."
  },
  {
    "objectID": "slides/intro_qk_slides.html#t-test-equal-variances",
    "href": "slides/intro_qk_slides.html#t-test-equal-variances",
    "title": "Intro to QK2E",
    "section": "T-test: equal variances",
    "text": "T-test: equal variances\n\nt.test(co2 ~ anesth, var.equal = TRUE, data = low)\n\n\n    Two Sample t-test\n\ndata:  co2 by anesth\nt = 3.0927, df = 21, p-value = 0.005515\nalternative hypothesis: true difference in means between group ac and group iso is not equal to 0\n95 percent confidence interval:\n  6.849172 34.969010\nsample estimates:\n mean in group ac mean in group iso \n         70.90909          50.00000 \n\n\n\n\nInterpret this result."
  },
  {
    "objectID": "slides/intro_qk_slides.html#your-turn",
    "href": "slides/intro_qk_slides.html#your-turn",
    "title": "Intro to QK2E",
    "section": "Your turn",
    "text": "Your turn\nWith a partner, go to the website, check out the source code, and work though the Chapter 2 notes together."
  }
]