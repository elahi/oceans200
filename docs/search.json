[
  {
    "objectID": "slides/slides.html",
    "href": "slides/slides.html",
    "title": "Slides",
    "section": "",
    "text": "Week 1: Intro to Quinn and Keough, 2nd edition\nWeek 3: Linear models: one-way designs"
  },
  {
    "objectID": "slides/intro_qk_slides.html#course-overview",
    "href": "slides/intro_qk_slides.html#course-overview",
    "title": "Intro to QK2E",
    "section": "Course overview",
    "text": "Course overview\n1 or 2 presenters per week\nExpectations (everyone)\n\nRead the chapter\nWork through the suggested examples in R"
  },
  {
    "objectID": "slides/intro_qk_slides.html#expectations-presenter",
    "href": "slides/intro_qk_slides.html#expectations-presenter",
    "title": "Intro to QK2E",
    "section": "Expectations (presenter)",
    "text": "Expectations (presenter)\n\nlead a discussion of the reading\nprepare a group / break-out activity\nbe creative and focus on what you want, in the context of the chapter\nupload relevant materials to Canvas / Gdrive"
  },
  {
    "objectID": "slides/intro_qk_slides.html#qk2e",
    "href": "slides/intro_qk_slides.html#qk2e",
    "title": "Intro to QK2E",
    "section": "QK2E",
    "text": "QK2E"
  },
  {
    "objectID": "slides/intro_qk_slides.html#sign-up-to-lead-a-week",
    "href": "slides/intro_qk_slides.html#sign-up-to-lead-a-week",
    "title": "Intro to QK2E",
    "section": "Sign up to lead a week",
    "text": "Sign up to lead a week\nLink to Google sign up is on Canvas\n\n\n\nProgramming challenge (should you choose to accept it):\n\nIf you are going to use slides, create them in Quarto (.qmd) or Rmarkdown (.Rmd)\n\nTotally optional. In any case, please share your ppt slides, R code, other relevant materials with the group in the Gdrive."
  },
  {
    "objectID": "slides/intro_qk_slides.html#prerequisites",
    "href": "slides/intro_qk_slides.html#prerequisites",
    "title": "Intro to QK2E",
    "section": "Prerequisites",
    "text": "Prerequisites\nYou have some familiarity with R computing and statistics.\n\nDo you know what all this means?\n\nx &lt;- c(2, 4, 3, 6)\ny &lt;- c(5, 12, 4, 10, 2)\nt.test(x, y)\n\n\n    Welch Two Sample t-test\n\ndata:  x and y\nt = -1.3761, df = 5.4988, p-value = 0.2222\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -8.031728  2.331728\nsample estimates:\nmean of x mean of y \n     3.75      6.60"
  },
  {
    "objectID": "slides/intro_qk_slides.html#statistics-vs-parameters",
    "href": "slides/intro_qk_slides.html#statistics-vs-parameters",
    "title": "Intro to QK2E",
    "section": "Statistics vs parameters",
    "text": "Statistics vs parameters\n\n\nA statistic is\n\na numerical description of a sample\n\n\n\n\n\nA parameter is\n\na numerical attribute of a population\n\n\n\n\nOften, statistics are used to estimate parameters."
  },
  {
    "objectID": "slides/intro_qk_slides.html#the-two-heads-of-classical-statistics",
    "href": "slides/intro_qk_slides.html#the-two-heads-of-classical-statistics",
    "title": "Intro to QK2E",
    "section": "The two heads of classical statistics",
    "text": "The two heads of classical statistics\n\nestimating parameters, with uncertainty (confidence intervals)\nevaluating (in-)consistency with a particular situation (\\(p\\)-values)\n\n\n\nWhat do these data tell us about the world?\nHow strongly do we believe it?"
  },
  {
    "objectID": "slides/intro_qk_slides.html#lurking-behind-everything",
    "href": "slides/intro_qk_slides.html#lurking-behind-everything",
    "title": "Intro to QK2E",
    "section": "Lurking, behind everything:",
    "text": "Lurking, behind everything:\nis uncertainty, thanks to:\n\n\nactual differences of biological interest (process uncertainty)\n\n\n\n\nuninteresting differences due to sampling variation and measurement error (observation uncertainty)\n\n\n\nHow do we understand uncertainty, concretely and quantitatively?\n\n\n\nwith models."
  },
  {
    "objectID": "slides/intro_qk_slides.html#break",
    "href": "slides/intro_qk_slides.html#break",
    "title": "Intro to QK2E",
    "section": "Break",
    "text": "Break\nStand up! Stretch! Get a drink, use the restroom.\nThen, with a partner(s), go to a board and discuss the following:\n\nWhat is hypothesis testing?\nWhat is a p-value?"
  },
  {
    "objectID": "slides/intro_qk_slides.html#data-story",
    "href": "slides/intro_qk_slides.html#data-story",
    "title": "Intro to QK2E",
    "section": "Data story",
    "text": "Data story\nLow et al (2016) examined the effects of two different anesthetics on aspects of the physiology of the mouse. Twelve mice were anesthetized with isoflurane and eleven mice were anesthetized with alpha chloralose and blood CO2 levels were recorded after 120 minutes. The H0 was that there was no difference between the anesthetics in the mean blood CO2 level. This is an independent comparison because individual mice were only given one of the two anesthetics."
  },
  {
    "objectID": "slides/intro_qk_slides.html#r",
    "href": "slides/intro_qk_slides.html#r",
    "title": "Intro to QK2E",
    "section": "R",
    "text": "R\n\nlibrary(tidyverse)\nlibrary(car)\n\ntheme_set(theme_bw(base_size = 16) + \n            theme(panel.grid.minor = element_blank(), \n                  strip.background = element_blank()))"
  },
  {
    "objectID": "slides/intro_qk_slides.html#the-data",
    "href": "slides/intro_qk_slides.html#the-data",
    "title": "Intro to QK2E",
    "section": "The data",
    "text": "The data\nDescribe what is happening in these lines of code.\n\nlow &lt;- read.csv(\"data/lowco2.csv\")\n\n \n\n\nnames(low)\n\n[1] \"anesth\" \"co2\"   \n\n\n\n\n\n\n\ndim(low)\n\n[1] 23  2\n\n\n\n\n\n\n\nstr(low)\n\n'data.frame':   23 obs. of  2 variables:\n $ anesth: chr  \"iso\" \"iso\" \"iso\" \"iso\" ...\n $ co2   : int  43 35 50 39 56 54 39 51 49 54 ..."
  },
  {
    "objectID": "slides/intro_qk_slides.html#visualize-data",
    "href": "slides/intro_qk_slides.html#visualize-data",
    "title": "Intro to QK2E",
    "section": "Visualize data",
    "text": "Visualize data\n\nlow %&gt;% \n  ggplot(aes(anesth, co2)) + \n  geom_point(alpha = 0.5, size = 5) + \n  labs(x = \"Anesthetic\", y = \"CO2\") + \n  theme_bw(base_size = 24)"
  },
  {
    "objectID": "slides/intro_qk_slides.html#summarizing-data-point-estimates-and-variability",
    "href": "slides/intro_qk_slides.html#summarizing-data-point-estimates-and-variability",
    "title": "Intro to QK2E",
    "section": "Summarizing data: point estimates and variability",
    "text": "Summarizing data: point estimates and variability\n\nlow %&gt;%  \n  group_by(anesth) %&gt;% \n  summarise(n = n(), \n            mean = mean(co2),\n            median = median(co2),\n            sd = sd(co2), \n            variance = var(co2), \n            se = sd / sqrt(n)\n            )\n\n# A tibble: 2 × 7\n  anesth     n  mean median    sd variance    se\n  &lt;chr&gt;  &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n1 ac        11  70.9   64    20.2     408.  6.09\n2 iso       12  50     50.5  11.4     130.  3.29"
  },
  {
    "objectID": "slides/intro_qk_slides.html#confidence-intervals",
    "href": "slides/intro_qk_slides.html#confidence-intervals",
    "title": "Intro to QK2E",
    "section": "Confidence intervals",
    "text": "Confidence intervals\n\nlow %&gt;%  \n  group_by(anesth) %&gt;% \n  summarise(n = n(), \n            mean = mean(co2),\n            sd = sd(co2), \n            se = sd / sqrt(n), \n            CI_upper = mean + se * qt(p = 0.975, df = n-1), \n            CI_lower = mean + se * qt(p = 0.025, df = n-1), \n            CI = se * qt(p = 0.975, df = n-1), \n            upper = mean + CI, \n            lower = mean - CI\n            )\n\n# A tibble: 2 × 10\n  anesth     n  mean    sd    se CI_upper CI_lower    CI upper lower\n  &lt;chr&gt;  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 ac        11  70.9  20.2  6.09     84.5     57.3 13.6   84.5  57.3\n2 iso       12  50    11.4  3.29     57.2     42.8  7.24  57.2  42.8\n\n\nIn a frequentist world, parameters are fixed (but unknowable). Interpret the CI in this context."
  },
  {
    "objectID": "slides/intro_qk_slides.html#hypothesis-testing",
    "href": "slides/intro_qk_slides.html#hypothesis-testing",
    "title": "Intro to QK2E",
    "section": "Hypothesis testing",
    "text": "Hypothesis testing\nA. Construct a null hypothesis (HO)\nB. Derive a test statistic from the data\nC. Compare the obtained test statistic to one derived from values obtained under the HO."
  },
  {
    "objectID": "slides/intro_qk_slides.html#a-p-value-is",
    "href": "slides/intro_qk_slides.html#a-p-value-is",
    "title": "Intro to QK2E",
    "section": "A \\(p\\)-value is",
    "text": "A \\(p\\)-value is\n\n\nthe probability of seeing a result at least as surprising as what was observed in the data, if the null hypothesis is true.\n\n\n\nUsually, this means\n\na result - numerical value of a statistic\nsurprising - big\nnull hypothesis - the model we use to calculate the \\(p\\)-value\n\nwhich can all be defined to suit the situation."
  },
  {
    "objectID": "slides/intro_qk_slides.html#what-does-a-small-p-value-mean",
    "href": "slides/intro_qk_slides.html#what-does-a-small-p-value-mean",
    "title": "Intro to QK2E",
    "section": "What does a small \\(p\\)-value mean?",
    "text": "What does a small \\(p\\)-value mean?\n\nIf the null hypothesis was true, then you’d be really unlikely to see something like what you actually did.\n\n\n\n\nSo, either the “null hypothesis” is not a good description of reality or something surprising happened.\n\n\n\n\nHow useful this is depends on the null hypothesis."
  },
  {
    "objectID": "slides/intro_qk_slides.html#t-test-equal-variances",
    "href": "slides/intro_qk_slides.html#t-test-equal-variances",
    "title": "Intro to QK2E",
    "section": "T-test: equal variances",
    "text": "T-test: equal variances\n\nt.test(co2 ~ anesth, var.equal = TRUE, data = low)\n\n\n    Two Sample t-test\n\ndata:  co2 by anesth\nt = 3.0927, df = 21, p-value = 0.005515\nalternative hypothesis: true difference in means between group ac and group iso is not equal to 0\n95 percent confidence interval:\n  6.849172 34.969010\nsample estimates:\n mean in group ac mean in group iso \n         70.90909          50.00000 \n\n\n\n\nInterpret this result."
  },
  {
    "objectID": "slides/intro_qk_slides.html#your-turn",
    "href": "slides/intro_qk_slides.html#your-turn",
    "title": "Intro to QK2E",
    "section": "Your turn",
    "text": "Your turn\nWith a partner, go to the website, check out the source code, and work though the Chapter 2 notes together."
  },
  {
    "objectID": "examples/serpulid.html",
    "href": "examples/serpulid.html",
    "title": "QK Box 6.7 & 6.11",
    "section": "",
    "text": "Keough and Raimondi (1995) set up an experiment to examine the response of serpulid (polychaete worms) larvae to four types of biofilms on hard substrata in shallow marine waters. The four treatments were: sterile substrata, biofilms developed in the field with a net (to keep invertebrates), biofilms developed in the lab, and lab biofilms with a covering net (as a control for the presence of a net). The substrata were left for one week, and then the newly settled worms identified and counted. To control for small numbers of larvae passing through the netting during the conditioning period, they used an additional treatment, which was netted, and returned to the laboratory after one week and censused. The values of this treatment were used to adjust the numbers in the treatment that started in the field.\nThe paper is here and the data file (also used in first edition) is here\nKeough, M. J. & Raimondi, P. T. (1995). Responses of settling invertebrate larvae to bioorganic films: effects of different types of films. Journal of Experimental Marine Biology and Ecology, 185, 235-53."
  },
  {
    "objectID": "examples/serpulid.html#generate-planned-comparisons",
    "href": "examples/serpulid.html#generate-planned-comparisons",
    "title": "QK Box 6.7 & 6.11",
    "section": "Generate planned comparisons",
    "text": "Generate planned comparisons\nWe’re doing this by defining contrasts and refitting the model using this contrast. The planned comparison then appears as the first effect when we look at the model fitting (i.e., film1).\n\nUL vs NL\n\n# Default\ncontrasts(serpulid$film)\n\n   NL SL UL\nF   0  0  0\nNL  1  0  0\nSL  0  1  0\nUL  0  0  1\n\n# Change it\ncontrasts(serpulid$film) &lt;- c(0,1,0,-1)\ncontrasts(serpulid$film)\n\n   [,1]       [,2]       [,3]\nF     0 -0.5000000 -0.7071068\nNL    1 -0.1666667  0.4714045\nSL    0  0.8333333 -0.2357023\nUL   -1 -0.1666667  0.4714045\n\n# Refit the model with new contrasts\nserpulid.aov &lt;- aov(lserp~film, data=serpulid)\nsummary(serpulid.aov)\n\n            Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nfilm         3 0.2458 0.08192   6.015 0.00331 **\nResiduals   24 0.3269 0.01362                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary.lm(serpulid.aov) \n\n\nCall:\naov(formula = lserp ~ film, data = serpulid)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.22929 -0.06500  0.01843  0.07054  0.19557 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.09039    0.02206  94.780  &lt; 2e-16 ***\nfilm1        0.02479    0.03119   0.795  0.43461    \nfilm2       -0.16407    0.04411  -3.720  0.00107 ** \nfilm3        0.08344    0.04411   1.892  0.07067 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1167 on 24 degrees of freedom\nMultiple R-squared:  0.4292,    Adjusted R-squared:  0.3578 \nF-statistic: 6.015 on 3 and 24 DF,  p-value: 0.003314\n\n\n\n\nF vs average (NL & UL)\n\ncontrasts(serpulid$film) &lt;- c(2,-1,0,-1)\ncontrasts(serpulid$film)\n\n   [,1]       [,2]        [,3]\nF     2 -0.2867757  0.03306064\nNL   -1 -0.3677574 -0.66939360\nSL    0  0.8603272 -0.09918192\nUL   -1 -0.2057940  0.73551488\n\n# Refit the model with new contrasts\nserpulid.aov &lt;- aov(lserp~film, data=serpulid)\nsummary(serpulid.aov)\n\n            Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nfilm         3 0.2458 0.08192   6.015 0.00331 **\nResiduals   24 0.3269 0.01362                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary.lm(serpulid.aov) \n\n\nCall:\naov(formula = lserp ~ film, data = serpulid)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.22929 -0.06500  0.01843  0.07054  0.19557 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.09039    0.02206  94.780  &lt; 2e-16 ***\nfilm1       -0.01455    0.01801  -0.808 0.427118    \nfilm2       -0.18341    0.04411  -4.158 0.000353 ***\nfilm3       -0.01414    0.04411  -0.321 0.751322    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1167 on 24 degrees of freedom\nMultiple R-squared:  0.4292,    Adjusted R-squared:  0.3578 \nF-statistic: 6.015 on 3 and 24 DF,  p-value: 0.003314\n\n\n\n\nSL vs average (F & NL & UL)\n\ncontrasts(serpulid$film) &lt;- c(-1,-1,3,-1)\ncontrasts(serpulid$film)\n\n   [,1]        [,2]       [,3]\nF    -1 -0.67052910 -0.4658942\nNL   -1  0.73874075 -0.3477481\nSL    3  0.00000000  0.0000000\nUL   -1 -0.06821164  0.8136423\n\n# Refit the model with new contrasts\nserpulid.aov &lt;- aov(lserp~film, data=serpulid)\nserpulid.aov\n\nCall:\n   aov(formula = lserp ~ film, data = serpulid)\n\nTerms:\n                     film Residuals\nSum of Squares  0.2457707 0.3268840\nDeg. of Freedom         3        24\n\nResidual standard error: 0.1167055\nEstimated effects are balanced\n\nsummary.lm(serpulid.aov) \n\n\nCall:\naov(formula = lserp ~ film, data = serpulid)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.22929 -0.06500  0.01843  0.07054  0.19557 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.090393   0.022055  94.780  &lt; 2e-16 ***\nfilm1       -0.052131   0.012734  -4.094 0.000415 ***\nfilm2        0.049265   0.044111   1.117 0.275117    \nfilm3       -0.008453   0.044111  -0.192 0.849643    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1167 on 24 degrees of freedom\nMultiple R-squared:  0.4292,    Adjusted R-squared:  0.3578 \nF-statistic: 6.015 on 3 and 24 DF,  p-value: 0.003314"
  },
  {
    "objectID": "examples/serpulid.html#diagnostics-for-untransformed-data",
    "href": "examples/serpulid.html#diagnostics-for-untransformed-data",
    "title": "QK Box 6.7 & 6.11",
    "section": "Diagnostics for untransformed data",
    "text": "Diagnostics for untransformed data\nWe used log-transformed data to match the original paper, but if analysing these data from first principles, we’d look at the raw data first to decide which form of model or transformation to use.\n\nserpraw.aov &lt;- aov(serp~film, data=serpulid)\nplot(serpraw.aov)"
  },
  {
    "objectID": "examples/serpulid.html#information-for-power-analysis-using-spirorbids-bugula",
    "href": "examples/serpulid.html#information-for-power-analysis-using-spirorbids-bugula",
    "title": "QK Box 6.7 & 6.11",
    "section": "Information for power analysis using spirorbids, Bugula",
    "text": "Information for power analysis using spirorbids, Bugula\nThese calculations are used for Box 6.11, where we consider data for two other invertebrate groups, spirorbid polychaetes and bryozoans in the genus Bugula, mainly B. neritina.\n\n\n\nSpirorbid polychaete worm, c. 2mm diameter. Mick Keough \n\n\n\nRecently metamorphosed bryozoan, Bugula. Approximately 1.5 mm high. Mick Keough \n\nRequired information\nWe need to run the analysis on each of these groups, to get two important pieces of information. We need estimates of the variance, and we generally use the residual mean square. We also want an estimate of a baseline for calculating a hypothetical Effect Size. In the context of this question, we’ll use the means for unfilmed surfaces, as we are thinking about the potential for our treatments to increase recruitment.\n\nboxplot(spir~film, data=serpulid)\n\n\n\nboxplot(bugula~film, data=serpulid)\n\n\n\nspir.aov&lt;-aov(spir~film, data=serpulid)\nplot(spir.aov)\n\n\n\n\n\n\n\n\n\n\n\n\nsummary(spir.aov)\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)\nfilm         3  6.507   2.169   1.678  0.198\nResiduals   24 31.022   1.293               \n\nbugula.aov&lt;-aov(bugula~film, data=serpulid)\nplot(bugula.aov)\n\n\n\n\n\n\n\n\n\n\n\n\nsummary(bugula.aov)\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)\nfilm         3   22.8   7.587   0.338  0.798\nResiduals   24  538.3  22.429               \n\nspirmean&lt;-summarySE(data=serpulid,measurevar = \"spir\", groupvars = \"film\")\nspirmean\n\n  film N      spir        sd        se        ci\n1    F 7 0.5357143 1.0818442 0.4088987 1.0005390\n2   NL 7 0.5714286 0.9759001 0.3688556 0.9025570\n3   SL 7 1.1428571 1.0690450 0.4040610 0.9887017\n4   UL 7 1.7142857 1.3801311 0.5216405 1.2764084\n\nbugmean&lt;-summarySE(data=serpulid,measurevar = \"bugula\", groupvars = \"film\")\nbugmean\n\n  film N   bugula       sd       se       ci\n1    F 7 6.982143 5.520524 2.086562 5.105634\n2   NL 7 8.857143 4.740906 1.791894 4.384607\n3   SL 7 6.857143 4.740906 1.791894 4.384607\n4   UL 7 6.571429 3.779645 1.428571 3.495588\n\n\n\n\nPower calculations\nThere are two scenarios in Box 6.11. Both involve a doubling of settlement from the base treatment SL above. In the first scenario, one treatment is 6.86 and the others are 13.72. In the second scenario, treatment means are spaced evenly between 6.86 and 13.72.\n\nalphasq1&lt;-3*var(c(6.86, 13.72, 13.72,13.72))\nalphasq2&lt;-3*var(c(6.86, 9.14, 11.44,13.72))\nmsres=22.43\nn=7\np=4\nlambda1&lt;-n*alphasq1/msres\nlambda1\n\n[1] 11.01484\n\nlambda2&lt;-n*alphasq2/msres\nlambda2\n\n[1] 8.168685\n\nf1&lt;-sqrt(alphasq1/p/msres)\nf1\n\n[1] 0.6272059\n\nf2&lt;-sqrt(alphasq2/p/msres)\nf2\n\n[1] 0.5401285\n\n#For scenario 1, λ= 11.01 and Cohens *f* = 0.627\n#For scenario 2, λ= 8.16 and Cohens *f* = 0.54\n# scenario 1: power\npwr.anova.test(k=p,f=f1,sig.level=0.05, n=n)\n\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 4\n              n = 7\n              f = 0.6272059\n      sig.level = 0.05\n          power = 0.7298327\n\nNOTE: n is number in each group\n\n#scenario 2: power\npwr.anova.test(k=p,f=f2,sig.level=0.05, n=n)\n\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 4\n              n = 7\n              f = 0.5401285\n      sig.level = 0.05\n          power = 0.5860053\n\nNOTE: n is number in each group\n\n#scenario 1: required sample size\npwr.anova.test(k=p,f=f1,sig.level=0.05, power=0.8)\n\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 4\n              n = 7.976459\n              f = 0.6272059\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: n is number in each group\n\n#scenario 2: required sample size\npwr.anova.test(k=p,f=f2,sig.level=0.05, power=0.8)\n\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 4\n              n = 10.37355\n              f = 0.5401285\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: n is number in each group"
  },
  {
    "objectID": "examples/lowbayes.html",
    "href": "examples/lowbayes.html",
    "title": "QK Box 2.5",
    "section": "",
    "text": "This box continues with the Low et al. example starting in Box 2.2\n\nPreliminaries\nUse rstanarm and BayesFactor packages; also needs bayestestR.\nAdd bayesplot for control over plot\nLoad graphics packages (if ggplot version of figures wanted)\nNote that iso is reference group so diff between means is -ve\n\n\nUninformative priors\n\nlow &lt;- read_csv(\"../data/lowco2.csv\")\nlow1 &lt;- stan_glm(co2~anesth,family = gaussian(link = \"identity\"),data=low)\nposteriors1 &lt;- describe_posterior(low1)\n\n\nprint_md(posteriors1, digits = 2)\n\n\nSummary of Posterior Distribution\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nMedian\n95% CI\npd\nROPE\n% in ROPE\nRhat\nESS\n\n\n\n\n(Intercept)\n70.89\n[ 61.17, 81.00]\n100%\n[-1.91, 1.91]\n0%\n1.000\n3887.00\n\n\nanesthiso\n-20.83\n[-34.65, -7.01]\n99.72%\n[-1.91, 1.91]\n0%\n1.000\n3795.00\n\n\n\n\n# plot posterior distribution for all three parameters (intercept, mean diff, sigma)\nplot(low1,plotfun=\"mcmc_hist\")\n## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n# get Bayes factor for mean diff\nlowx &lt;- as.data.frame(low)\nlmBF(co2~anesth, data=lowx,posterior=FALSE)\n## Bayes factor analysis\n## --------------\n## [1] anesth : 8.039109 ±0%\n## \n## Against denominator:\n##   Intercept only \n## ---\n## Bayes factor type: BFlinearModel, JZS\n\n\n\nInformative priors\nRun three options, mean difference with high and low precision, and a bigger mean difference with high precision\n\n#for mean difference with high precision\nlow2 &lt;- stan_glm(co2~anesth,family = gaussian(link = \"identity\"),prior=normal(-25,5),data=low)\n## \n## SAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\n## Chain 1: \n## Chain 1: Gradient evaluation took 1e-05 seconds\n## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.1 seconds.\n## Chain 1: Adjust your expectations accordingly!\n## Chain 1: \n## Chain 1: \n## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\n## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\n## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\n## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\n## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\n## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\n## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\n## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\n## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\n## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\n## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\n## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\n## Chain 1: \n## Chain 1:  Elapsed Time: 0.015 seconds (Warm-up)\n## Chain 1:                0.013 seconds (Sampling)\n## Chain 1:                0.028 seconds (Total)\n## Chain 1: \n## \n## SAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\n## Chain 2: \n## Chain 2: Gradient evaluation took 3e-06 seconds\n## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds.\n## Chain 2: Adjust your expectations accordingly!\n## Chain 2: \n## Chain 2: \n## Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\n## Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\n## Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\n## Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\n## Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\n## Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\n## Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\n## Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\n## Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\n## Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\n## Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\n## Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\n## Chain 2: \n## Chain 2:  Elapsed Time: 0.016 seconds (Warm-up)\n## Chain 2:                0.013 seconds (Sampling)\n## Chain 2:                0.029 seconds (Total)\n## Chain 2: \n## \n## SAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\n## Chain 3: \n## Chain 3: Gradient evaluation took 2e-06 seconds\n## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds.\n## Chain 3: Adjust your expectations accordingly!\n## Chain 3: \n## Chain 3: \n## Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\n## Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\n## Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\n## Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\n## Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\n## Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\n## Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\n## Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\n## Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\n## Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\n## Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\n## Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\n## Chain 3: \n## Chain 3:  Elapsed Time: 0.019 seconds (Warm-up)\n## Chain 3:                0.012 seconds (Sampling)\n## Chain 3:                0.031 seconds (Total)\n## Chain 3: \n## \n## SAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\n## Chain 4: \n## Chain 4: Gradient evaluation took 2e-06 seconds\n## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds.\n## Chain 4: Adjust your expectations accordingly!\n## Chain 4: \n## Chain 4: \n## Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\n## Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\n## Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\n## Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\n## Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\n## Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\n## Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\n## Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\n## Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\n## Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\n## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\n## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\n## Chain 4: \n## Chain 4:  Elapsed Time: 0.015 seconds (Warm-up)\n## Chain 4:                0.012 seconds (Sampling)\n## Chain 4:                0.027 seconds (Total)\n## Chain 4:\nposteriors2 &lt;- describe_posterior(low2)\nprint_md(posteriors2, digits = 2)\n\n\nSummary of Posterior Distribution\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nMedian\n95% CI\npd\nROPE\n% in ROPE\nRhat\nESS\n\n\n\n\n(Intercept)\n72.30\n[ 64.37, 80.50]\n100%\n[-1.91, 1.91]\n0%\n1.002\n3239.00\n\n\nanesthiso\n-23.53\n[-31.62, -15.78]\n100%\n[-1.91, 1.91]\n0%\n1.000\n3925.00\n\n\n\n\n# informative prior for mean difference with low precision\nlow3 &lt;- stan_glm(co2~anesth,family = gaussian(link = \"identity\"),prior=normal(-25,20),data=low)\n## \n## SAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\n## Chain 1: \n## Chain 1: Gradient evaluation took 8e-06 seconds\n## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds.\n## Chain 1: Adjust your expectations accordingly!\n## Chain 1: \n## Chain 1: \n## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\n## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\n## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\n## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\n## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\n## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\n## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\n## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\n## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\n## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\n## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\n## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\n## Chain 1: \n## Chain 1:  Elapsed Time: 0.013 seconds (Warm-up)\n## Chain 1:                0.012 seconds (Sampling)\n## Chain 1:                0.025 seconds (Total)\n## Chain 1: \n## \n## SAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\n## Chain 2: \n## Chain 2: Gradient evaluation took 3e-06 seconds\n## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds.\n## Chain 2: Adjust your expectations accordingly!\n## Chain 2: \n## Chain 2: \n## Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\n## Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\n## Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\n## Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\n## Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\n## Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\n## Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\n## Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\n## Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\n## Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\n## Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\n## Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\n## Chain 2: \n## Chain 2:  Elapsed Time: 0.014 seconds (Warm-up)\n## Chain 2:                0.012 seconds (Sampling)\n## Chain 2:                0.026 seconds (Total)\n## Chain 2: \n## \n## SAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\n## Chain 3: \n## Chain 3: Gradient evaluation took 2e-06 seconds\n## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds.\n## Chain 3: Adjust your expectations accordingly!\n## Chain 3: \n## Chain 3: \n## Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\n## Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\n## Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\n## Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\n## Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\n## Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\n## Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\n## Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\n## Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\n## Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\n## Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\n## Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\n## Chain 3: \n## Chain 3:  Elapsed Time: 0.013 seconds (Warm-up)\n## Chain 3:                0.012 seconds (Sampling)\n## Chain 3:                0.025 seconds (Total)\n## Chain 3: \n## \n## SAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\n## Chain 4: \n## Chain 4: Gradient evaluation took 2e-06 seconds\n## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds.\n## Chain 4: Adjust your expectations accordingly!\n## Chain 4: \n## Chain 4: \n## Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\n## Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\n## Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\n## Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\n## Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\n## Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\n## Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\n## Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\n## Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\n## Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\n## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\n## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\n## Chain 4: \n## Chain 4:  Elapsed Time: 0.014 seconds (Warm-up)\n## Chain 4:                0.014 seconds (Sampling)\n## Chain 4:                0.028 seconds (Total)\n## Chain 4:\nposteriors3 &lt;- describe_posterior(low3)\nprint_md(posteriors3, digits = 2)\n\n\nSummary of Posterior Distribution\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nMedian\n95% CI\npd\nROPE\n% in ROPE\nRhat\nESS\n\n\n\n\n(Intercept)\n71.22\n[ 61.15, 80.74]\n100%\n[-1.91, 1.91]\n0%\n1.001\n2991.00\n\n\nanesthiso\n-21.26\n[-34.56, -8.84]\n99.98%\n[-1.91, 1.91]\n0%\n1.000\n3137.00\n\n\n\n\n# informative prior for bigger mean difference with high precision\nlow4 &lt;- stan_glm(co2~anesth,family = gaussian(link = \"identity\"),prior=normal(-50,5),data=low)\n## \n## SAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\n## Chain 1: \n## Chain 1: Gradient evaluation took 9e-06 seconds\n## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.09 seconds.\n## Chain 1: Adjust your expectations accordingly!\n## Chain 1: \n## Chain 1: \n## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\n## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\n## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\n## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\n## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\n## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\n## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\n## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\n## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\n## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\n## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\n## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\n## Chain 1: \n## Chain 1:  Elapsed Time: 0.017 seconds (Warm-up)\n## Chain 1:                0.014 seconds (Sampling)\n## Chain 1:                0.031 seconds (Total)\n## Chain 1: \n## \n## SAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\n## Chain 2: \n## Chain 2: Gradient evaluation took 3e-06 seconds\n## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds.\n## Chain 2: Adjust your expectations accordingly!\n## Chain 2: \n## Chain 2: \n## Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\n## Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\n## Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\n## Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\n## Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\n## Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\n## Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\n## Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\n## Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\n## Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\n## Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\n## Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\n## Chain 2: \n## Chain 2:  Elapsed Time: 0.018 seconds (Warm-up)\n## Chain 2:                0.012 seconds (Sampling)\n## Chain 2:                0.03 seconds (Total)\n## Chain 2: \n## \n## SAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\n## Chain 3: \n## Chain 3: Gradient evaluation took 2e-06 seconds\n## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds.\n## Chain 3: Adjust your expectations accordingly!\n## Chain 3: \n## Chain 3: \n## Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\n## Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\n## Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\n## Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\n## Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\n## Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\n## Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\n## Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\n## Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\n## Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\n## Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\n## Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\n## Chain 3: \n## Chain 3:  Elapsed Time: 0.013 seconds (Warm-up)\n## Chain 3:                0.012 seconds (Sampling)\n## Chain 3:                0.025 seconds (Total)\n## Chain 3: \n## \n## SAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\n## Chain 4: \n## Chain 4: Gradient evaluation took 2e-06 seconds\n## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds.\n## Chain 4: Adjust your expectations accordingly!\n## Chain 4: \n## Chain 4: \n## Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\n## Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\n## Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\n## Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\n## Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\n## Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\n## Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\n## Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\n## Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\n## Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\n## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\n## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\n## Chain 4: \n## Chain 4:  Elapsed Time: 0.016 seconds (Warm-up)\n## Chain 4:                0.012 seconds (Sampling)\n## Chain 4:                0.028 seconds (Total)\n## Chain 4:\nposteriors4 &lt;- describe_posterior(low4)\nprint_md(posteriors4, digits = 2)\n\n\nSummary of Posterior Distribution\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nMedian\n95% CI\npd\nROPE\n% in ROPE\nRhat\nESS\n\n\n\n\n(Intercept)\n81.90\n[ 72.65, 91.97]\n100%\n[-1.91, 1.91]\n0%\n1.001\n2681.00\n\n\nanesthiso\n-42.05\n[-51.37, -33.29]\n100%\n[-1.91, 1.91]\n0%\n1.000\n3075.00\n\n\n\n\n\n\n\nGenerate ggplot-compatible figure for mean difference posterior distribution\n\nposterior&lt;-as.array(low1)\ncolor_scheme_set(\"gray\")\np&lt;-mcmc_hist(posterior, pars = c(\"anesthiso\"))+\n  xlab(\"Mean difference\")\np\n## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n# ggsave (\"QK F2_07.pdf\", plot = p, height = ph, width = pw, units='cm')"
  },
  {
    "objectID": "examples/linton.html",
    "href": "examples/linton.html",
    "title": "QK Box 7.1",
    "section": "",
    "text": "Linton et al. (2009) studied the effects of the insecticide pyriproxyfen on ovarian development in an endemic Christmas Island land crab, Geocarcoidea natalis. The insecticide was proposed as a means of controlling numbers of an introduced ant species that was viewed as a major threat, and it is an endocrine disruptor. The experiment was designed to test whether the insecticide might pose risks to the crabs, which have a hormone similar to the one targeted in insects, and consisted of feeding crabs a mixture of leaf litter and a bait. Half of the baits contained the insecticide, and the other half were controls (bait type factor). The baits were supplied at three rates, with two levels corresponding to levels used in field applications (2 kg ha-1 and 4 kg ha-1), with the third rate being ad libitum feeding (bait dosage factor). The experimental units in this case were large plastic tubs, each containing a single female crab, and there were 7 crabs for each combination of factors. The response variable was the dry mass of the ovaries of each crab. A two-factor linear model (7.2) including the fixed main effects of bait type and bait dosage and their interaction was fitted to these data.\n\n\n\nJohn Tann from Sydney, Australia, [CC BY 2.0](https://creativecommons.org/licenses/by/2.0), via Wikimedia Commons\n\n\nHere is the paper and the data\nLinton, S., Barrow, L., Davies, C. & Harman, L. (2009). Potential endocrine disruption of ovary synthesis in the Christmas Island red crab Gecarcoidea natalis by the insecticide pyriproxyfen. Comparative Biochemistry and Physiology, Part A, 154, 289-97.\n\nPreliminaries\nLoad packages\n\nlibrary(tidyverse)\nlibrary(sjstats)\nlibrary(effectsize)\nlibrary(afex)\nlibrary(emmeans)\nlibrary(ggsci)\nlibrary(patchwork)\n\nImport linton data file (linton.csv)\n\nlinton &lt;- read.csv(\"data/linton.csv\")\nhead(linton,10)\n\n      type dosage drymass nitrogen\n1  Control      2   0.524     3620\n2  Control      2   0.535     4030\n3  Control      2   1.094     6530\n4  Control      2   0.525     3938\n5  Control      2   0.707     4312\n6  Control      2   0.551     3740\n7  Control      2   0.489     3860\n8  Control      4   0.461     4329\n9  Control      4   0.584     5108\n10 Control      4   0.715     5877\n\n\n\n\nFit model to untransformed data and check residuals\nStart with boxlplots. Too few reps for boxplot by cell so boxplot for each factor separately\n\nboxplot(drymass~type,data=linton)\n\n\n\nboxplot(drymass~dosage,data=linton)\n\n\n\nlinton.aov &lt;- aov(drymass~type*dosage, data=linton)\nplot(linton.aov)\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo strong pattern in residuals or boxplots so examine analysis with untransformed data\n\nsummary(linton.aov)\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)  \ntype         1  2.606  2.6060   7.360 0.0102 *\ndosage       2  0.671  0.3353   0.947 0.3974  \ntype:dosage  2  1.157  0.5784   1.634 0.2094  \nResiduals   36 12.747  0.3541                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nGet effect size measures (eta- and omega-squared (effectsize package)\n\neta_squared(linton.aov)\n\n# Effect Size for ANOVA (Type I)\n\nParameter   | Eta2 (partial) |       95% CI\n-------------------------------------------\ntype        |           0.17 | [0.03, 1.00]\ndosage      |           0.05 | [0.00, 1.00]\ntype:dosage |           0.08 | [0.00, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\nomega_squared(linton.aov)\n\n# Effect Size for ANOVA (Type I)\n\nParameter   | Omega2 (partial) |       95% CI\n---------------------------------------------\ntype        |             0.13 | [0.01, 1.00]\ndosage      |             0.00 | [0.00, 1.00]\ntype:dosage |             0.03 | [0.00, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\n\nInteraction plot\n\nafex_plot(linton.aov, \"type\", \"dosage\", dodge=0.05)+theme_light()\n\ndv column detected: drymass\n\n\nNo id column passed. Assuming all rows are independent samples.\n\n\n\n\n\n\n\nHigh quality figures\n\nResidual plot\n\np1 &lt;- ggplot(linton.aov, aes(x = linton.aov$fitted.values, y = linton.aov$residuals)) +\n  geom_point(color=sc) +\n  theme_classic(base_size = 10)+\n  theme(\n    axis.text = element_text(colour = ac),\n    axis.line = element_line(color = ac),\n    axis.ticks = element_line(color = ac),\n        )+labs(x = \"Predicted ovary mass\", y = \"Residuals\", \n       )\n\n\n\nInteraction plot\nUse emmeans to get dataframe of means and se\n\nemm1&lt;-emmeans(linton.aov, ~type|dosage)\nemm2&lt;-as.data.frame(emm1)\nemm2\n\ndosage = 2:\n type            emmean        SE df  lower.CL upper.CL\n Control      0.6321429 0.2249031 36 0.1760182 1.088268\n Experimental 1.1504286 0.2249031 36 0.6943039 1.606553\n\ndosage = 4:\n type            emmean        SE df  lower.CL upper.CL\n Control      0.8801429 0.2249031 36 0.4240182 1.336268\n Experimental 0.9621429 0.2249031 36 0.5060182 1.418268\n\ndosage = ad lib:\n type            emmean        SE df  lower.CL upper.CL\n Control      0.7258571 0.2249031 36 0.2697324 1.181982\n Experimental 1.6201429 0.2249031 36 1.1640182 2.076268\n\nConfidence level used: 0.95 \n\n\nMeans and error bars\n\npd=position_dodge(width=0.05)\n\np2&lt;-ggplot(emm2,aes(x=dosage,y=emmean,shape=type, group=type, color=type))+\n  geom_point(position=pd,aes(shape=type), size=3,show.legend = FALSE)+\n  geom_errorbar(aes(ymin = emmean-SE, ymax = emmean+SE), width=0, position = pd,show.legend = FALSE)+\n  geom_line(aes(color=type), position=pd, linewidth=1.5)+\n  scale_color_uchicago(labels = c(\"Control\", \"Experimental\"))+\n    scale_linetype_manual(values=c(\"solid\", \"solid\"))+\n  labs(x = \"Food level\", y = \"Hg (mg/g dw\"\n       )+\n  theme_classic(base_size = 10)+\n  theme(\n    axis.text.x = element_text(color=\"black\",size=10),\n    axis.text.y= element_text(color=ac),\n    axis.line = element_line(color = ac),\n    axis.ticks = element_line(color = ac),\n        )+\n  theme(\n  legend.position = c(.6, .95),\n  legend.justification = c(\"right\", \"top\"),\n  legend.box.just = \"right\",\n  legend.margin = margin(6, 6, 6, 6),\n  legend.title = element_blank(),\n)\n\nCombine figures\n\np1+p2"
  },
  {
    "objectID": "examples/caballes.html",
    "href": "examples/caballes.html",
    "title": "QK Box 10.6",
    "section": "",
    "text": "Caballes et al. (2016) examined the effects of maternal nutrition (three treatments: starved or fed one of two coral genera: Acropora or Porites) on the larval biology of crown-of-thorns seastars. There were three female seastars nested within each treatment, 50 larvae reared from each female were placed into each of three glass culture jars and the lengths of ten larvae from each jar after four days were measured after 4 days. This fully balanced design has maternal nutrition as a fixed factor with three random levels of nesting: females within nutrition treatment, jars within females and individual larvae within jars.\nThe paper is here\nCaballes, C. F., Pratchett, M. S., Kerr, A. M. & Rivera-Posada, J. A. (2016). The role of maternal nutrition on oocyte size and quality, with respect to early larval development in the coral-eating starfish, Acanthaster planci. PLoS One, 11, e0158007."
  },
  {
    "objectID": "examples/caballes.html#visualize-data",
    "href": "examples/caballes.html#visualize-data",
    "title": "QK Box 10.6",
    "section": "Visualize data",
    "text": "Visualize data\n\ncaballes_length %&gt;% \n  ggplot(aes(jar, length, color = female)) + \n  geom_point(alpha = 0.5) +\n  facet_wrap(~ diet, scales = \"free_x\")"
  },
  {
    "objectID": "examples/caballes.html#run-model",
    "href": "examples/caballes.html#run-model",
    "title": "QK Box 10.6",
    "section": "Run model",
    "text": "Run model\nNote: can’t get the aov commands to work for 3 level nested design\n\ncaballes.aov &lt;- aov(length~diet+Error(female/jar), caballes_length)\n\nError in qr.qty(qr.e, resp): NA/NaN/Inf in foreign function call (arg 1)\n\n\n\nFit as lm using OLS estimation\n\ncaballes.lm &lt;- lm(length ~ diet/female/jar, caballes_length)\nanova(caballes.lm)\n\nAnalysis of Variance Table\n\nResponse: length\n                 Df Sum Sq Mean Sq F value    Pr(&gt;F)    \ndiet              2 2.5334 1.26668 72.1624 &lt; 2.2e-16 ***\ndiet:female       6 0.3735 0.06224  3.5460  0.002199 ** \ndiet:female:jar  18 0.5265 0.02925  1.6664  0.045989 *  \nResiduals       243 4.2654 0.01755                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCheck diagnostics and lm output (not shown):\n\nplot(caballes.lm)\nsummary(caballes.lm)\n\nGet F and P values using correct denominators\n\n#Diet F\nf &lt;- 1.26668/0.06224\npf(f, df1 = 2, df2 = 6, lower.tail = FALSE)\n\n[1] 0.002120396\n\n#Females F\nf &lt;- 0.06224/0.02925\npf(f, df1 = 6, df2 = 18, lower.tail = FALSE)\n\n[1] 0.100229\n\n#Jars F\nf &lt;- 0.02925/0.01755\npf(f, df1 = 18, df2 = 243, lower.tail = FALSE)\n\n[1] 0.04594475\n\n\nVariance components from VCA\n\ncaballes.vca &lt;- anovaMM(length ~ diet/(female)/(jar), caballes_length)\ncaballes.vca\n\n\n\nANOVA-Type Estimation of Mixed Model:\n--------------------------------------\n\n    [Fixed Effects]\n\n        int      dietaa      dietpr dietstarved \n   0.526911    0.218889    0.188744    0.000000 \n\n\n    [Variance Components]\n\n  Name            DF         SS       MS       VC       %Total    SD      \n1 total           200.914152                   0.019823 100       0.140793\n2 diet:female     6          0.37346  0.062243 0.0011   5.547827  0.033162\n3 diet:female:jar 18         0.526523 0.029251 0.00117  5.90134   0.034202\n4 error           243        4.265431 0.017553 0.017553 88.550832 0.132489\n  CV[%]    \n1 21.242557\n2 5.003435 \n3 5.160385 \n4 19.989554\n\nMean: 0.662789 (N = 270) \n\nExperimental Design: balanced  |  Method: ANOVA\n\nVCAinference(caballes.vca, alpha = 0.05, VarVC = TRUE, ci.method = \"satterthwaite\")\n\n\n\n\nInference from Mixed Model Fit\n------------------------------\n\n&gt; VCA Result:\n-------------\n\n    [Fixed Effects]\n\n        int      dietaa      dietpr dietstarved \n     0.5269      0.2189      0.1887      0.0000 \n\n\n    [Variance Components]\n\n  Name            DF       SS     MS     VC     %Total  SD     CV[%]   Var(VC)\n1 total           200.9142               0.0198 100     0.1408 21.2426        \n2 diet:female     6        0.3735 0.0622 0.0011 5.5478  0.0332 5.0034  0      \n3 diet:female:jar 18       0.5265 0.0293 0.0012 5.9013  0.0342 5.1604  0      \n4 error           243      4.2654 0.0176 0.0176 88.5508 0.1325 19.9896 0      \n\nMean: 0.6628 (N = 270) \n\nExperimental Design: balanced  |  Method: ANOVA\n\n\n&gt; VC:\n-----\n                Estimate       DF CI LCL CI UCL One-Sided LCL One-Sided UCL\ntotal             0.0198 200.9142 0.0165 0.0244        0.0169        0.0235\ndiet:female       0.0011   1.5701 0.0003 0.1039        0.0003        0.0427\ndiet:female:jar   0.0012   2.8040 0.0004 0.0188        0.0004        0.0112\nerror             0.0176 243.0000 0.0148 0.0211        0.0152        0.0205\n\n&gt; SD:\n-----\n                Estimate       DF CI LCL CI UCL One-Sided LCL One-Sided UCL\ntotal             0.1408 200.9142 0.1283 0.1560        0.1302        0.1535\ndiet:female       0.0332   1.5701 0.0164 0.3223        0.0184        0.2066\ndiet:female:jar   0.0342   2.8040 0.0191 0.1371        0.0210        0.1058\nerror             0.1325 243.0000 0.1217 0.1454        0.1233        0.1432\n\n&gt; CV[%]:\n--------\n                Estimate       DF  CI LCL  CI UCL One-Sided LCL One-Sided UCL\ntotal            21.2426 200.9142 19.3530 23.5442       19.6423       23.1536\ndiet:female       5.0034   1.5701  2.4710 48.6259        2.7686       31.1672\ndiet:female:jar   5.1604   2.8040  2.8837 20.6905        3.1612       15.9636\nerror            19.9896 243.0000 18.3593 21.9400       18.6100       21.6107\n\n\n95% Confidence Level  \nSatterthwaite methodology used for computing CIs \n\n\n\n\nFit mixed effects model using REML/ML\n\ncaballes.lmer &lt;- lmer(length ~ diet + (1|female/jar), caballes_length)\nsummary(caballes.lmer)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: length ~ diet + (1 | female/jar)\n   Data: caballes_length\n\nREML criterion at convergence: -289.2\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.69014 -0.69659  0.01388  0.64143  2.59374 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n jar:female (Intercept) 0.00117  0.03420 \n female     (Intercept) 0.00110  0.03316 \n Residual               0.01755  0.13249 \nNumber of obs: 270, groups:  jar:female, 27; female, 9\n\nFixed effects:\n            Estimate Std. Error       df t value Pr(&gt;|t|)    \n(Intercept)  0.66279    0.01518  5.99986  43.652 9.68e-09 ***\ndiet1       -0.13588    0.02147  5.99986  -6.328 0.000728 ***\ndiet2        0.08301    0.02147  5.99986   3.866 0.008306 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n      (Intr) diet1 \ndiet1  0.000       \ndiet2  0.000 -0.500\n\n\nGet F-ratio for diet test using lmerTest\n\nanova(caballes.lmer, ddf = \"Kenward-Roger\")\n\nType III Analysis of Variance Table with Kenward-Roger's method\n      Sum Sq Mean Sq NumDF DenDF F value   Pr(&gt;F)   \ndiet 0.71442 0.35721     2     6   20.35 0.002121 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCI on variance components (remembering to square CIs from lmer which are in SD units)\n\ncaballes.ci &lt;- confint.merMod(caballes.lmer)\ncaballes.vc &lt;- (caballes.ci)^2\nprint(caballes.vc)\n\n                  2.5 %      97.5 %\n.sig01      0.000000000 0.004051007\n.sig02      0.000000000 0.003177589\n.sigma      0.014769187 0.021083239\n(Intercept) 0.404053917 0.475997080\ndiet1       0.030364676 0.009506432\ndiet2       0.001992217 0.014735036"
  },
  {
    "objectID": "examples/caballes.html#simplify-dataset-by-averaging-across-larvae-within-a-jar",
    "href": "examples/caballes.html#simplify-dataset-by-averaging-across-larvae-within-a-jar",
    "title": "QK Box 10.6",
    "section": "Simplify dataset by averaging across larvae within a jar",
    "text": "Simplify dataset by averaging across larvae within a jar\n\nd &lt;- caballes_length %&gt;% \n  group_by(diet, female, jar) %&gt;%\n  dplyr::summarise(mean = mean(length), \n            sd = sd(length), \n            n = n(), \n            se = sd / sqrt(n)) %&gt;% \n  ungroup()\n\nd %&gt;% \n  ggplot(aes(female, mean)) + \n  geom_point(alpha = 0.5) +\n  facet_wrap(~ diet, scales = \"free_x\") + \n  labs(y = \"Mean length (mm)\")\n\n\n\n\n\nFit nested ANOVA with OLS\nFemale is nested within treatment:\n\nd_aov &lt;- aov(mean~diet+Error(female), d)\nsummary(d_aov)\n\n\nError: female\n          Df  Sum Sq Mean Sq F value  Pr(&gt;F)   \ndiet       2 0.25334 0.12667   20.35 0.00212 **\nResiduals  6 0.03735 0.00622                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nError: Within\n          Df  Sum Sq  Mean Sq F value Pr(&gt;F)\nResiduals 18 0.05265 0.002925               \n\nm1 &lt;- lm(mean ~ diet / female, d)\nanova(m1)\n\nAnalysis of Variance Table\n\nResponse: mean\n            Df   Sum Sq  Mean Sq F value    Pr(&gt;F)    \ndiet         2 0.253336 0.126668 43.3035 1.323e-07 ***\ndiet:female  6 0.037346 0.006224  2.1279    0.1002    \nResiduals   18 0.052652 0.002925                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nGet F and P values using correct denominators. Note that I’m using broom:tidy to tidy the anova output, and lead() to calculate the new_F and new_P. This avoids extracting statistic (F-value) and df into new objects, or hard-coding the calculations.\n\ntidy(anova(m1)) %&gt;% \n  mutate(new_F = meansq / lead(meansq), \n         new_P = pf(new_F, df1 = df, df2 = lead(df), lower.tail = FALSE)) %&gt;% \n  kable(digits = 3)\n\n\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\nnew_F\nnew_P\n\n\n\n\ndiet\n2\n0.253\n0.127\n43.303\n0.0\n20.351\n0.002\n\n\ndiet:female\n6\n0.037\n0.006\n2.128\n0.1\n2.128\n0.100\n\n\nResiduals\n18\n0.053\n0.003\nNA\nNA\nNA\nNA\n\n\n\n\n\n\n\nFit mixed effects model using REML/ML\n\nm2 &lt;- lmer(mean ~ diet + (1|female), d)\nsummary(m2)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: mean ~ diet + (1 | female)\n   Data: d\n\nREML criterion at convergence: -60.8\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.05994 -0.46141  0.08908  0.48149  2.22410 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n female   (Intercept) 0.001100 0.03316 \n Residual             0.002925 0.05408 \nNumber of obs: 27, groups:  female, 9\n\nFixed effects:\n            Estimate Std. Error      df t value Pr(&gt;|t|)    \n(Intercept)  0.52691    0.02630 6.00000  20.036    1e-06 ***\ndietaa       0.21889    0.03719 6.00000   5.886  0.00107 ** \ndietpr       0.18874    0.03719 6.00000   5.075  0.00228 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n       (Intr) dietaa\ndietaa -0.707       \ndietpr -0.707  0.500\n\nanova(m2, ddf = \"Kenward-Roger\")\n\nType III Analysis of Variance Table with Kenward-Roger's method\n      Sum Sq  Mean Sq NumDF DenDF F value   Pr(&gt;F)   \ndiet 0.11906 0.059528     2     6   20.35 0.002121 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "chapter_notes/chapter3.html",
    "href": "chapter_notes/chapter3.html",
    "title": "Chapter 3",
    "section": "",
    "text": "Questions for review and discussion, based on chapter 3 from Quinn and Keough 2023."
  },
  {
    "objectID": "chapter_notes/chapter2.html",
    "href": "chapter_notes/chapter2.html",
    "title": "Chapter 2",
    "section": "",
    "text": "Questions for review and discussion, based on chapter 2 from Quinn and Keough 2023."
  },
  {
    "objectID": "chapter_notes/chapter2.html#section",
    "href": "chapter_notes/chapter2.html#section",
    "title": "Chapter 2",
    "section": "2.1",
    "text": "2.1\nWhat is a sample?\nWhat is the difference between a statistic and a parameter?\nWhat is the difference between process and observation uncertainty?"
  },
  {
    "objectID": "chapter_notes/chapter2.html#section-1",
    "href": "chapter_notes/chapter2.html#section-1",
    "title": "Chapter 2",
    "section": "2.2",
    "text": "2.2\nDefine the following terms:\n\nprobability\nsample space\nconditional probability\n\nDraw a Venn diagram representing the probability of three outcomes (A, B) in a sample space. Let C be mutually exclusive of A and B, but allow A and B to overlap. Use this diagram to visualize the idea of conditional probability, and relate it to the the mathematical equation for conditional probability."
  },
  {
    "objectID": "chapter_notes/chapter2.html#section-2",
    "href": "chapter_notes/chapter2.html#section-2",
    "title": "Chapter 2",
    "section": "2.3",
    "text": "2.3\nDefine the following terms:\n\nrandom variable\ndiscrete variable\ncontinuous variable\nprobability distribution\nprobability mass function\nprobability density function\n\nImagine you have counted rockfish along transects in the kelp forest. You are an avid diver, so you completed 1000 transects. Because these are count data, we will use a Poisson distribution (the support for the Poisson is non-negative integers). The average number of fish is 2 per transect. Note that in the Poisson, one parameter (\\(\\lambda\\)), governs the central tendency and the spread of the distribution (unlike, e.g., the Normal). This means the single parameter of the Poisson distribution is \\(\\lambda = 2\\).\nHere, we generate data according to our data story:\n\nset.seed(101)\nx &lt;- rpois(n = 1000, lambda = 2)\nhist(x, xlab = \"Rockfish per transect\",\n           main = expression(paste(\"1000 samples from a Poisson(\", lambda, \" = 2)\")))\n\n\n\n\nNow run these two lines of code. Why do these two expressions give you the same answer? (To force you to dig into the help files for these functions, I am not writing explicit code).\n\ndpois(0, 2) + dpois(1, 2) + dpois(2, 2)\nppois(2, 2)\n\nThe above code tells us that ~67% of the probability mass of a Poisson(2) distribution lies at or below 2. Returning to our random samples, let’s use quantile to figure out which of our values is at the 67th percentile in our data:\n\nquantile(x = x, probs = 0.67)\n\nNow let’s use qpois to identify the value at which we have 67% of the observations:\n\nqpois(p = 0.67, lambda = 2)\n\nFinally, let’s just tabulate the data to see if this all makes sense:\n\ntable(x)\nsum(table(x)[1:3]) / 1000\n\nPonder all of this until your understanding of the inter-relationships between d, p, q, r - pois is solid.\n\nPlotting distributions\nLet’s plot a Normal distribution centered at 0, with different standard deviations:\n\nx_grid &lt;- seq(-4, 4, 0.01)\n\nplot(x_grid, dnorm(x_grid, mean = 0, sd = 0.5), type = \"l\", \n     col = \"red\", xlab = \"x\", ylab = \"probability density\")\n\nlines(x_grid, dnorm(x_grid, mean = 0, sd = 1), type = \"l\", \n     col = \"blue\")\n\nlines(x_grid, dnorm(x_grid, mean = 0, sd = 1.5), type = \"l\", \n     col = \"black\")\n\n\n\n\nLet’s compare with a Student t-distribution:\n\nx_grid &lt;- seq(-3, 3, 0.01)\n\nplot(x_grid, dt(x_grid, df = 40), type = \"l\", \n     col = \"red\", xlab = \"x\", ylab = \"probability density\")\n\nlines(x_grid, dt(x_grid, df = 20), type = \"l\", \n     col = \"blue\")\n\nlines(x_grid, dt(x_grid, df = 10), type = \"l\", \n     col = \"black\")\n\nlines(x_grid, dnorm(x_grid, mean = 0, sd = 1), type = \"l\", \n     col = \"gray\", lty = 2)\n\n\n\n\nThe difference appears to be small. But the ‘thicker tails’ of the t-distribution dramatically increases the probability of extreme events, so-called black swans (e.g., in animal populations).\n\n\nMore practice\n\nFind the mean, variance, and 95% quantiles (i.e., 2.5% and 97.5% quantiles) of 1000 random draws from a Poisson distribution with \\(\\lambda=33\\).\nWhat is the probability \\(\\text P (X \\leq 6)\\) that a random draw from a Poisson distribution with \\(\\lambda = 4\\) will be less than or equal to 6?\nWhat is the probability \\(\\text P(X = 3)\\) of obtaining a value of 3 from a Binomial distribution with \\(p = 0.3\\) and \\(n = 5\\)?\nWhat is the probability \\(\\text P(-1.5 \\leq X \\leq 1.5)\\) that a value drawn from a standard normal distribution will be between -1.5 and 1.5? It may help to approach this visually.\nFind the value \\(x\\) that satisfies to \\(\\text P(X \\leq x) = 0.8\\), if \\(X\\) is a Gamma random variable with \\(k=2\\) and \\(\\theta = 1\\)."
  },
  {
    "objectID": "chapter_notes/chapter2.html#section-3",
    "href": "chapter_notes/chapter2.html#section-3",
    "title": "Chapter 2",
    "section": "2.4",
    "text": "2.4\nWhat is estimation?\nWhat makes a good estimator?\nCompare and contrast these frequentist estimation methods:\n\nordinary least squares\nmaximum likelihood\nresampling (bootstrap)\n\nPopulation parameters and sample statistics\n\nBox 2.2\nWhat is a sampling distribution?\nWhat does the central limit theorem tell us about the shape of a sampling distribution (e.g., a distribution of sample means)?\nWhat happens to the standard error of the mean as you increase sample size?\nWhen calculating the confidence interval for \\(\\mu\\), should you use a Normal distribution or a \\(t\\) distribution? Why? How do these two approaches compare?\nIn the frequentist world, parameters are fixed (but unknowable). In this context, how do you interpret a frequentist confidence interval?\nBootstrap methods and Box 2.3"
  },
  {
    "objectID": "chapter_notes/chapter2.html#section-4",
    "href": "chapter_notes/chapter2.html#section-4",
    "title": "Chapter 2",
    "section": "2.5",
    "text": "2.5\nHypothesis testing"
  },
  {
    "objectID": "chapter_notes/chapter2.html#section-5",
    "href": "chapter_notes/chapter2.html#section-5",
    "title": "Chapter 2",
    "section": "2.6",
    "text": "2.6\nComments on frequentist inference"
  },
  {
    "objectID": "chapter_notes/chapter2.html#section-6",
    "href": "chapter_notes/chapter2.html#section-6",
    "title": "Chapter 2",
    "section": "2.7",
    "text": "2.7\nBayesian inference"
  },
  {
    "objectID": "examples/low.html",
    "href": "examples/low.html",
    "title": "QK Box 2.2",
    "section": "",
    "text": "Low et al (2016) examined the effects of two different anesthetics on aspects of the physiology of the mouse. Twelve mice were anesthetized with isoflurane and eleven mice were anesthetized with alpha chloralose and blood CO2 levels were recorded after 120 minutes. The H0 was that there is no difference between the anesthetics in the mean blood CO2 level. This is an independent comparison because individual mice were only given one of the two anesthetics."
  },
  {
    "objectID": "examples/low.html#preliminaries",
    "href": "examples/low.html#preliminaries",
    "title": "QK Box 2.2",
    "section": "Preliminaries",
    "text": "Preliminaries\nFirst, load the required packages (tidyverse, RMisc, MKinfer, car, emmeans)\nImport low data file\n\nlow &lt;- read.csv(\"data/lowco2.csv\")\nlow\n\n   anesth co2\n1     iso  43\n2     iso  35\n3     iso  50\n4     iso  39\n5     iso  56\n6     iso  54\n7     iso  39\n8     iso  51\n9     iso  49\n10    iso  54\n11    iso  51\n12    iso  79\n13     ac  60\n14     ac  53\n15     ac  54\n16     ac  73\n17     ac  64\n18     ac  95\n19     ac  57\n20     ac  80\n21     ac 115\n22     ac  79\n23     ac  50"
  },
  {
    "objectID": "examples/low.html#get-summary-statistics-by-anesthetic",
    "href": "examples/low.html#get-summary-statistics-by-anesthetic",
    "title": "QK Box 2.2",
    "section": "Get summary statistics by anesthetic",
    "text": "Get summary statistics by anesthetic\n\nlow_stats &lt;- summarySE(data=low,measurevar=\"co2\", groupvars=\"anesth\")\nlow_stats\n\n  anesth  N      co2       sd       se        ci\n1     ac 11 70.90909 20.20126 6.090909 13.571391\n2    iso 12 50.00000 11.39378 3.289100  7.239261\n\nlow %&gt;% dplyr::count(anesth)\n\n  anesth  n\n1     ac 11\n2    iso 12\n\nlow %&gt;%  \n  group_by(anesth) %&gt;% \n  dplyr::summarise(n = n(), \n            mean = mean(co2),\n            median = median(co2),\n            sd = sd(co2), \n            variance = var(co2), \n            se = sd / sqrt(n), \n            CI_upper = mean + se * qt(p = 0.975, df = n-1), \n            CI_lower = mean + se * qt(p = 0.025, df = n-1), \n            CI = se * qt(p = 0.975, df = n-1), \n            upper = mean + CI, \n            lower = mean - CI\n            )\n\n# A tibble: 2 × 12\n  anesth     n  mean median    sd variance    se CI_upper CI_lower    CI upper\n  &lt;chr&gt;  &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 ac        11  70.9   64    20.2     408.  6.09     84.5     57.3 13.6   84.5\n2 iso       12  50     50.5  11.4     130.  3.29     57.2     42.8  7.24  57.2\n# ℹ 1 more variable: lower &lt;dbl&gt;\n\n\nPlay around with df to see how the z-multiplier changes when using the T-distribution to calculate the 95% confidence interval.\n\n# Standard normal distribution\nqnorm(p = 0.025)\n\n[1] -1.959964\n\nqnorm(p = 0.975)\n\n[1] 1.959964\n\n# Student-t distribution\nqt(p = 0.025, df = 100)\n\n[1] -1.983972\n\nqt(p = 0.975, df = 100)\n\n[1] 1.983972"
  },
  {
    "objectID": "examples/low.html#plot-data",
    "href": "examples/low.html#plot-data",
    "title": "QK Box 2.2",
    "section": "Plot data",
    "text": "Plot data\n\nlow %&gt;% \n  ggplot(aes(anesth, co2)) + \n  geom_point(alpha = 0.5) + \n  theme_qk()"
  },
  {
    "objectID": "examples/low.html#fit-model-and-get-effect-size",
    "href": "examples/low.html#fit-model-and-get-effect-size",
    "title": "QK Box 2.2",
    "section": "Fit model and get effect size",
    "text": "Fit model and get effect size\n\nlow.aov &lt;- aov(co2~anesth,data=low)\ntidy(low.aov, conf.int=TRUE)\n\n# A tibble: 2 × 6\n  term         df sumsq meansq statistic  p.value\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 anesth        1 2509.  2509.      9.56  0.00552\n2 Residuals    21 5509.   262.     NA    NA      \n\nlow.emm &lt;- emmeans(low.aov,\"anesth\")\neff_size(low.emm, sigma=sigma(low.aov), edf=df.residual(low.aov))\n\n contrast effect.size    SE df lower.CL upper.CL\n ac - iso        1.29 0.463 21    0.329     2.25\n\nsigma used for effect sizes: 16.2 \nConfidence level used: 0.95 \n\n\nNote that we’ve chosen to show a standardized effect size, using the pooled variance from the analysis of variance - Residual MS = 262.44, and √262.44 = 16.2"
  },
  {
    "objectID": "examples/low.html#test-variances",
    "href": "examples/low.html#test-variances",
    "title": "QK Box 2.2",
    "section": "Test variances",
    "text": "Test variances\n\nleveneTest(co2 ~ anesth, low)\n\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(&gt;F)\ngroup  1   2.604 0.1215\n      21               \n\n\n\nt-test for equal variances\n\nt.test(co2~anesth,var.equal=TRUE, data=low)\n\n\n    Two Sample t-test\n\ndata:  co2 by anesth\nt = 3.0927, df = 21, p-value = 0.005515\nalternative hypothesis: true difference in means between group ac and group iso is not equal to 0\n95 percent confidence interval:\n  6.849172 34.969010\nsample estimates:\n mean in group ac mean in group iso \n         70.90909          50.00000 \n\n\n\n\nt-test for separate variances\n\nt.test(co2~anesth,data=low)\n\n\n    Welch Two Sample t-test\n\ndata:  co2 by anesth\nt = 3.0206, df = 15.485, p-value = 0.008362\nalternative hypothesis: true difference in means between group ac and group iso is not equal to 0\n95 percent confidence interval:\n  6.194866 35.623316\nsample estimates:\n mean in group ac mean in group iso \n         70.90909          50.00000"
  },
  {
    "objectID": "examples/low.html#wilcoxon-mann-whitney",
    "href": "examples/low.html#wilcoxon-mann-whitney",
    "title": "QK Box 2.2",
    "section": "Wilcoxon-Mann-Whitney",
    "text": "Wilcoxon-Mann-Whitney\n\nwilcox.test(co2~anesth,data=low)\n\nWarning in wilcox.test.default(x = DATA[[1L]], y = DATA[[2L]], ...): cannot\ncompute exact p-value with ties\n\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  co2 by anesth\nW = 114, p-value = 0.003398\nalternative hypothesis: true location shift is not equal to 0\n\nsum(rank(low$co2)[low$anesth==\"ac\"])\n\n[1] 180\n\nsum(rank(low$co2)[low$anesth==\"iso\"])\n\n[1] 96"
  },
  {
    "objectID": "examples/lowboot.html",
    "href": "examples/lowboot.html",
    "title": "QK Box 2.3",
    "section": "",
    "text": "This box continues with the Low et al. anesthetic example from Box 2.2\n\nPreliminaries\nPackages: MKinfer, resample\n\nlibrary(MKinfer)\nlibrary(resample)\nlibrary(tidyverse)\n\nUse low data:\n\nlow &lt;- read_csv(\"data/lowco2.csv\")\n\n\n\nGet jackknife SE for two groups\n\nlow1 &lt;- subset(low,anesth==\"iso\")\njackknife(low1$co2,mean)\n## Call:\n## jackknife(data = low1$co2, statistic = mean)\n## Replications: 12\n## \n## Summary Statistics:\n##      Observed     SE Mean Bias\n## mean       50 3.2891   50    0\nlow2 &lt;- subset(low,anesth==\"ac\")\njackknife(low2$co2,mean)\n## Call:\n## jackknife(data = low2$co2, statistic = mean)\n## Replications: 11\n## \n## Summary Statistics:\n##      Observed       SE     Mean         Bias\n## mean 70.90909 6.090909 70.90909 1.421085e-13\n\n\n\nGet bootstrap SE and 95%CI\n\nlow1boot &lt;- bootstrap(low1$co2,mean,R=9999)\nlow1boot\n## Call:\n## bootstrap(data = low1$co2, statistic = mean, R = 9999)\n## Replications: 9999\n## \n## Summary Statistics:\n##      Observed       SE    Mean       Bias\n## mean       50 3.188776 50.0104 0.01040104\nCI.percentile(low1boot, probs=c(0.025,0.975))\n##      2.5% 97.5%\n## mean 43.5    58\nCI.bca(low1boot, probs=c(0.025,0.975))\n##          2.5%    97.5%\n## mean 44.33333 60.16667\n\nlow2boot &lt;- bootstrap(low2$co2,mean,R=9999)\nlow2boot\n## Call:\n## bootstrap(data = low2$co2, statistic = mean, R = 9999)\n## Replications: 9999\n## \n## Summary Statistics:\n##      Observed       SE     Mean        Bias\n## mean 70.90909 5.860237 70.89403 -0.01505605\nCI.percentile(low2boot, probs=c(0.025,0.975))\n##          2.5%    97.5%\n## mean 58.72727 86.16145\nCI.bca(low2boot, probs=c(0.025,0.975))\n##      2.5%    97.5%\n## mean   60 88.36364\n\n\n\nGet bootstrap SE and CI on difference\n\nlowboot &lt;- bootstrap2(low$co2,mean,treatment=low$anesth,R=9999,ratio=FALSE)\nlowboot\n## Call:\n## bootstrap2(data = low$co2, statistic = mean, treatment = low$anesth, \n##     R = 9999, ratio = FALSE)\n## Replications: 9999\n## Two samples, sample sizes are 11 12\n## \n## Summary Statistics for the difference between samples 1 and 2:\n##              Observed       SE     Mean       Bias\n## mean: ac-iso 20.90909 6.492549 20.91318 0.00408753\nCI.percentile(lowboot, probs=c(0.025,0.975))\n##                  2.5%    97.5%\n## mean: ac-iso 6.212121 37.08751\n\n\n\nRandomization test\n\nperm.t.test(co2~anesth, data=low, R=9999, paired= FALSE)\n## \n##  Permutation Welch Two Sample t-test\n## \n## data:  co2 by anesth\n## (Monte-Carlo) permutation p-value = 0.0038 \n## permutation difference of means (SE) = 20.9089 (7.966646) \n## 95 percent (Monte-Carlo) permutation percentile confidence interval:\n##   5.924242 36.251136\n## \n## Results without permutation:\n## t = 3.0206, df = 15.485, p-value = 0.008362\n## alternative hypothesis: true difference in means is not equal to 0\n## 95 percent confidence interval:\n##   6.194866 35.623316\n## sample estimates:\n##  mean in group ac mean in group iso \n##          70.90909          50.00000"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "qk2e",
    "section": "",
    "text": "This website is a space for working through Quinn and Keough’s 2nd edition textbook on data analysis for biologists. The Chapter notes are intended for review and discussion in a seminar (Oceans 200). The Slides are intended as a short primer on the topic by the discussion leader. The Examples are reproduced and lightly modified from the original worked examples.\nOceans 200 schedule\n\n\n\n\n\n\n\n\n\n\n\nWeek\nDate\nTopic\nQK chapter\nPre-class box example\nIn-class box example\n\n\n\n\n1\nJan. 17th 2023\nIntro to seminar and review of intro stats\n2\n2.2\n\n\n\n2\nJan. 10th 2023\nSampling and experimental design\n3\n\n\n\n\n3\nJan. 24th 2023\nOne-way designs\n6\n6.7\n6.11\n\n\n4\nJan. 31st 2023\nFactorial designs\n7\n7.1\n7.3\n\n\n5\nFeb. 7th 2023\nNested designs\n10\n10.6\n10.7\n\n\n6\nFeb 14th 2023\nSplit-plot designs\n11\n11.1\n11.2\n\n\n7\nFeb. 21st 2023\nRepeated measures designs\n12\n12.1\n12.2\n\n\n8\nFeb. 28th 2023\nStudents’ choice / presentations\ntbd\n\n\n\n\n9\nMar. 6th 2023\nStudents’ choice / presentations\ntbd\n\n\n\n\n10\nMar. 13th 2023\nStudents’ choice / presentations\ntbd"
  },
  {
    "objectID": "slides/lm_one_way.html#outline",
    "href": "slides/lm_one_way.html#outline",
    "title": "Linear models: one-way designs",
    "section": "Outline",
    "text": "Outline\n\nIntro to linear models (board)\nVisualizing ANOVA (board)\nR examples (slides)"
  },
  {
    "objectID": "slides/lm_one_way.html#examples-from-qk2e-chapter-6",
    "href": "slides/lm_one_way.html#examples-from-qk2e-chapter-6",
    "title": "Linear models: one-way designs",
    "section": "Examples from QK2E chapter 6",
    "text": "Examples from QK2E chapter 6\n\nBox 6.1 (regression; continuous predictor)\n\nDiscussion\n\nBox 6.7 (ANOVA; categorical predictor)\n\nDiscussion"
  },
  {
    "objectID": "slides/lm_one_way.html#box-6.1",
    "href": "slides/lm_one_way.html#box-6.1",
    "title": "Linear models: one-way designs",
    "section": "Box 6.1",
    "text": "Box 6.1\nChristensen et al. (1996) studied the relationships between coarse woody debris (CWD) and shoreline vegetation and lake development in a sample of 16 lakes in North America. The main variables of interest:\n\ndensity of cabins (no. km−1)\ndensity of riparian trees (trees km−1)\nbasal area of riparian trees (m2 km−1)\ndensity of coarse woody debris (no. km−1)\nbasal area of coarse woody debris (m2 km−1)"
  },
  {
    "objectID": "slides/lm_one_way.html#visualize-data",
    "href": "slides/lm_one_way.html#visualize-data",
    "title": "Linear models: one-way designs",
    "section": "Visualize data",
    "text": "Visualize data"
  },
  {
    "objectID": "slides/lm_one_way.html#linear-model",
    "href": "slides/lm_one_way.html#linear-model",
    "title": "Linear models: one-way designs",
    "section": "Linear model",
    "text": "Linear model\n\nm1 &lt;- lm(cwdbasal ~ ripdens, data = d)\nsummary(m1)\n\n\nCall:\nlm(formula = cwdbasal ~ ripdens, data = d)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-38.62 -22.41 -13.33  26.16  61.35 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -77.09908   30.60801  -2.519 0.024552 *  \nripdens       0.11552    0.02343   4.930 0.000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 36.32 on 14 degrees of freedom\nMultiple R-squared:  0.6345,    Adjusted R-squared:  0.6084 \nF-statistic:  24.3 on 1 and 14 DF,  p-value: 0.0002216"
  },
  {
    "objectID": "slides/lm_one_way.html#plot-predicted-values-vs-residuals",
    "href": "slides/lm_one_way.html#plot-predicted-values-vs-residuals",
    "title": "Linear models: one-way designs",
    "section": "Plot predicted values vs residuals",
    "text": "Plot predicted values vs residuals"
  },
  {
    "objectID": "slides/lm_one_way.html#discussion",
    "href": "slides/lm_one_way.html#discussion",
    "title": "Linear models: one-way designs",
    "section": "Discussion",
    "text": "Discussion\n\nThoughts on the reading (6.1; continuous predictor)\nThis material is foundational. You’ve probably done a linear regression before. If you were teaching an undergrad, what is a key point you would emphasize?"
  },
  {
    "objectID": "slides/lm_one_way.html#box-6.7",
    "href": "slides/lm_one_way.html#box-6.7",
    "title": "Linear models: one-way designs",
    "section": "Box 6.7",
    "text": "Box 6.7\nKeough and Raimondi (1995) set up an experiment to examine the response of serpulid (polychaete worms) larvae to four types of biofilms on hard substrata in shallow marine waters. The four treatments were:\n\nF: field substrata (with a net, to exclude other invertebrates)\nNL: netted substrata developed in the lab\nUL: un-netted substrata developed in the lab\nSL: sterile substrata in the lab, without a net"
  },
  {
    "objectID": "slides/lm_one_way.html#visualize-data-1",
    "href": "slides/lm_one_way.html#visualize-data-1",
    "title": "Linear models: one-way designs",
    "section": "Visualize data",
    "text": "Visualize data"
  },
  {
    "objectID": "slides/lm_one_way.html#linear-model-1",
    "href": "slides/lm_one_way.html#linear-model-1",
    "title": "Linear models: one-way designs",
    "section": "Linear model",
    "text": "Linear model\n\nm1 &lt;- lm(lserp ~ film, data = d)\nsummary(m1)\n\n\nCall:\nlm(formula = lserp ~ film, data = d)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.22929 -0.06500  0.01843  0.07054  0.19557 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.11343    0.04411  47.912  &lt; 2e-16 ***\nfilmNL       0.06843    0.06238   1.097  0.28356    \nfilmSL      -0.17943    0.06238  -2.876  0.00831 ** \nfilmUL       0.01886    0.06238   0.302  0.76504    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1167 on 24 degrees of freedom\nMultiple R-squared:  0.4292,    Adjusted R-squared:  0.3578 \nF-statistic: 6.015 on 3 and 24 DF,  p-value: 0.003314"
  },
  {
    "objectID": "slides/lm_one_way.html#set-your-reference-level",
    "href": "slides/lm_one_way.html#set-your-reference-level",
    "title": "Linear models: one-way designs",
    "section": "Set your reference level",
    "text": "Set your reference level\n\nd &lt;- d %&gt;% mutate(film_ = fct_relevel(film, \"SL\", \"UL\", \"NL\", \"F\"))\nglimpse(d$film); glimpse(d$film_)\n\n Factor w/ 4 levels \"F\",\"NL\",\"SL\",..: 3 3 3 3 3 3 3 4 4 4 ...\n\n\n Factor w/ 4 levels \"SL\",\"UL\",\"NL\",..: 1 1 1 1 1 1 1 2 2 2 ..."
  },
  {
    "objectID": "slides/lm_one_way.html#linear-model-take-2",
    "href": "slides/lm_one_way.html#linear-model-take-2",
    "title": "Linear models: one-way designs",
    "section": "Linear model, take 2",
    "text": "Linear model, take 2\n\nm2 &lt;- lm(lserp ~ film_, data = d)\nsummary(m2)\n\n\nCall:\nlm(formula = lserp ~ film_, data = d)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.22929 -0.06500  0.01843  0.07054  0.19557 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.93400    0.04411  43.844  &lt; 2e-16 ***\nfilm_UL      0.19829    0.06238   3.179 0.004045 ** \nfilm_NL      0.24786    0.06238   3.973 0.000564 ***\nfilm_F       0.17943    0.06238   2.876 0.008310 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1167 on 24 degrees of freedom\nMultiple R-squared:  0.4292,    Adjusted R-squared:  0.3578 \nF-statistic: 6.015 on 3 and 24 DF,  p-value: 0.003314"
  },
  {
    "objectID": "slides/lm_one_way.html#null-model",
    "href": "slides/lm_one_way.html#null-model",
    "title": "Linear models: one-way designs",
    "section": "Null model",
    "text": "Null model\n\nm0 &lt;- lm(lserp ~ 1, data = d)\nsummary(m0)\n\n\nCall:\nlm(formula = lserp ~ 1, data = d)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.31239 -0.11064  0.03261  0.10961  0.21861 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.09039    0.02752   75.95   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1456 on 27 degrees of freedom"
  },
  {
    "objectID": "slides/lm_one_way.html#comparing-linear-models",
    "href": "slides/lm_one_way.html#comparing-linear-models",
    "title": "Linear models: one-way designs",
    "section": "Comparing linear models",
    "text": "Comparing linear models\n\nanova(m0, m1)\n\nAnalysis of Variance Table\n\nModel 1: lserp ~ 1\nModel 2: lserp ~ film\n  Res.Df     RSS Df Sum of Sq      F   Pr(&gt;F)   \n1     27 0.57265                                \n2     24 0.32688  3   0.24577 6.0149 0.003314 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nanova(m1)\n\nAnalysis of Variance Table\n\nResponse: lserp\n          Df  Sum Sq  Mean Sq F value   Pr(&gt;F)   \nfilm       3 0.24577 0.081924  6.0149 0.003314 **\nResiduals 24 0.32688 0.013620                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "slides/lm_one_way.html#planned-contrasts",
    "href": "slides/lm_one_way.html#planned-contrasts",
    "title": "Linear models: one-way designs",
    "section": "Planned contrasts",
    "text": "Planned contrasts\nDefine a linear combination of the p means that represent the comparison of interest\n\\[\n    c_1 \\bar{y}_1 + ... + c_i \\bar{y}_i + c_p \\bar{y}_p\n\\]\nwhere \\(c_i\\) represent contrast coefficients and sum to zero, and \\(\\bar{y}_i\\) are group means.\n\nTo omit a group, give it a coefficient of zero.\n\n\nVery cool: you can do contrasts for polynomial trends with group means! (see Box 6.4)"
  },
  {
    "objectID": "slides/lm_one_way.html#ul-vs-nl",
    "href": "slides/lm_one_way.html#ul-vs-nl",
    "title": "Linear models: one-way designs",
    "section": "UL vs NL",
    "text": "UL vs NL\n\n# Default\ncontrasts(d$film)\n\n   NL SL UL\nF   0  0  0\nNL  1  0  0\nSL  0  1  0\nUL  0  0  1\n\n\n\n\n# Change it\ncontrasts(d$film) &lt;- c(0, 1, 0, -1)\ncontrasts(d$film)\n\n   [,1]       [,2]       [,3]\nF     0 -0.5000000 -0.7071068\nNL    1 -0.1666667  0.4714045\nSL    0  0.8333333 -0.2357023\nUL   -1 -0.1666667  0.4714045"
  },
  {
    "objectID": "slides/lm_one_way.html#ul-vs-nl-1",
    "href": "slides/lm_one_way.html#ul-vs-nl-1",
    "title": "Linear models: one-way designs",
    "section": "UL vs NL",
    "text": "UL vs NL\n\n# Refit the model with new contrasts\nserpulid.aov &lt;- aov(lserp ~ film, data = d)\nsummary(serpulid.aov)\n\n            Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nfilm         3 0.2458 0.08192   6.015 0.00331 **\nResiduals   24 0.3269 0.01362                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nNote that the aov results have not changed."
  },
  {
    "objectID": "slides/lm_one_way.html#ul-vs-nl-2",
    "href": "slides/lm_one_way.html#ul-vs-nl-2",
    "title": "Linear models: one-way designs",
    "section": "UL vs NL",
    "text": "UL vs NL\n\ntidy(summary.lm(serpulid.aov))\n\n# A tibble: 4 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   2.09      0.0221    94.8   2.07e-32\n2 film1         0.0248    0.0312     0.795 4.35e- 1\n3 film2        -0.164     0.0441    -3.72  1.07e- 3\n4 film3         0.0834    0.0441     1.89  7.07e- 2\n\n\n\nTo evaluate the contrast, look at the statistic and p-value for film1"
  },
  {
    "objectID": "slides/lm_one_way.html#f-vs-average-nl-ul",
    "href": "slides/lm_one_way.html#f-vs-average-nl-ul",
    "title": "Linear models: one-way designs",
    "section": "F vs average (NL & UL)",
    "text": "F vs average (NL & UL)\n\ncontrasts(d$film) &lt;- c(2,-1,0,-1)\ncontrasts(d$film)\n\n   [,1]       [,2]        [,3]\nF     2 -0.2867757  0.03306064\nNL   -1 -0.3677574 -0.66939360\nSL    0  0.8603272 -0.09918192\nUL   -1 -0.2057940  0.73551488\n\n\n\n\nserpulid.aov &lt;- aov(lserp ~ film, data = d)\ntidy(summary.lm(serpulid.aov))\n\n# A tibble: 4 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   2.09      0.0221    94.8   2.07e-32\n2 film1        -0.0145    0.0180    -0.808 4.27e- 1\n3 film2        -0.183     0.0441    -4.16  3.53e- 4\n4 film3        -0.0141    0.0441    -0.321 7.51e- 1"
  },
  {
    "objectID": "slides/lm_one_way.html#sl-vs-average-f-nl-ul",
    "href": "slides/lm_one_way.html#sl-vs-average-f-nl-ul",
    "title": "Linear models: one-way designs",
    "section": "SL vs average (F & NL & UL)",
    "text": "SL vs average (F & NL & UL)\n\ncontrasts(d$film) &lt;- c(-1,-1,3,-1)\ncontrasts(d$film)\n\n   [,1]        [,2]       [,3]\nF    -1 -0.67052910 -0.4658942\nNL   -1  0.73874075 -0.3477481\nSL    3  0.00000000  0.0000000\nUL   -1 -0.06821164  0.8136423\n\n\n\n\nserpulid.aov &lt;- aov(lserp ~ film, data = d)\ntidy(summary.lm(serpulid.aov))\n\n# A tibble: 4 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)  2.09       0.0221    94.8   2.07e-32\n2 film1       -0.0521     0.0127    -4.09  4.15e- 4\n3 film2        0.0493     0.0441     1.12  2.75e- 1\n4 film3       -0.00845    0.0441    -0.192 8.50e- 1"
  },
  {
    "objectID": "slides/lm_one_way.html#discussion-1",
    "href": "slides/lm_one_way.html#discussion-1",
    "title": "Linear models: one-way designs",
    "section": "Discussion",
    "text": "Discussion\n\nThoughts on the reading (6.2; categorical predictor)\nThis material is foundational. You’ve probably done a one-way ANOVA before. If you were teaching an undergrad, what is a key point you would emphasize?"
  }
]