[
  {
    "objectID": "slides/slides.html",
    "href": "slides/slides.html",
    "title": "Slides",
    "section": "",
    "text": "Week 1: Intro to Quinn and Keough, 2nd edition\nWeek 3: Linear models: one-way designs\nWeek 9: Maximum likelihood"
  },
  {
    "objectID": "slides/lm_one_way.html#outline",
    "href": "slides/lm_one_way.html#outline",
    "title": "Linear models: one-way designs",
    "section": "Outline",
    "text": "Outline\n\nIntro to linear models (board)\nVisualizing ANOVA (board)\nR examples (slides)"
  },
  {
    "objectID": "slides/lm_one_way.html#examples-from-qk2e-chapter-6",
    "href": "slides/lm_one_way.html#examples-from-qk2e-chapter-6",
    "title": "Linear models: one-way designs",
    "section": "Examples from QK2E chapter 6",
    "text": "Examples from QK2E chapter 6\n\nBox 6.1 (regression; continuous predictor)\n\nDiscussion\n\nBox 6.7 (ANOVA; categorical predictor)\n\nDiscussion"
  },
  {
    "objectID": "slides/lm_one_way.html#box-6.1",
    "href": "slides/lm_one_way.html#box-6.1",
    "title": "Linear models: one-way designs",
    "section": "Box 6.1",
    "text": "Box 6.1\nChristensen et al. (1996) studied the relationships between coarse woody debris (CWD) and shoreline vegetation and lake development in a sample of 16 lakes in North America. The main variables of interest:\n\ndensity of cabins (no. km−1)\ndensity of riparian trees (trees km−1)\nbasal area of riparian trees (m2 km−1)\ndensity of coarse woody debris (no. km−1)\nbasal area of coarse woody debris (m2 km−1)"
  },
  {
    "objectID": "slides/lm_one_way.html#visualize-data",
    "href": "slides/lm_one_way.html#visualize-data",
    "title": "Linear models: one-way designs",
    "section": "Visualize data",
    "text": "Visualize data"
  },
  {
    "objectID": "slides/lm_one_way.html#linear-model",
    "href": "slides/lm_one_way.html#linear-model",
    "title": "Linear models: one-way designs",
    "section": "Linear model",
    "text": "Linear model\n\nm1 &lt;- lm(cwdbasal ~ ripdens, data = d)\nsummary(m1)\n\n\nCall:\nlm(formula = cwdbasal ~ ripdens, data = d)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-38.62 -22.41 -13.33  26.16  61.35 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -77.09908   30.60801  -2.519 0.024552 *  \nripdens       0.11552    0.02343   4.930 0.000222 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 36.32 on 14 degrees of freedom\nMultiple R-squared:  0.6345,    Adjusted R-squared:  0.6084 \nF-statistic:  24.3 on 1 and 14 DF,  p-value: 0.0002216"
  },
  {
    "objectID": "slides/lm_one_way.html#plot-predicted-values-vs-residuals",
    "href": "slides/lm_one_way.html#plot-predicted-values-vs-residuals",
    "title": "Linear models: one-way designs",
    "section": "Plot predicted values vs residuals",
    "text": "Plot predicted values vs residuals"
  },
  {
    "objectID": "slides/lm_one_way.html#discussion",
    "href": "slides/lm_one_way.html#discussion",
    "title": "Linear models: one-way designs",
    "section": "Discussion",
    "text": "Discussion\n\nThoughts on the reading (6.1; continuous predictor)\nThis material is foundational. You’ve probably done a linear regression before. If you were teaching an undergrad, what is a key point you would emphasize?"
  },
  {
    "objectID": "slides/lm_one_way.html#box-6.7",
    "href": "slides/lm_one_way.html#box-6.7",
    "title": "Linear models: one-way designs",
    "section": "Box 6.7",
    "text": "Box 6.7\nKeough and Raimondi (1995) set up an experiment to examine the response of serpulid (polychaete worms) larvae to four types of biofilms on hard substrata in shallow marine waters. The four treatments were:\n\nF: field substrata (with a net, to exclude other invertebrates)\nNL: netted substrata developed in the lab\nUL: un-netted substrata developed in the lab\nSL: sterile substrata in the lab, without a net"
  },
  {
    "objectID": "slides/lm_one_way.html#visualize-data-1",
    "href": "slides/lm_one_way.html#visualize-data-1",
    "title": "Linear models: one-way designs",
    "section": "Visualize data",
    "text": "Visualize data"
  },
  {
    "objectID": "slides/lm_one_way.html#linear-model-1",
    "href": "slides/lm_one_way.html#linear-model-1",
    "title": "Linear models: one-way designs",
    "section": "Linear model",
    "text": "Linear model\n\nm1 &lt;- lm(lserp ~ film, data = d)\nsummary(m1)\n\n\nCall:\nlm(formula = lserp ~ film, data = d)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.22929 -0.06500  0.01843  0.07054  0.19557 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.11343    0.04411  47.912  &lt; 2e-16 ***\nfilmNL       0.06843    0.06238   1.097  0.28356    \nfilmSL      -0.17943    0.06238  -2.876  0.00831 ** \nfilmUL       0.01886    0.06238   0.302  0.76504    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1167 on 24 degrees of freedom\nMultiple R-squared:  0.4292,    Adjusted R-squared:  0.3578 \nF-statistic: 6.015 on 3 and 24 DF,  p-value: 0.003314"
  },
  {
    "objectID": "slides/lm_one_way.html#set-your-reference-level",
    "href": "slides/lm_one_way.html#set-your-reference-level",
    "title": "Linear models: one-way designs",
    "section": "Set your reference level",
    "text": "Set your reference level\n\nd &lt;- d %&gt;% mutate(film_ = fct_relevel(film, \"SL\", \"UL\", \"NL\", \"F\"))\nglimpse(d$film); glimpse(d$film_)\n\n Factor w/ 4 levels \"F\",\"NL\",\"SL\",..: 3 3 3 3 3 3 3 4 4 4 ...\n\n\n Factor w/ 4 levels \"SL\",\"UL\",\"NL\",..: 1 1 1 1 1 1 1 2 2 2 ..."
  },
  {
    "objectID": "slides/lm_one_way.html#linear-model-take-2",
    "href": "slides/lm_one_way.html#linear-model-take-2",
    "title": "Linear models: one-way designs",
    "section": "Linear model, take 2",
    "text": "Linear model, take 2\n\nm2 &lt;- lm(lserp ~ film_, data = d)\nsummary(m2)\n\n\nCall:\nlm(formula = lserp ~ film_, data = d)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.22929 -0.06500  0.01843  0.07054  0.19557 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.93400    0.04411  43.844  &lt; 2e-16 ***\nfilm_UL      0.19829    0.06238   3.179 0.004045 ** \nfilm_NL      0.24786    0.06238   3.973 0.000564 ***\nfilm_F       0.17943    0.06238   2.876 0.008310 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1167 on 24 degrees of freedom\nMultiple R-squared:  0.4292,    Adjusted R-squared:  0.3578 \nF-statistic: 6.015 on 3 and 24 DF,  p-value: 0.003314"
  },
  {
    "objectID": "slides/lm_one_way.html#null-model",
    "href": "slides/lm_one_way.html#null-model",
    "title": "Linear models: one-way designs",
    "section": "Null model",
    "text": "Null model\n\nm0 &lt;- lm(lserp ~ 1, data = d)\nsummary(m0)\n\n\nCall:\nlm(formula = lserp ~ 1, data = d)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.31239 -0.11064  0.03261  0.10961  0.21861 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.09039    0.02752   75.95   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1456 on 27 degrees of freedom"
  },
  {
    "objectID": "slides/lm_one_way.html#comparing-linear-models",
    "href": "slides/lm_one_way.html#comparing-linear-models",
    "title": "Linear models: one-way designs",
    "section": "Comparing linear models",
    "text": "Comparing linear models\n\nanova(m0, m1)\n\nAnalysis of Variance Table\n\nModel 1: lserp ~ 1\nModel 2: lserp ~ film\n  Res.Df     RSS Df Sum of Sq      F   Pr(&gt;F)   \n1     27 0.57265                                \n2     24 0.32688  3   0.24577 6.0149 0.003314 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nanova(m1)\n\nAnalysis of Variance Table\n\nResponse: lserp\n          Df  Sum Sq  Mean Sq F value   Pr(&gt;F)   \nfilm       3 0.24577 0.081924  6.0149 0.003314 **\nResiduals 24 0.32688 0.013620                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "slides/lm_one_way.html#planned-contrasts",
    "href": "slides/lm_one_way.html#planned-contrasts",
    "title": "Linear models: one-way designs",
    "section": "Planned contrasts",
    "text": "Planned contrasts\nDefine a linear combination of the p means that represent the comparison of interest\n\\[\n    c_1 \\bar{y}_1 + ... + c_i \\bar{y}_i + c_p \\bar{y}_p\n\\]\nwhere \\(c_i\\) represent contrast coefficients and sum to zero, and \\(\\bar{y}_i\\) are group means.\n\nTo omit a group, give it a coefficient of zero.\n\n\nVery cool: you can do contrasts for polynomial trends with group means! (see Box 6.4)"
  },
  {
    "objectID": "slides/lm_one_way.html#ul-vs-nl",
    "href": "slides/lm_one_way.html#ul-vs-nl",
    "title": "Linear models: one-way designs",
    "section": "UL vs NL",
    "text": "UL vs NL\n\n# Default\ncontrasts(d$film)\n\n   NL SL UL\nF   0  0  0\nNL  1  0  0\nSL  0  1  0\nUL  0  0  1\n\n\n\n\n# Change it\ncontrasts(d$film) &lt;- c(0, 1, 0, -1)\ncontrasts(d$film)\n\n   [,1]       [,2]       [,3]\nF     0 -0.5000000 -0.7071068\nNL    1 -0.1666667  0.4714045\nSL    0  0.8333333 -0.2357023\nUL   -1 -0.1666667  0.4714045"
  },
  {
    "objectID": "slides/lm_one_way.html#ul-vs-nl-1",
    "href": "slides/lm_one_way.html#ul-vs-nl-1",
    "title": "Linear models: one-way designs",
    "section": "UL vs NL",
    "text": "UL vs NL\n\n# Refit the model with new contrasts\nserpulid.aov &lt;- aov(lserp ~ film, data = d)\nsummary(serpulid.aov)\n\n            Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nfilm         3 0.2458 0.08192   6.015 0.00331 **\nResiduals   24 0.3269 0.01362                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nNote that the aov results have not changed."
  },
  {
    "objectID": "slides/lm_one_way.html#ul-vs-nl-2",
    "href": "slides/lm_one_way.html#ul-vs-nl-2",
    "title": "Linear models: one-way designs",
    "section": "UL vs NL",
    "text": "UL vs NL\n\ntidy(summary.lm(serpulid.aov))\n\n# A tibble: 4 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   2.09      0.0221    94.8   2.07e-32\n2 film1         0.0248    0.0312     0.795 4.35e- 1\n3 film2        -0.164     0.0441    -3.72  1.07e- 3\n4 film3         0.0834    0.0441     1.89  7.07e- 2\n\n\n\nTo evaluate the contrast, look at the statistic and p-value for film1"
  },
  {
    "objectID": "slides/lm_one_way.html#f-vs-average-nl-ul",
    "href": "slides/lm_one_way.html#f-vs-average-nl-ul",
    "title": "Linear models: one-way designs",
    "section": "F vs average (NL & UL)",
    "text": "F vs average (NL & UL)\n\ncontrasts(d$film) &lt;- c(2,-1,0,-1)\ncontrasts(d$film)\n\n   [,1]       [,2]        [,3]\nF     2 -0.2867757  0.03306064\nNL   -1 -0.3677574 -0.66939360\nSL    0  0.8603272 -0.09918192\nUL   -1 -0.2057940  0.73551488\n\n\n\n\nserpulid.aov &lt;- aov(lserp ~ film, data = d)\ntidy(summary.lm(serpulid.aov))\n\n# A tibble: 4 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   2.09      0.0221    94.8   2.07e-32\n2 film1        -0.0145    0.0180    -0.808 4.27e- 1\n3 film2        -0.183     0.0441    -4.16  3.53e- 4\n4 film3        -0.0141    0.0441    -0.321 7.51e- 1"
  },
  {
    "objectID": "slides/lm_one_way.html#sl-vs-average-f-nl-ul",
    "href": "slides/lm_one_way.html#sl-vs-average-f-nl-ul",
    "title": "Linear models: one-way designs",
    "section": "SL vs average (F & NL & UL)",
    "text": "SL vs average (F & NL & UL)\n\ncontrasts(d$film) &lt;- c(-1,-1,3,-1)\ncontrasts(d$film)\n\n   [,1]        [,2]       [,3]\nF    -1 -0.67052910 -0.4658942\nNL   -1  0.73874075 -0.3477481\nSL    3  0.00000000  0.0000000\nUL   -1 -0.06821164  0.8136423\n\n\n\n\nserpulid.aov &lt;- aov(lserp ~ film, data = d)\ntidy(summary.lm(serpulid.aov))\n\n# A tibble: 4 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)  2.09       0.0221    94.8   2.07e-32\n2 film1       -0.0521     0.0127    -4.09  4.15e- 4\n3 film2        0.0493     0.0441     1.12  2.75e- 1\n4 film3       -0.00845    0.0441    -0.192 8.50e- 1"
  },
  {
    "objectID": "slides/lm_one_way.html#discussion-1",
    "href": "slides/lm_one_way.html#discussion-1",
    "title": "Linear models: one-way designs",
    "section": "Discussion",
    "text": "Discussion\n\nThoughts on the reading (6.2; categorical predictor)\nThis material is foundational. You’ve probably done a one-way ANOVA before. If you were teaching an undergrad, what is a key point you would emphasize?"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "qk2e",
    "section": "",
    "text": "This website is a space for working through Quinn and Keough’s 2nd edition textbook on data analysis for biologists. The Chapter notes are intended for review and discussion in a seminar (Oceans 200). The Slides are intended as a short primer on the topic by the discussion leader. The Examples are reproduced and lightly modified from the original worked examples.\nOceans 200 schedule\n\n\n\n\n\n\n\n\n\n\n\nWeek\nDate\nTopic\nQK chapter\nPre-class box example\nIn-class box example\n\n\n\n\n1\nJan. 17th 2023\nIntro to seminar and review of intro stats\n2\n2.2\n\n\n\n2\nJan. 10th 2023\nSampling and experimental design\n3\n\n\n\n\n3\nJan. 24th 2023\nOne-way designs\n6\n6.7\n6.11\n\n\n4\nJan. 31st 2023\nFactorial designs\n7\n7.1\n7.3\n\n\n5\nFeb. 7th 2023\nNested designs\n10\n10.6\n10.7\n\n\n6\nFeb 14th 2023\nSplit-plot designs\n11\n11.1\n11.2\n\n\n7\nFeb. 21st 2023\nRepeated measures designs\n12\n12.1\n12.2\n\n\n8\nFeb. 28th 2023\nStudents’ choice / presentations\ntbd\n\n\n\n\n9\nMar. 6th 2023\nStudents’ choice / presentations\ntbd\n\n\n\n\n10\nMar. 13th 2023\nStudents’ choice / presentations\ntbd"
  },
  {
    "objectID": "examples/steiger.html",
    "href": "examples/steiger.html",
    "title": "QK Box 12.1",
    "section": "",
    "text": "Steiger et al. (2008) studied the Coolidge effect, the decline in males’ interest in mating with the same female compared to novel females, using the burying beetle Nicrophorus vespilloides. Eighteen male beetles were presented with the same female beetle four times, and then a novel female on the fifth occasion. This was a repeated measures design as the same individual males were repeatedly presented with females. There was no evidence that physical exhaustion affected time to mating as a separate control group of males were presented with novel, unmated females five times in succession, and there was no change in time to mating. The within-subjects factor was the order of presented females, and while this could have been treated as a continuous covariate, we treated it as a fixed factor with five groups. The response variable recorded on each occasion was time to mating.\nFrancisco Welter-Schultes, CC0, via Wikimedia Commons\nSteiger, S., Franz, R., Eggert, A. K. & Muller, J. K. (2008). The Coolidge effect, individual recognition and selection for distinctive cuticular signatures in a burying beetle. Proceedings of the Royal Society B, 275, 1831-8.\nLink to the paper: doi: 10.1098/rspb.2008.0375 and data",
    "crumbs": [
      "Examples",
      "QK Box 12.1"
    ]
  },
  {
    "objectID": "examples/steiger.html#fit-random-intercept-model-using-lme4-and-reml",
    "href": "examples/steiger.html#fit-random-intercept-model-using-lme4-and-reml",
    "title": "QK Box 12.1",
    "section": "Fit random intercept model using lme4 and REML",
    "text": "Fit random intercept model using lme4 and REML\n\nsteiger.lmer1 &lt;- lmer(ltime~matingnumber + (1|individual), REML=TRUE, steiger)\nsummary(steiger.lmer1, ddf=\"Kenward-Roger\")\n\nLinear mixed model fit by REML. t-tests use Kenward-Roger's method [\nlmerModLmerTest]\nFormula: ltime ~ matingnumber + (1 | individual)\n   Data: steiger\n\nREML criterion at convergence: 167.9\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.9213 -0.8145  0.2051  0.6980  1.9271 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n individual (Intercept) 0.01289  0.1135  \n Residual               0.33094  0.5753  \nNumber of obs: 90, groups:  individual, 18\n\nFixed effects:\n              Estimate Std. Error       df t value Pr(&gt;|t|)    \n(Intercept)    1.75927    0.06628 17.00000  26.542 2.81e-15 ***\nmatingnumber1 -0.29345    0.12128 68.00000  -2.420   0.0182 *  \nmatingnumber2  0.07000    0.12128 68.00000   0.577   0.5657    \nmatingnumber3  0.25006    0.12128 68.00000   2.062   0.0430 *  \nmatingnumber4  0.38087    0.12128 68.00000   3.140   0.0025 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) mtngn1 mtngn2 mtngn3\nmatingnmbr1  0.000                     \nmatingnmbr2  0.000 -0.250              \nmatingnmbr3  0.000 -0.250 -0.250       \nmatingnmbr4  0.000 -0.250 -0.250 -0.250\n\nanova(steiger.lmer1, type=3, ddf=\"Kenward-Roger\")\n\nType III Analysis of Variance Table with Kenward-Roger's method\n             Sum Sq Mean Sq NumDF DenDF F value    Pr(&gt;F)    \nmatingnumber 8.3635  2.0909     4    68   6.318 0.0002202 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nemmeans(steiger.lmer1, ~matingnumber)\n\n matingnumber emmean    SE   df lower.CL upper.CL\n first          1.47 0.138 84.5     1.19     1.74\n second         1.83 0.138 84.5     1.55     2.10\n third          2.01 0.138 84.5     1.73     2.28\n fourth         2.14 0.138 84.5     1.87     2.41\n fifth          1.35 0.138 84.5     1.08     1.63\n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \n\n\nNote different CIs compared to OLS model fitting due to K-R adjustment\nCI on variance components (remembering to square CIs from lmer which are in SD units)\n\nsteiger.ci1 &lt;- confint.merMod(steiger.lmer1, oldNames=FALSE)\nsteiger.vc1 &lt;- (steiger.ci1)^2\nprint(steiger.vc1)\n\n                                 2.5 %     97.5 %\nsd_(Intercept)|individual 0.0000000000 0.09444955\nsigma                     0.2292861943 0.43705072\n(Intercept)               2.6438048318 3.58181697\nmatingnumber1             0.2783247753 0.00351979\nmatingnumber2             0.0269356442 0.09248673\nmatingnumber3             0.0002542346 0.23443208\nmatingnumber4             0.0215345611 0.37820498\n\n\n\nBar graph for time main effect\n\nsteiger_sum &lt;- summarySE(steiger, measurevar= 'ltime', groupvars= 'matingnumber')\nggplot(steiger_sum, aes(x=matingnumber, y=ltime))+\n  geom_bar(stat=\"identity\", position=\"dodge\", fill=\"lightblue\")+\n  geom_errorbar(aes(ymin=ltime-se, ymax=ltime+se), width=0.3, color=\"darkblue\")",
    "crumbs": [
      "Examples",
      "QK Box 12.1"
    ]
  },
  {
    "objectID": "examples/probability_distributions_key.html",
    "href": "examples/probability_distributions_key.html",
    "title": "Probability distributions",
    "section": "",
    "text": "Probability distributions\n\n# Packages\nlibrary(tidyverse)\nlibrary(knitr)\n\n\n1. Choosing Probability Distributions\nChoose one or more appropriate distributions for the types of data shown below and justify your decision(s). 1pt. each\n\nThe number of seals on a haul-out beach in the gulf of Alaska. &gt; Poisson\nPresence or absence of an invasive species in forest patches. &gt; Bernoulli\nThe absolute distance that seastar larvae will settle from the location of spawning (assume it cannot be exactly 0). &gt; exponential\nThe number of abalone at time \\(t\\) surviving until time \\(t + 1\\). &gt; Poisson\nThe proportion of reef sharks on a reef captured by a camera trap. &gt; beta\nThe number of prey (from an initial number \\(n\\)) eaten by a predator during an experiment in aquaria. &gt; binomial\nThe body length of a cohort of adult whale sharks. | &gt; normal\n\n\n\n2. Exploring Distributions & Parameters\n\nYou’re modeling a population of gophers at Hopkins Marine Station, and you want to incorporate predation by the local red-shouldered hawk. On average, the hawk eats 2 gophers per month, but it doesn’t catch the same number of gophers each month. Choose a distribution from which to simulate monthly hawk predation (justify your answer). What values should you choose for this distribution’s parameter(s), and why? What is the probability that your simulated hawk will eat 4 gophers in a given month (you can eyeball this from the plot)? 3pt.\n\n\ny &lt;- rpois(n = 1000, lambda = 2)\nrange(y)\n\n[1] 0 9\n\nhist(y, breaks = c(0:max(y)))\n\n\n\n\n\n\n\ndpois(4, lambda = 2)\n\n[1] 0.09022352\n\n\n\nAt each of 15 sites, you’ve set up 10 enclosures in which you’ve placed juvenile kelp (one individual per enclosure), and after checking on these cages 3 months later you’d like to fit a model which relates the number of juvenile kelp surviving at each site to temperature at each site. Why is a binomial distribution with \\(n = 10\\) the best choice for this data? Vary the other parameter (\\(p\\)), and observe what happens to the shape of the distribution. What is the most likely number of surviving kelp (per site) if the probability of survival \\(p = 0.1\\)? Choose a value of \\(p\\) for which the distribution’s shape is most symmetrical. 3pt.\n\n\nvec_x &lt;- seq(0, 10, by = 1)\nvec_p &lt;- dbinom(x = vec_x, size = 10, prob = 0.1)\nplot(vec_x, vec_p)\n\n\n\n\n\n\n\n# 1 individual is most likely to survive\nvec_p &lt;- dbinom(x = vec_x, size = 10, prob = 0.5)\nplot(vec_x, vec_p)\n\n\n\n\n\n\n\n\n\nYou’re simulating a spatial model for which a proportion of the available habitat will be suitable (and the other proportion unsuitable) for abalone, and you’ve chosen a Beta distribution to do this. Choose three pairs of values for \\(\\alpha\\) and \\(\\beta\\) that give you three Beta distributions which are all symmetrical around 0.5 (so that, on average, your simulations will have about 50% suitable habitat), each with the following properties:\n\nOne in which simulations with a suitable habitat of around 0.5 will be the most likely.\n\n\n\n# ?beta\n# E[X] = alpha / (alpha + beta)\n# shape1 = alpha, shape2 = beta\nvec_x &lt;- seq(0, 1, by = 0.05)\nvec_x\n\n [1] 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45 0.50 0.55 0.60 0.65 0.70\n[16] 0.75 0.80 0.85 0.90 0.95 1.00\n\nvec_p &lt;- dbeta(vec_x, shape1 = 2, shape2 = 2)\nplot(vec_x, vec_p, type = \"l\", col = \"red\")\n\n\n\n\n\n\n\n\n-   One in which some simulations will have lots of suitable habitat, and some will have very little suitable habitat, but few will have around 50% suitable habitat.\n\nvec_x &lt;- seq(0, 1, by = 0.01)\nvec_x\n\n  [1] 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.10 0.11 0.12 0.13 0.14\n [16] 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.23 0.24 0.25 0.26 0.27 0.28 0.29\n [31] 0.30 0.31 0.32 0.33 0.34 0.35 0.36 0.37 0.38 0.39 0.40 0.41 0.42 0.43 0.44\n [46] 0.45 0.46 0.47 0.48 0.49 0.50 0.51 0.52 0.53 0.54 0.55 0.56 0.57 0.58 0.59\n [61] 0.60 0.61 0.62 0.63 0.64 0.65 0.66 0.67 0.68 0.69 0.70 0.71 0.72 0.73 0.74\n [76] 0.75 0.76 0.77 0.78 0.79 0.80 0.81 0.82 0.83 0.84 0.85 0.86 0.87 0.88 0.89\n [91] 0.90 0.91 0.92 0.93 0.94 0.95 0.96 0.97 0.98 0.99 1.00\n\nvec_p &lt;- dbeta(vec_x, shape1 = 1/4, shape2 = 1/4)\nplot(vec_x, vec_p, type = \"l\", col = \"red\")\n\n\n\n\n\n\n\n\n-   One in which all possible percentages of suitable habitat are about equally likely, except for the extreme values.\n\nvec_x &lt;- seq(0, 1, by = 0.01)\nvec_x\n\n  [1] 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.10 0.11 0.12 0.13 0.14\n [16] 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.23 0.24 0.25 0.26 0.27 0.28 0.29\n [31] 0.30 0.31 0.32 0.33 0.34 0.35 0.36 0.37 0.38 0.39 0.40 0.41 0.42 0.43 0.44\n [46] 0.45 0.46 0.47 0.48 0.49 0.50 0.51 0.52 0.53 0.54 0.55 0.56 0.57 0.58 0.59\n [61] 0.60 0.61 0.62 0.63 0.64 0.65 0.66 0.67 0.68 0.69 0.70 0.71 0.72 0.73 0.74\n [76] 0.75 0.76 0.77 0.78 0.79 0.80 0.81 0.82 0.83 0.84 0.85 0.86 0.87 0.88 0.89\n [91] 0.90 0.91 0.92 0.93 0.94 0.95 0.96 0.97 0.98 0.99 1.00\n\nvec_p &lt;- dbeta(vec_x, shape1 = 1.2, shape2 = 1.2)\nplot(vec_x, vec_p, type = \"l\", col = \"red\")\n\n\n\n\n\n\n\n\nTaking a look at the equation for the mean of the Beta distribution will probably make this pretty straightforward. What are the parameter combinations that you chose? Take a screenshot of these distributions (or draw them) and include them in your answers. **3pt.**\n\n\n3. Using R’s distribution functions\nAnswer these questions using the R console (if you’re running the distribution dashboard locally, you’ll have to close it to do so). Provide your code for all answers. 1pt. each\n\nFind the mean, variance, and 95% quantiles (i.e., 2.5% and 97.5% quantiles) of 1000 random draws from a Poisson distribution with \\(\\lambda=33\\).\nWhat is the probability \\(P(X \\leq 6)\\) that a random draw from a Poisson distribution with \\(\\lambda = 4\\) will be less than or equal to 6?\nWhat is the probability \\(P(X = 3)\\) of obtaining a value of 3 from a Binomial distribution with \\(p = 0.3\\) and \\(n = 5\\)?\nWhat is the probability \\(P(-1.5 \\leq X \\leq 1.5)\\) that a value drawn from a standard normal distribution will be between -1.5 and 1.5 (it may help to approach this visually)?\nFind the value \\(x\\) that satisfies to \\(P(X \\leq x) = 0.8\\), if \\(X\\) is a Gamma random variable with \\(k =2\\) and \\(\\theta = 1\\).\n\n\n# a\nlambda &lt;- 33; n &lt;- 10000\ny &lt;- rpois(n, lambda)\nmean(y); var(y); quantile (y, c(0.025, 0.975))\n\n[1] 32.9882\n\n\n[1] 32.97476\n\n\n 2.5% 97.5% \n   22    45 \n\n# b\nppois(q = 6, lambda = 4, lower.tail = TRUE)\n\n[1] 0.889326\n\n# c\ndbinom(x = 3, size = 5, prob = 0.3)\n\n[1] 0.1323\n\n# d\npnorm(q = 1.5) - pnorm(q = -1.5)\n\n[1] 0.8663856\n\n# e\n# shape = k, scale = theta\nqgamma(p = 0.8, shape = 2, scale = 1)\n\n[1] 2.994308\n\n\n\n\n4. Samples and their means\n\nRead the intro text below the Normal distribution in the distribution dashboard. What is the difference between the distribution of the sample and the “sampling distribution?” How is the sample size distinguished from the number of samples? 2pt.\nDrag the sliders for the sample size and the number of samples all the way to the right. Visit all of the distributions and spend a little time there, messing with their parameters. In general, what do you notice about the distribution of their sample means? Are there any distributions (/parameter values) for which this pattern does not hold? If so, what seems to be the reason? 3pt.\n\n\n\nsample size: # of replicates used to calculate the mean (or other statistic from the data) for a sample data set number of samples: the # of sampled datasets sampling distribution: a distribution of statistics calculated from a set of samples distribution of samples: the count of outcomes (freq distribution) for a (single) sample dataset\n\n\n\n\nSampling distribution converges to normal for all distributions, except for pareto under certain parameter values. When the scale and shape parameters are less than ~, we start to see right-skewed sampling distributions."
  },
  {
    "objectID": "examples/lowbayes.html",
    "href": "examples/lowbayes.html",
    "title": "QK Box 2.5",
    "section": "",
    "text": "This box continues with the Low et al. example starting in Box 2.2\n\nPreliminaries\nUse rstanarm and BayesFactor packages; also needs bayestestR.\nAdd bayesplot for control over plot\nLoad graphics packages (if ggplot version of figures wanted)\nNote that iso is reference group so diff between means is -ve\n\n\nUninformative priors\n\nlow &lt;- read_csv(\"../data/lowco2.csv\")\nlow1 &lt;- stan_glm(co2~anesth,family = gaussian(link = \"identity\"),data=low)\nposteriors1 &lt;- describe_posterior(low1)\n\n\nprint_md(posteriors1, digits = 2)\n\n\nSummary of Posterior Distribution\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nMedian\n95% CI\npd\nROPE\n% in ROPE\nRhat\nESS\n\n\n\n\n(Intercept)\n70.69\n[ 60.24, 81.14]\n100%\n[-1.91, 1.91]\n0%\n0.999\n3374.00\n\n\nanesthiso\n-20.53\n[-34.82, -6.54]\n99.65%\n[-1.91, 1.91]\n0%\n0.999\n3404.00\n\n\n\n\n# plot posterior distribution for all three parameters (intercept, mean diff, sigma)\nplot(low1,plotfun=\"mcmc_hist\")\n## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n# get Bayes factor for mean diff\nlowx &lt;- as.data.frame(low)\nlmBF(co2~anesth, data=lowx,posterior=FALSE)\n## Bayes factor analysis\n## --------------\n## [1] anesth : 8.039109 ±0%\n## \n## Against denominator:\n##   Intercept only \n## ---\n## Bayes factor type: BFlinearModel, JZS\n\n\n\nInformative priors\nRun three options, mean difference with high and low precision, and a bigger mean difference with high precision\n\n#for mean difference with high precision\nlow2 &lt;- stan_glm(co2~anesth,family = gaussian(link = \"identity\"),prior=normal(-25,5),data=low)\n## \n## SAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\n## Chain 1: \n## Chain 1: Gradient evaluation took 1e-05 seconds\n## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.1 seconds.\n## Chain 1: Adjust your expectations accordingly!\n## Chain 1: \n## Chain 1: \n## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\n## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\n## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\n## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\n## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\n## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\n## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\n## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\n## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\n## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\n## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\n## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\n## Chain 1: \n## Chain 1:  Elapsed Time: 0.014 seconds (Warm-up)\n## Chain 1:                0.011 seconds (Sampling)\n## Chain 1:                0.025 seconds (Total)\n## Chain 1: \n## \n## SAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\n## Chain 2: \n## Chain 2: Gradient evaluation took 4e-06 seconds\n## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.\n## Chain 2: Adjust your expectations accordingly!\n## Chain 2: \n## Chain 2: \n## Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\n## Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\n## Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\n## Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\n## Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\n## Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\n## Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\n## Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\n## Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\n## Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\n## Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\n## Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\n## Chain 2: \n## Chain 2:  Elapsed Time: 0.015 seconds (Warm-up)\n## Chain 2:                0.012 seconds (Sampling)\n## Chain 2:                0.027 seconds (Total)\n## Chain 2: \n## \n## SAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\n## Chain 3: \n## Chain 3: Gradient evaluation took 5e-06 seconds\n## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.\n## Chain 3: Adjust your expectations accordingly!\n## Chain 3: \n## Chain 3: \n## Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\n## Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\n## Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\n## Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\n## Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\n## Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\n## Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\n## Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\n## Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\n## Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\n## Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\n## Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\n## Chain 3: \n## Chain 3:  Elapsed Time: 0.014 seconds (Warm-up)\n## Chain 3:                0.012 seconds (Sampling)\n## Chain 3:                0.026 seconds (Total)\n## Chain 3: \n## \n## SAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\n## Chain 4: \n## Chain 4: Gradient evaluation took 3e-06 seconds\n## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds.\n## Chain 4: Adjust your expectations accordingly!\n## Chain 4: \n## Chain 4: \n## Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\n## Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\n## Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\n## Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\n## Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\n## Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\n## Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\n## Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\n## Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\n## Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\n## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\n## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\n## Chain 4: \n## Chain 4:  Elapsed Time: 0.014 seconds (Warm-up)\n## Chain 4:                0.011 seconds (Sampling)\n## Chain 4:                0.025 seconds (Total)\n## Chain 4:\nposteriors2 &lt;- describe_posterior(low2)\nprint_md(posteriors2, digits = 2)\n\n\nSummary of Posterior Distribution\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nMedian\n95% CI\npd\nROPE\n% in ROPE\nRhat\nESS\n\n\n\n\n(Intercept)\n72.26\n[ 64.18, 80.39]\n100%\n[-1.91, 1.91]\n0%\n1.001\n3603.00\n\n\nanesthiso\n-23.62\n[-31.88, -15.51]\n100%\n[-1.91, 1.91]\n0%\n1.001\n3990.00\n\n\n\n\n# informative prior for mean difference with low precision\nlow3 &lt;- stan_glm(co2~anesth,family = gaussian(link = \"identity\"),prior=normal(-25,20),data=low)\n## \n## SAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\n## Chain 1: \n## Chain 1: Gradient evaluation took 8e-06 seconds\n## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds.\n## Chain 1: Adjust your expectations accordingly!\n## Chain 1: \n## Chain 1: \n## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\n## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\n## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\n## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\n## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\n## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\n## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\n## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\n## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\n## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\n## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\n## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\n## Chain 1: \n## Chain 1:  Elapsed Time: 0.015 seconds (Warm-up)\n## Chain 1:                0.011 seconds (Sampling)\n## Chain 1:                0.026 seconds (Total)\n## Chain 1: \n## \n## SAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\n## Chain 2: \n## Chain 2: Gradient evaluation took 3e-06 seconds\n## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds.\n## Chain 2: Adjust your expectations accordingly!\n## Chain 2: \n## Chain 2: \n## Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\n## Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\n## Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\n## Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\n## Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\n## Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\n## Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\n## Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\n## Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\n## Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\n## Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\n## Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\n## Chain 2: \n## Chain 2:  Elapsed Time: 0.019 seconds (Warm-up)\n## Chain 2:                0.012 seconds (Sampling)\n## Chain 2:                0.031 seconds (Total)\n## Chain 2: \n## \n## SAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\n## Chain 3: \n## Chain 3: Gradient evaluation took 2e-06 seconds\n## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds.\n## Chain 3: Adjust your expectations accordingly!\n## Chain 3: \n## Chain 3: \n## Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\n## Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\n## Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\n## Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\n## Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\n## Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\n## Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\n## Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\n## Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\n## Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\n## Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\n## Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\n## Chain 3: \n## Chain 3:  Elapsed Time: 0.014 seconds (Warm-up)\n## Chain 3:                0.012 seconds (Sampling)\n## Chain 3:                0.026 seconds (Total)\n## Chain 3: \n## \n## SAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\n## Chain 4: \n## Chain 4: Gradient evaluation took 2e-06 seconds\n## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds.\n## Chain 4: Adjust your expectations accordingly!\n## Chain 4: \n## Chain 4: \n## Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\n## Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\n## Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\n## Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\n## Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\n## Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\n## Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\n## Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\n## Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\n## Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\n## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\n## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\n## Chain 4: \n## Chain 4:  Elapsed Time: 0.014 seconds (Warm-up)\n## Chain 4:                0.011 seconds (Sampling)\n## Chain 4:                0.025 seconds (Total)\n## Chain 4:\nposteriors3 &lt;- describe_posterior(low3)\nprint_md(posteriors3, digits = 2)\n\n\nSummary of Posterior Distribution\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nMedian\n95% CI\npd\nROPE\n% in ROPE\nRhat\nESS\n\n\n\n\n(Intercept)\n71.17\n[ 61.16, 81.20]\n100%\n[-1.91, 1.91]\n0%\n1.000\n3504.00\n\n\nanesthiso\n-21.44\n[-34.89, -8.30]\n99.78%\n[-1.91, 1.91]\n0%\n1.000\n3517.00\n\n\n\n\n# informative prior for bigger mean difference with high precision\nlow4 &lt;- stan_glm(co2~anesth,family = gaussian(link = \"identity\"),prior=normal(-50,5),data=low)\n## \n## SAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\n## Chain 1: \n## Chain 1: Gradient evaluation took 9e-06 seconds\n## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.09 seconds.\n## Chain 1: Adjust your expectations accordingly!\n## Chain 1: \n## Chain 1: \n## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\n## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\n## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\n## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\n## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\n## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\n## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\n## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\n## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\n## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\n## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\n## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\n## Chain 1: \n## Chain 1:  Elapsed Time: 0.015 seconds (Warm-up)\n## Chain 1:                0.011 seconds (Sampling)\n## Chain 1:                0.026 seconds (Total)\n## Chain 1: \n## \n## SAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\n## Chain 2: \n## Chain 2: Gradient evaluation took 3e-06 seconds\n## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds.\n## Chain 2: Adjust your expectations accordingly!\n## Chain 2: \n## Chain 2: \n## Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\n## Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\n## Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\n## Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\n## Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\n## Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\n## Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\n## Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\n## Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\n## Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\n## Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\n## Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\n## Chain 2: \n## Chain 2:  Elapsed Time: 0.017 seconds (Warm-up)\n## Chain 2:                0.011 seconds (Sampling)\n## Chain 2:                0.028 seconds (Total)\n## Chain 2: \n## \n## SAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\n## Chain 3: \n## Chain 3: Gradient evaluation took 3e-06 seconds\n## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds.\n## Chain 3: Adjust your expectations accordingly!\n## Chain 3: \n## Chain 3: \n## Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\n## Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\n## Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\n## Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\n## Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\n## Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\n## Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\n## Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\n## Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\n## Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\n## Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\n## Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\n## Chain 3: \n## Chain 3:  Elapsed Time: 0.016 seconds (Warm-up)\n## Chain 3:                0.012 seconds (Sampling)\n## Chain 3:                0.028 seconds (Total)\n## Chain 3: \n## \n## SAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\n## Chain 4: \n## Chain 4: Gradient evaluation took 1e-06 seconds\n## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\n## Chain 4: Adjust your expectations accordingly!\n## Chain 4: \n## Chain 4: \n## Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\n## Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\n## Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\n## Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\n## Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\n## Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\n## Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\n## Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\n## Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\n## Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\n## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\n## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\n## Chain 4: \n## Chain 4:  Elapsed Time: 0.014 seconds (Warm-up)\n## Chain 4:                0.014 seconds (Sampling)\n## Chain 4:                0.028 seconds (Total)\n## Chain 4:\nposteriors4 &lt;- describe_posterior(low4)\nprint_md(posteriors4, digits = 2)\n\n\nSummary of Posterior Distribution\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nMedian\n95% CI\npd\nROPE\n% in ROPE\nRhat\nESS\n\n\n\n\n(Intercept)\n81.88\n[ 72.55, 92.10]\n100%\n[-1.91, 1.91]\n0%\n1.001\n2611.00\n\n\nanesthiso\n-41.97\n[-51.25, -33.12]\n100%\n[-1.91, 1.91]\n0%\n1.000\n2972.00\n\n\n\n\n\n\n\nGenerate ggplot-compatible figure for mean difference posterior distribution\n\nposterior&lt;-as.array(low1)\ncolor_scheme_set(\"gray\")\np&lt;-mcmc_hist(posterior, pars = c(\"anesthiso\"))+\n  xlab(\"Mean difference\")\np\n## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n# ggsave (\"QK F2_07.pdf\", plot = p, height = ph, width = pw, units='cm')"
  },
  {
    "objectID": "examples/linton.html",
    "href": "examples/linton.html",
    "title": "QK Box 7.1",
    "section": "",
    "text": "Linton et al. (2009) studied the effects of the insecticide pyriproxyfen on ovarian development in an endemic Christmas Island land crab, Geocarcoidea natalis. The insecticide was proposed as a means of controlling numbers of an introduced ant species that was viewed as a major threat, and it is an endocrine disruptor. The experiment was designed to test whether the insecticide might pose risks to the crabs, which have a hormone similar to the one targeted in insects, and consisted of feeding crabs a mixture of leaf litter and a bait. Half of the baits contained the insecticide, and the other half were controls (bait type factor). The baits were supplied at three rates, with two levels corresponding to levels used in field applications (2 kg ha-1 and 4 kg ha-1), with the third rate being ad libitum feeding (bait dosage factor). The experimental units in this case were large plastic tubs, each containing a single female crab, and there were 7 crabs for each combination of factors. The response variable was the dry mass of the ovaries of each crab. A two-factor linear model (7.2) including the fixed main effects of bait type and bait dosage and their interaction was fitted to these data.\n\n\n\nJohn Tann from Sydney, Australia, [CC BY 2.0](https://creativecommons.org/licenses/by/2.0), via Wikimedia Commons\n\n\nHere is the paper and the data\nLinton, S., Barrow, L., Davies, C. & Harman, L. (2009). Potential endocrine disruption of ovary synthesis in the Christmas Island red crab Gecarcoidea natalis by the insecticide pyriproxyfen. Comparative Biochemistry and Physiology, Part A, 154, 289-97.\n\nPreliminaries\nLoad packages\n\nlibrary(tidyverse)\nlibrary(sjstats)\nlibrary(effectsize)\nlibrary(afex)\nlibrary(emmeans)\nlibrary(ggsci)\nlibrary(patchwork)\n\nImport linton data file (linton.csv)\n\nlinton &lt;- read.csv(\"data/linton.csv\")\nhead(linton,10)\n\n      type dosage drymass nitrogen\n1  Control      2   0.524     3620\n2  Control      2   0.535     4030\n3  Control      2   1.094     6530\n4  Control      2   0.525     3938\n5  Control      2   0.707     4312\n6  Control      2   0.551     3740\n7  Control      2   0.489     3860\n8  Control      4   0.461     4329\n9  Control      4   0.584     5108\n10 Control      4   0.715     5877\n\n\n\n\nFit model to untransformed data and check residuals\nStart with boxlplots. Too few reps for boxplot by cell so boxplot for each factor separately\n\nboxplot(drymass~type,data=linton)\n\n\n\n\n\n\n\nboxplot(drymass~dosage,data=linton)\n\n\n\n\n\n\n\nlinton.aov &lt;- aov(drymass~type*dosage, data=linton)\nplot(linton.aov)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo strong pattern in residuals or boxplots so examine analysis with untransformed data\n\nsummary(linton.aov)\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)  \ntype         1  2.606  2.6060   7.360 0.0102 *\ndosage       2  0.671  0.3353   0.947 0.3974  \ntype:dosage  2  1.157  0.5784   1.634 0.2094  \nResiduals   36 12.747  0.3541                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nGet effect size measures (eta- and omega-squared (effectsize package)\n\neta_squared(linton.aov)\n\n# Effect Size for ANOVA (Type I)\n\nParameter   | Eta2 (partial) |       95% CI\n-------------------------------------------\ntype        |           0.17 | [0.03, 1.00]\ndosage      |           0.05 | [0.00, 1.00]\ntype:dosage |           0.08 | [0.00, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\nomega_squared(linton.aov)\n\n# Effect Size for ANOVA (Type I)\n\nParameter   | Omega2 (partial) |       95% CI\n---------------------------------------------\ntype        |             0.13 | [0.01, 1.00]\ndosage      |             0.00 | [0.00, 1.00]\ntype:dosage |             0.03 | [0.00, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\n\nInteraction plot\n\nafex_plot(linton.aov, \"type\", \"dosage\", dodge=0.05)+theme_light()\n\ndv column detected: drymass\n\n\nNo id column passed. Assuming all rows are independent samples.\n\n\n\n\n\n\n\n\n\n\n\nHigh quality figures\n\nResidual plot\n\np1 &lt;- ggplot(linton.aov, aes(x = linton.aov$fitted.values, y = linton.aov$residuals)) +\n  geom_point(color=sc) +\n  theme_classic(base_size = 10)+\n  theme(\n    axis.text = element_text(colour = ac),\n    axis.line = element_line(color = ac),\n    axis.ticks = element_line(color = ac),\n        )+labs(x = \"Predicted ovary mass\", y = \"Residuals\", \n       )\n\n\n\nInteraction plot\nUse emmeans to get dataframe of means and se\n\nemm1&lt;-emmeans(linton.aov, ~type|dosage)\nemm2&lt;-as.data.frame(emm1)\nemm2\n\ndosage = 2:\n type            emmean        SE df  lower.CL upper.CL\n Control      0.6321429 0.2249031 36 0.1760182 1.088268\n Experimental 1.1504286 0.2249031 36 0.6943039 1.606553\n\ndosage = 4:\n type            emmean        SE df  lower.CL upper.CL\n Control      0.8801429 0.2249031 36 0.4240182 1.336268\n Experimental 0.9621429 0.2249031 36 0.5060182 1.418268\n\ndosage = ad lib:\n type            emmean        SE df  lower.CL upper.CL\n Control      0.7258571 0.2249031 36 0.2697324 1.181982\n Experimental 1.6201429 0.2249031 36 1.1640182 2.076268\n\nConfidence level used: 0.95 \n\n\nMeans and error bars\n\npd=position_dodge(width=0.05)\n\np2&lt;-ggplot(emm2,aes(x=dosage,y=emmean,shape=type, group=type, color=type))+\n  geom_point(position=pd,aes(shape=type), size=3,show.legend = FALSE)+\n  geom_errorbar(aes(ymin = emmean-SE, ymax = emmean+SE), width=0, position = pd,show.legend = FALSE)+\n  geom_line(aes(color=type), position=pd, linewidth=1.5)+\n  scale_color_uchicago(labels = c(\"Control\", \"Experimental\"))+\n    scale_linetype_manual(values=c(\"solid\", \"solid\"))+\n  labs(x = \"Food level\", y = \"Hg (mg/g dw\"\n       )+\n  theme_classic(base_size = 10)+\n  theme(\n    axis.text.x = element_text(color=\"black\",size=10),\n    axis.text.y= element_text(color=ac),\n    axis.line = element_line(color = ac),\n    axis.ticks = element_line(color = ac),\n        )+\n  theme(\n  legend.position = c(.6, .95),\n  legend.justification = c(\"right\", \"top\"),\n  legend.box.just = \"right\",\n  legend.margin = margin(6, 6, 6, 6),\n  legend.title = element_blank(),\n)\n\nCombine figures\n\np1+p2",
    "crumbs": [
      "Examples",
      "QK Box 7.1"
    ]
  },
  {
    "objectID": "examples/caballes.html",
    "href": "examples/caballes.html",
    "title": "QK Box 10.6",
    "section": "",
    "text": "Caballes et al. (2016) examined the effects of maternal nutrition (three treatments: starved or fed one of two coral genera: Acropora or Porites) on the larval biology of crown-of-thorns seastars. There were three female seastars nested within each treatment, 50 larvae reared from each female were placed into each of three glass culture jars and the lengths of ten larvae from each jar after four days were measured after 4 days. This fully balanced design has maternal nutrition as a fixed factor with three random levels of nesting: females within nutrition treatment, jars within females and individual larvae within jars.\nThe paper is here\nCaballes, C. F., Pratchett, M. S., Kerr, A. M. & Rivera-Posada, J. A. (2016). The role of maternal nutrition on oocyte size and quality, with respect to early larval development in the coral-eating starfish, Acanthaster planci. PLoS One, 11, e0158007.",
    "crumbs": [
      "Examples",
      "QK Box 10.6"
    ]
  },
  {
    "objectID": "examples/caballes.html#visualize-data",
    "href": "examples/caballes.html#visualize-data",
    "title": "QK Box 10.6",
    "section": "Visualize data",
    "text": "Visualize data\n\ncaballes_length %&gt;% \n  ggplot(aes(jar, length, color = female)) + \n  geom_point(alpha = 0.5) +\n  facet_wrap(~ diet, scales = \"free_x\")",
    "crumbs": [
      "Examples",
      "QK Box 10.6"
    ]
  },
  {
    "objectID": "examples/caballes.html#run-model",
    "href": "examples/caballes.html#run-model",
    "title": "QK Box 10.6",
    "section": "Run model",
    "text": "Run model\nNote: can’t get the aov commands to work for 3 level nested design\n\ncaballes.aov &lt;- aov(length~diet+Error(female/jar), caballes_length)\n\n\nFit as lm using OLS estimation\n\ncaballes.lm &lt;- lm(length ~ diet/female/jar, caballes_length)\nanova(caballes.lm)\n\nAnalysis of Variance Table\n\nResponse: length\n                 Df Sum Sq Mean Sq F value    Pr(&gt;F)    \ndiet              2 2.5334 1.26668 72.1624 &lt; 2.2e-16 ***\ndiet:female       6 0.3735 0.06224  3.5460  0.002199 ** \ndiet:female:jar  18 0.5265 0.02925  1.6664  0.045989 *  \nResiduals       243 4.2654 0.01755                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCheck diagnostics and lm output (not shown):\n\nplot(caballes.lm)\nsummary(caballes.lm)\n\nGet F and P values using correct denominators\n\n#Diet F\nf &lt;- 1.26668/0.06224\npf(f, df1 = 2, df2 = 6, lower.tail = FALSE)\n\n[1] 0.002120396\n\n#Females F\nf &lt;- 0.06224/0.02925\npf(f, df1 = 6, df2 = 18, lower.tail = FALSE)\n\n[1] 0.100229\n\n#Jars F\nf &lt;- 0.02925/0.01755\npf(f, df1 = 18, df2 = 243, lower.tail = FALSE)\n\n[1] 0.04594475\n\n\nVariance components from VCA\n\ncaballes.vca &lt;- anovaMM(length ~ diet/(female)/(jar), caballes_length)\ncaballes.vca\nVCAinference(caballes.vca, alpha = 0.05, VarVC = TRUE, ci.method = \"satterthwaite\")\n\n\n\nFit mixed effects model using REML/ML\n\ncaballes.lmer &lt;- lmer(length ~ diet + (1|female/jar), caballes_length)\nsummary(caballes.lmer)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: length ~ diet + (1 | female/jar)\n   Data: caballes_length\n\nREML criterion at convergence: -289.2\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.69014 -0.69659  0.01388  0.64143  2.59374 \n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n jar:female (Intercept) 0.00117  0.03420 \n female     (Intercept) 0.00110  0.03316 \n Residual               0.01755  0.13249 \nNumber of obs: 270, groups:  jar:female, 27; female, 9\n\nFixed effects:\n            Estimate Std. Error       df t value Pr(&gt;|t|)    \n(Intercept)  0.66279    0.01518  5.99986  43.652 9.68e-09 ***\ndiet1       -0.13588    0.02147  5.99986  -6.328 0.000728 ***\ndiet2        0.08301    0.02147  5.99986   3.866 0.008306 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n      (Intr) diet1 \ndiet1  0.000       \ndiet2  0.000 -0.500\n\n\nGet F-ratio for diet test using lmerTest\n\nanova(caballes.lmer, ddf = \"Kenward-Roger\")\n\nType III Analysis of Variance Table with Kenward-Roger's method\n      Sum Sq Mean Sq NumDF DenDF F value   Pr(&gt;F)   \ndiet 0.71442 0.35721     2     6   20.35 0.002121 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCI on variance components (remembering to square CIs from lmer which are in SD units)\n\ncaballes.ci &lt;- confint.merMod(caballes.lmer)\ncaballes.vc &lt;- (caballes.ci)^2\nprint(caballes.vc)\n\n                  2.5 %      97.5 %\n.sig01      0.000000000 0.004051007\n.sig02      0.000000000 0.003177589\n.sigma      0.014769187 0.021083239\n(Intercept) 0.404053917 0.475997080\ndiet1       0.030364676 0.009506432\ndiet2       0.001992217 0.014735036",
    "crumbs": [
      "Examples",
      "QK Box 10.6"
    ]
  },
  {
    "objectID": "examples/caballes.html#simplify-dataset-by-averaging-across-larvae-within-a-jar",
    "href": "examples/caballes.html#simplify-dataset-by-averaging-across-larvae-within-a-jar",
    "title": "QK Box 10.6",
    "section": "Simplify dataset by averaging across larvae within a jar",
    "text": "Simplify dataset by averaging across larvae within a jar\n\nd &lt;- caballes_length %&gt;% \n  group_by(diet, female, jar) %&gt;%\n  dplyr::summarise(mean = mean(length), \n            sd = sd(length), \n            n = n(), \n            se = sd / sqrt(n)) %&gt;% \n  ungroup()\n\nd %&gt;% \n  ggplot(aes(female, mean)) + \n  geom_point(alpha = 0.5) +\n  facet_wrap(~ diet, scales = \"free_x\") + \n  labs(y = \"Mean length (mm)\")\n\n\n\n\n\n\n\n\n\nFit nested ANOVA with OLS\nFemale is nested within treatment:\n\nd_aov &lt;- aov(mean~diet+Error(female), d)\nsummary(d_aov)\n\n\nError: female\n          Df  Sum Sq Mean Sq F value  Pr(&gt;F)   \ndiet       2 0.25334 0.12667   20.35 0.00212 **\nResiduals  6 0.03735 0.00622                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nError: Within\n          Df  Sum Sq  Mean Sq F value Pr(&gt;F)\nResiduals 18 0.05265 0.002925               \n\nm1 &lt;- lm(mean ~ diet / female, d)\nanova(m1)\n\nAnalysis of Variance Table\n\nResponse: mean\n            Df   Sum Sq  Mean Sq F value    Pr(&gt;F)    \ndiet         2 0.253336 0.126668 43.3035 1.323e-07 ***\ndiet:female  6 0.037346 0.006224  2.1279    0.1002    \nResiduals   18 0.052652 0.002925                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nGet F and P values using correct denominators. Note that I’m using broom:tidy to tidy the anova output, and lead() to calculate the new_F and new_P. This avoids extracting statistic (F-value) and df into new objects, or hard-coding the calculations.\n\ntidy(anova(m1)) %&gt;% \n  mutate(new_F = meansq / lead(meansq), \n         new_P = pf(new_F, df1 = df, df2 = lead(df), lower.tail = FALSE)) %&gt;% \n  kable(digits = 3)\n\n\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\nnew_F\nnew_P\n\n\n\n\ndiet\n2\n0.253\n0.127\n43.303\n0.0\n20.351\n0.002\n\n\ndiet:female\n6\n0.037\n0.006\n2.128\n0.1\n2.128\n0.100\n\n\nResiduals\n18\n0.053\n0.003\nNA\nNA\nNA\nNA\n\n\n\n\n\n\n\nFit mixed effects model using REML/ML\n\nm2 &lt;- lmer(mean ~ diet + (1|female), d)\nsummary(m2)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: mean ~ diet + (1 | female)\n   Data: d\n\nREML criterion at convergence: -58.6\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.05994 -0.46141  0.08908  0.48149  2.22410 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n female   (Intercept) 0.001100 0.03316 \n Residual             0.002925 0.05408 \nNumber of obs: 27, groups:  female, 9\n\nFixed effects:\n            Estimate Std. Error       df t value Pr(&gt;|t|)    \n(Intercept)  0.66279    0.01518  6.00000  43.653 9.68e-09 ***\ndiet1       -0.13588    0.02147  6.00000  -6.328 0.000728 ***\ndiet2        0.08301    0.02147  6.00000   3.866 0.008305 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n      (Intr) diet1 \ndiet1  0.000       \ndiet2  0.000 -0.500\n\nanova(m2, ddf = \"Kenward-Roger\")\n\nType III Analysis of Variance Table with Kenward-Roger's method\n      Sum Sq  Mean Sq NumDF DenDF F value   Pr(&gt;F)   \ndiet 0.11906 0.059528     2     6   20.35 0.002121 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "Examples",
      "QK Box 10.6"
    ]
  },
  {
    "objectID": "chapter_notes/chapter3.html",
    "href": "chapter_notes/chapter3.html",
    "title": "Chapter 3",
    "section": "",
    "text": "Questions for review and discussion, based on chapter 3 from Quinn and Keough 2023.",
    "crumbs": [
      "Chapter notes",
      "Chapter 3"
    ]
  },
  {
    "objectID": "chapter_notes/chapter2.html",
    "href": "chapter_notes/chapter2.html",
    "title": "Chapter 2",
    "section": "",
    "text": "Questions for review and discussion, based on chapter 2 from Quinn and Keough 2023.",
    "crumbs": [
      "Chapter notes",
      "Chapter 2"
    ]
  },
  {
    "objectID": "chapter_notes/chapter2.html#section",
    "href": "chapter_notes/chapter2.html#section",
    "title": "Chapter 2",
    "section": "2.1",
    "text": "2.1\nWhat is a sample?\nWhat is the difference between a statistic and a parameter?\nWhat is the difference between process and observation uncertainty?",
    "crumbs": [
      "Chapter notes",
      "Chapter 2"
    ]
  },
  {
    "objectID": "chapter_notes/chapter2.html#section-1",
    "href": "chapter_notes/chapter2.html#section-1",
    "title": "Chapter 2",
    "section": "2.2",
    "text": "2.2\nDefine the following terms:\n\nprobability\nsample space\nconditional probability\n\nDraw a Venn diagram representing the probability of three outcomes (A, B) in a sample space. Let C be mutually exclusive of A and B, but allow A and B to overlap. Use this diagram to visualize the idea of conditional probability, and relate it to the the mathematical equation for conditional probability.",
    "crumbs": [
      "Chapter notes",
      "Chapter 2"
    ]
  },
  {
    "objectID": "chapter_notes/chapter2.html#section-2",
    "href": "chapter_notes/chapter2.html#section-2",
    "title": "Chapter 2",
    "section": "2.3",
    "text": "2.3\nDefine the following terms:\n\nrandom variable\ndiscrete variable\ncontinuous variable\nprobability distribution\nprobability mass function\nprobability density function\n\nImagine you have counted rockfish along transects in the kelp forest. You are an avid diver, so you completed 1000 transects. Because these are count data, we will use a Poisson distribution (the support for the Poisson is non-negative integers). The average number of fish is 2 per transect. Note that in the Poisson, one parameter (\\(\\lambda\\)), governs the central tendency and the spread of the distribution (unlike, e.g., the Normal). This means the single parameter of the Poisson distribution is \\(\\lambda = 2\\).\nHere, we generate data according to our data story:\n\nset.seed(101)\nx &lt;- rpois(n = 1000, lambda = 2)\nhist(x, xlab = \"Rockfish per transect\",\n           main = expression(paste(\"1000 samples from a Poisson(\", lambda, \" = 2)\")))\n\n\n\n\n\n\n\n\nNow run these two lines of code. Why do these two expressions give you the same answer? (To force you to dig into the help files for these functions, I am not writing explicit code).\n\ndpois(0, 2) + dpois(1, 2) + dpois(2, 2)\nppois(2, 2)\n\nThe above code tells us that ~67% of the probability mass of a Poisson(2) distribution lies at or below 2. Returning to our random samples, let’s use quantile to figure out which of our values is at the 67th percentile in our data:\n\nquantile(x = x, probs = 0.67)\n\nNow let’s use qpois to identify the value at which we have 67% of the observations:\n\nqpois(p = 0.67, lambda = 2)\n\nFinally, let’s just tabulate the data to see if this all makes sense:\n\ntable(x)\nsum(table(x)[1:3]) / 1000\n\nPonder all of this until your understanding of the inter-relationships between d, p, q, r - pois is solid.\n\nPlotting distributions\nLet’s plot a Normal distribution centered at 0, with different standard deviations:\n\nx_grid &lt;- seq(-4, 4, 0.01)\n\nplot(x_grid, dnorm(x_grid, mean = 0, sd = 0.5), type = \"l\", \n     col = \"red\", xlab = \"x\", ylab = \"probability density\")\n\nlines(x_grid, dnorm(x_grid, mean = 0, sd = 1), type = \"l\", \n     col = \"blue\")\n\nlines(x_grid, dnorm(x_grid, mean = 0, sd = 1.5), type = \"l\", \n     col = \"black\")\n\n\n\n\n\n\n\n\nLet’s compare with a Student t-distribution:\n\nx_grid &lt;- seq(-3, 3, 0.01)\n\nplot(x_grid, dt(x_grid, df = 40), type = \"l\", \n     col = \"red\", xlab = \"x\", ylab = \"probability density\")\n\nlines(x_grid, dt(x_grid, df = 20), type = \"l\", \n     col = \"blue\")\n\nlines(x_grid, dt(x_grid, df = 10), type = \"l\", \n     col = \"black\")\n\nlines(x_grid, dnorm(x_grid, mean = 0, sd = 1), type = \"l\", \n     col = \"gray\", lty = 2)\n\n\n\n\n\n\n\n\nThe difference appears to be small. But the ‘thicker tails’ of the t-distribution dramatically increases the probability of extreme events, so-called black swans (e.g., in animal populations).\n\n\nMore practice\n\nFind the mean, variance, and 95% quantiles (i.e., 2.5% and 97.5% quantiles) of 1000 random draws from a Poisson distribution with \\(\\lambda=33\\).\nWhat is the probability \\(\\text P (X \\leq 6)\\) that a random draw from a Poisson distribution with \\(\\lambda = 4\\) will be less than or equal to 6?\nWhat is the probability \\(\\text P(X = 3)\\) of obtaining a value of 3 from a Binomial distribution with \\(p = 0.3\\) and \\(n = 5\\)?\nWhat is the probability \\(\\text P(-1.5 \\leq X \\leq 1.5)\\) that a value drawn from a standard normal distribution will be between -1.5 and 1.5? It may help to approach this visually.\nFind the value \\(x\\) that satisfies to \\(\\text P(X \\leq x) = 0.8\\), if \\(X\\) is a Gamma random variable with \\(k=2\\) and \\(\\theta = 1\\).",
    "crumbs": [
      "Chapter notes",
      "Chapter 2"
    ]
  },
  {
    "objectID": "chapter_notes/chapter2.html#section-3",
    "href": "chapter_notes/chapter2.html#section-3",
    "title": "Chapter 2",
    "section": "2.4",
    "text": "2.4\nWhat is estimation?\nWhat makes a good estimator?\nCompare and contrast these frequentist estimation methods:\n\nordinary least squares\nmaximum likelihood\nresampling (bootstrap)\n\nPopulation parameters and sample statistics\n\nBox 2.2\nWhat is a sampling distribution?\nWhat does the central limit theorem tell us about the shape of a sampling distribution (e.g., a distribution of sample means)?\nWhat happens to the standard error of the mean as you increase sample size?\nWhen calculating the confidence interval for \\(\\mu\\), should you use a Normal distribution or a \\(t\\) distribution? Why? How do these two approaches compare?\nIn the frequentist world, parameters are fixed (but unknowable). In this context, how do you interpret a frequentist confidence interval?\nBootstrap methods and Box 2.3",
    "crumbs": [
      "Chapter notes",
      "Chapter 2"
    ]
  },
  {
    "objectID": "chapter_notes/chapter2.html#section-4",
    "href": "chapter_notes/chapter2.html#section-4",
    "title": "Chapter 2",
    "section": "2.5",
    "text": "2.5\nHypothesis testing",
    "crumbs": [
      "Chapter notes",
      "Chapter 2"
    ]
  },
  {
    "objectID": "chapter_notes/chapter2.html#section-5",
    "href": "chapter_notes/chapter2.html#section-5",
    "title": "Chapter 2",
    "section": "2.6",
    "text": "2.6\nComments on frequentist inference",
    "crumbs": [
      "Chapter notes",
      "Chapter 2"
    ]
  },
  {
    "objectID": "chapter_notes/chapter2.html#section-6",
    "href": "chapter_notes/chapter2.html#section-6",
    "title": "Chapter 2",
    "section": "2.7",
    "text": "2.7\nBayesian inference",
    "crumbs": [
      "Chapter notes",
      "Chapter 2"
    ]
  },
  {
    "objectID": "examples/low.html",
    "href": "examples/low.html",
    "title": "QK Box 2.2",
    "section": "",
    "text": "Low et al (2016) examined the effects of two different anesthetics on aspects of the physiology of the mouse. Twelve mice were anesthetized with isoflurane and eleven mice were anesthetized with alpha chloralose and blood CO2 levels were recorded after 120 minutes. The H0 was that there is no difference between the anesthetics in the mean blood CO2 level. This is an independent comparison because individual mice were only given one of the two anesthetics.",
    "crumbs": [
      "Examples",
      "QK Box 2.2"
    ]
  },
  {
    "objectID": "examples/low.html#preliminaries",
    "href": "examples/low.html#preliminaries",
    "title": "QK Box 2.2",
    "section": "Preliminaries",
    "text": "Preliminaries\nFirst, load the required packages (tidyverse, RMisc, MKinfer, car, emmeans)\nImport low data file\n\nlow &lt;- read.csv(\"data/lowco2.csv\")\nlow\n\n   anesth co2\n1     iso  43\n2     iso  35\n3     iso  50\n4     iso  39\n5     iso  56\n6     iso  54\n7     iso  39\n8     iso  51\n9     iso  49\n10    iso  54\n11    iso  51\n12    iso  79\n13     ac  60\n14     ac  53\n15     ac  54\n16     ac  73\n17     ac  64\n18     ac  95\n19     ac  57\n20     ac  80\n21     ac 115\n22     ac  79\n23     ac  50",
    "crumbs": [
      "Examples",
      "QK Box 2.2"
    ]
  },
  {
    "objectID": "examples/low.html#get-summary-statistics-by-anesthetic",
    "href": "examples/low.html#get-summary-statistics-by-anesthetic",
    "title": "QK Box 2.2",
    "section": "Get summary statistics by anesthetic",
    "text": "Get summary statistics by anesthetic\n\nlow_stats &lt;- summarySE(data=low,measurevar=\"co2\", groupvars=\"anesth\")\nlow_stats\n\n  anesth  N      co2       sd       se        ci\n1     ac 11 70.90909 20.20126 6.090909 13.571391\n2    iso 12 50.00000 11.39378 3.289100  7.239261\n\nlow %&gt;% dplyr::count(anesth)\n\n  anesth  n\n1     ac 11\n2    iso 12\n\nlow %&gt;%  \n  group_by(anesth) %&gt;% \n  dplyr::summarise(n = n(), \n            mean = mean(co2),\n            median = median(co2),\n            sd = sd(co2), \n            variance = var(co2), \n            se = sd / sqrt(n), \n            CI_upper = mean + se * qt(p = 0.975, df = n-1), \n            CI_lower = mean + se * qt(p = 0.025, df = n-1), \n            CI = se * qt(p = 0.975, df = n-1), \n            upper = mean + CI, \n            lower = mean - CI\n            )\n\n# A tibble: 2 × 12\n  anesth     n  mean median    sd variance    se CI_upper CI_lower    CI upper\n  &lt;chr&gt;  &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 ac        11  70.9   64    20.2     408.  6.09     84.5     57.3 13.6   84.5\n2 iso       12  50     50.5  11.4     130.  3.29     57.2     42.8  7.24  57.2\n# ℹ 1 more variable: lower &lt;dbl&gt;\n\n\nPlay around with df to see how the z-multiplier changes when using the T-distribution to calculate the 95% confidence interval.\n\n# Standard normal distribution\nqnorm(p = 0.025)\n\n[1] -1.959964\n\nqnorm(p = 0.975)\n\n[1] 1.959964\n\n# Student-t distribution\nqt(p = 0.025, df = 100)\n\n[1] -1.983972\n\nqt(p = 0.975, df = 100)\n\n[1] 1.983972",
    "crumbs": [
      "Examples",
      "QK Box 2.2"
    ]
  },
  {
    "objectID": "examples/low.html#plot-data",
    "href": "examples/low.html#plot-data",
    "title": "QK Box 2.2",
    "section": "Plot data",
    "text": "Plot data\n\nlow %&gt;% \n  ggplot(aes(anesth, co2)) + \n  geom_point(alpha = 0.5) + \n  theme_qk()",
    "crumbs": [
      "Examples",
      "QK Box 2.2"
    ]
  },
  {
    "objectID": "examples/low.html#fit-model-and-get-effect-size",
    "href": "examples/low.html#fit-model-and-get-effect-size",
    "title": "QK Box 2.2",
    "section": "Fit model and get effect size",
    "text": "Fit model and get effect size\n\nlow.aov &lt;- aov(co2~anesth,data=low)\ntidy(low.aov, conf.int=TRUE)\n\n# A tibble: 2 × 6\n  term         df sumsq meansq statistic  p.value\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 anesth        1 2509.  2509.      9.56  0.00552\n2 Residuals    21 5509.   262.     NA    NA      \n\nlow.emm &lt;- emmeans(low.aov,\"anesth\")\neff_size(low.emm, sigma=sigma(low.aov), edf=df.residual(low.aov))\n\n contrast effect.size    SE df lower.CL upper.CL\n ac - iso        1.29 0.463 21    0.329     2.25\n\nsigma used for effect sizes: 16.2 \nConfidence level used: 0.95 \n\n\nNote that we’ve chosen to show a standardized effect size, using the pooled variance from the analysis of variance - Residual MS = 262.44, and √262.44 = 16.2",
    "crumbs": [
      "Examples",
      "QK Box 2.2"
    ]
  },
  {
    "objectID": "examples/low.html#test-variances",
    "href": "examples/low.html#test-variances",
    "title": "QK Box 2.2",
    "section": "Test variances",
    "text": "Test variances\n\nleveneTest(co2 ~ anesth, low)\n\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(&gt;F)\ngroup  1   2.604 0.1215\n      21               \n\n\n\nt-test for equal variances\n\nt.test(co2~anesth,var.equal=TRUE, data=low)\n\n\n    Two Sample t-test\n\ndata:  co2 by anesth\nt = 3.0927, df = 21, p-value = 0.005515\nalternative hypothesis: true difference in means between group ac and group iso is not equal to 0\n95 percent confidence interval:\n  6.849172 34.969010\nsample estimates:\n mean in group ac mean in group iso \n         70.90909          50.00000 \n\n\n\n\nt-test for separate variances\n\nt.test(co2~anesth,data=low)\n\n\n    Welch Two Sample t-test\n\ndata:  co2 by anesth\nt = 3.0206, df = 15.485, p-value = 0.008362\nalternative hypothesis: true difference in means between group ac and group iso is not equal to 0\n95 percent confidence interval:\n  6.194866 35.623316\nsample estimates:\n mean in group ac mean in group iso \n         70.90909          50.00000",
    "crumbs": [
      "Examples",
      "QK Box 2.2"
    ]
  },
  {
    "objectID": "examples/low.html#wilcoxon-mann-whitney",
    "href": "examples/low.html#wilcoxon-mann-whitney",
    "title": "QK Box 2.2",
    "section": "Wilcoxon-Mann-Whitney",
    "text": "Wilcoxon-Mann-Whitney\n\nwilcox.test(co2~anesth,data=low)\n\nWarning in wilcox.test.default(x = DATA[[1L]], y = DATA[[2L]], ...): cannot\ncompute exact p-value with ties\n\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  co2 by anesth\nW = 114, p-value = 0.003398\nalternative hypothesis: true location shift is not equal to 0\n\nsum(rank(low$co2)[low$anesth==\"ac\"])\n\n[1] 180\n\nsum(rank(low$co2)[low$anesth==\"iso\"])\n\n[1] 96",
    "crumbs": [
      "Examples",
      "QK Box 2.2"
    ]
  },
  {
    "objectID": "examples/lowboot.html",
    "href": "examples/lowboot.html",
    "title": "QK Box 2.3",
    "section": "",
    "text": "This box continues with the Low et al. anesthetic example from Box 2.2\n\nPreliminaries\nPackages: MKinfer, resample\n\nlibrary(MKinfer)\nlibrary(resample)\nlibrary(tidyverse)\n\nUse low data:\n\nlow &lt;- read_csv(\"data/lowco2.csv\")\n\n\n\nGet jackknife SE for two groups\n\nlow1 &lt;- subset(low,anesth==\"iso\")\njackknife(low1$co2,mean)\n## Call:\n## jackknife(data = low1$co2, statistic = mean)\n## Replications: 12\n## \n## Summary Statistics:\n##      Observed     SE Mean Bias\n## mean       50 3.2891   50    0\nlow2 &lt;- subset(low,anesth==\"ac\")\njackknife(low2$co2,mean)\n## Call:\n## jackknife(data = low2$co2, statistic = mean)\n## Replications: 11\n## \n## Summary Statistics:\n##      Observed       SE     Mean         Bias\n## mean 70.90909 6.090909 70.90909 1.421085e-13\n\n\n\nGet bootstrap SE and 95%CI\n\nlow1boot &lt;- bootstrap(low1$co2,mean,R=9999)\nlow1boot\n## Call:\n## bootstrap(data = low1$co2, statistic = mean, R = 9999)\n## Replications: 9999\n## \n## Summary Statistics:\n##      Observed       SE     Mean        Bias\n## mean       50 3.146432 49.96038 -0.03962063\nCI.percentile(low1boot, probs=c(0.025,0.975))\n##          2.5%    97.5%\n## mean 43.66667 58.11949\nCI.bca(low1boot, probs=c(0.025,0.975))\n##          2.5%    97.5%\n## mean 44.58333 60.16667\n\nlow2boot &lt;- bootstrap(low2$co2,mean,R=9999)\nlow2boot\n## Call:\n## bootstrap(data = low2$co2, statistic = mean, R = 9999)\n## Replications: 9999\n## \n## Summary Statistics:\n##      Observed       SE     Mean       Bias\n## mean 70.90909 5.850436 70.92512 0.01602888\nCI.percentile(low2boot, probs=c(0.025,0.975))\n##      2.5%    97.5%\n## mean   59 85.63636\nCI.bca(low2boot, probs=c(0.025,0.975))\n##          2.5%    97.5%\n## mean 60.27273 88.09091\n\n\n\nGet bootstrap SE and CI on difference\n\nlowboot &lt;- bootstrap2(low$co2,mean,treatment=low$anesth,R=9999,ratio=FALSE)\nlowboot\n## Call:\n## bootstrap2(data = low$co2, statistic = mean, treatment = low$anesth, \n##     R = 9999, ratio = FALSE)\n## Replications: 9999\n## Two samples, sample sizes are 11 12\n## \n## Summary Statistics for the difference between samples 1 and 2:\n##              Observed       SE     Mean        Bias\n## mean: ac-iso 20.90909 6.666543 20.86282 -0.04626826\nCI.percentile(lowboot, probs=c(0.025,0.975))\n##                  2.5%    97.5%\n## mean: ac-iso 6.016849 36.92163\n\n\n\nRandomization test\n\nperm.t.test(co2~anesth, data=low, R=9999, paired= FALSE)\n## \n##  Permutation Welch Two Sample t-test\n## \n## data:  co2 by anesth\n## (Monte-Carlo) permutation p-value = 0.005001 \n## permutation difference of means (SE) = 20.92096 (7.964221) \n## 95 percent (Monte-Carlo) permutation percentile confidence interval:\n##   5.75000 36.06818\n## \n## Results without permutation:\n## t = 3.0206, df = 15.485, p-value = 0.008362\n## alternative hypothesis: true difference in means is not equal to 0\n## 95 percent confidence interval:\n##   6.194866 35.623316\n## sample estimates:\n##  mean in group ac mean in group iso \n##          70.90909          50.00000",
    "crumbs": [
      "Examples",
      "QK Box 2.3"
    ]
  },
  {
    "objectID": "examples/serpulid.html",
    "href": "examples/serpulid.html",
    "title": "QK Box 6.7 & 6.11",
    "section": "",
    "text": "Keough and Raimondi (1995) set up an experiment to examine the response of serpulid (polychaete worms) larvae to four types of biofilms on hard substrata in shallow marine waters. The four treatments were: sterile substrata, biofilms developed in the field with a net (to keep invertebrates), biofilms developed in the lab, and lab biofilms with a covering net (as a control for the presence of a net). The substrata were left for one week, and then the newly settled worms identified and counted. To control for small numbers of larvae passing through the netting during the conditioning period, they used an additional treatment, which was netted, and returned to the laboratory after one week and censused. The values of this treatment were used to adjust the numbers in the treatment that started in the field.\nThe paper is here and the data file (also used in first edition) is here\nKeough, M. J. & Raimondi, P. T. (1995). Responses of settling invertebrate larvae to bioorganic films: effects of different types of films. Journal of Experimental Marine Biology and Ecology, 185, 235-53.",
    "crumbs": [
      "Examples",
      "QK Box 6.7 & 6.11"
    ]
  },
  {
    "objectID": "examples/serpulid.html#generate-planned-comparisons",
    "href": "examples/serpulid.html#generate-planned-comparisons",
    "title": "QK Box 6.7 & 6.11",
    "section": "Generate planned comparisons",
    "text": "Generate planned comparisons\nWe’re doing this by defining contrasts and refitting the model using this contrast. The planned comparison then appears as the first effect when we look at the model fitting (i.e., film1).\n\nUL vs NL\n\n# Default\ncontrasts(serpulid$film)\n\n   NL SL UL\nF   0  0  0\nNL  1  0  0\nSL  0  1  0\nUL  0  0  1\n\n# Change it\ncontrasts(serpulid$film) &lt;- c(0,1,0,-1)\ncontrasts(serpulid$film)\n\n   [,1]       [,2]       [,3]\nF     0 -0.5000000 -0.7071068\nNL    1 -0.1666667  0.4714045\nSL    0  0.8333333 -0.2357023\nUL   -1 -0.1666667  0.4714045\n\n# Refit the model with new contrasts\nserpulid.aov &lt;- aov(lserp~film, data=serpulid)\nsummary(serpulid.aov)\n\n            Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nfilm         3 0.2458 0.08192   6.015 0.00331 **\nResiduals   24 0.3269 0.01362                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary.lm(serpulid.aov) \n\n\nCall:\naov(formula = lserp ~ film, data = serpulid)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.22929 -0.06500  0.01843  0.07054  0.19557 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.09039    0.02206  94.780  &lt; 2e-16 ***\nfilm1        0.02479    0.03119   0.795  0.43461    \nfilm2       -0.16407    0.04411  -3.720  0.00107 ** \nfilm3        0.08344    0.04411   1.892  0.07067 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1167 on 24 degrees of freedom\nMultiple R-squared:  0.4292,    Adjusted R-squared:  0.3578 \nF-statistic: 6.015 on 3 and 24 DF,  p-value: 0.003314\n\n\n\n\nF vs average (NL & UL)\n\ncontrasts(serpulid$film) &lt;- c(2,-1,0,-1)\ncontrasts(serpulid$film)\n\n   [,1]       [,2]        [,3]\nF     2 -0.2867757  0.03306064\nNL   -1 -0.3677574 -0.66939360\nSL    0  0.8603272 -0.09918192\nUL   -1 -0.2057940  0.73551488\n\n# Refit the model with new contrasts\nserpulid.aov &lt;- aov(lserp~film, data=serpulid)\nsummary(serpulid.aov)\n\n            Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nfilm         3 0.2458 0.08192   6.015 0.00331 **\nResiduals   24 0.3269 0.01362                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary.lm(serpulid.aov) \n\n\nCall:\naov(formula = lserp ~ film, data = serpulid)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.22929 -0.06500  0.01843  0.07054  0.19557 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.09039    0.02206  94.780  &lt; 2e-16 ***\nfilm1       -0.01455    0.01801  -0.808 0.427118    \nfilm2       -0.18341    0.04411  -4.158 0.000353 ***\nfilm3       -0.01414    0.04411  -0.321 0.751322    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1167 on 24 degrees of freedom\nMultiple R-squared:  0.4292,    Adjusted R-squared:  0.3578 \nF-statistic: 6.015 on 3 and 24 DF,  p-value: 0.003314\n\n\n\n\nSL vs average (F & NL & UL)\n\ncontrasts(serpulid$film) &lt;- c(-1,-1,3,-1)\ncontrasts(serpulid$film)\n\n   [,1]        [,2]       [,3]\nF    -1 -0.67052910 -0.4658942\nNL   -1  0.73874075 -0.3477481\nSL    3  0.00000000  0.0000000\nUL   -1 -0.06821164  0.8136423\n\n# Refit the model with new contrasts\nserpulid.aov &lt;- aov(lserp~film, data=serpulid)\nserpulid.aov\n\nCall:\n   aov(formula = lserp ~ film, data = serpulid)\n\nTerms:\n                     film Residuals\nSum of Squares  0.2457707 0.3268840\nDeg. of Freedom         3        24\n\nResidual standard error: 0.1167055\nEstimated effects are balanced\n\nsummary.lm(serpulid.aov) \n\n\nCall:\naov(formula = lserp ~ film, data = serpulid)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.22929 -0.06500  0.01843  0.07054  0.19557 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.090393   0.022055  94.780  &lt; 2e-16 ***\nfilm1       -0.052131   0.012734  -4.094 0.000415 ***\nfilm2        0.049265   0.044111   1.117 0.275117    \nfilm3       -0.008453   0.044111  -0.192 0.849643    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1167 on 24 degrees of freedom\nMultiple R-squared:  0.4292,    Adjusted R-squared:  0.3578 \nF-statistic: 6.015 on 3 and 24 DF,  p-value: 0.003314",
    "crumbs": [
      "Examples",
      "QK Box 6.7 & 6.11"
    ]
  },
  {
    "objectID": "examples/serpulid.html#diagnostics-for-untransformed-data",
    "href": "examples/serpulid.html#diagnostics-for-untransformed-data",
    "title": "QK Box 6.7 & 6.11",
    "section": "Diagnostics for untransformed data",
    "text": "Diagnostics for untransformed data\nWe used log-transformed data to match the original paper, but if analysing these data from first principles, we’d look at the raw data first to decide which form of model or transformation to use.\n\nserpraw.aov &lt;- aov(serp~film, data=serpulid)\nplot(serpraw.aov)",
    "crumbs": [
      "Examples",
      "QK Box 6.7 & 6.11"
    ]
  },
  {
    "objectID": "examples/serpulid.html#information-for-power-analysis-using-spirorbids-bugula",
    "href": "examples/serpulid.html#information-for-power-analysis-using-spirorbids-bugula",
    "title": "QK Box 6.7 & 6.11",
    "section": "Information for power analysis using spirorbids, Bugula",
    "text": "Information for power analysis using spirorbids, Bugula\nThese calculations are used for Box 6.11, where we consider data for two other invertebrate groups, spirorbid polychaetes and bryozoans in the genus Bugula, mainly B. neritina.\n\n\n\nSpirorbid polychaete worm, c. 2mm diameter. Mick Keough \n\n\n\nRecently metamorphosed bryozoan, Bugula. Approximately 1.5 mm high. Mick Keough \n\nRequired information\nWe need to run the analysis on each of these groups, to get two important pieces of information. We need estimates of the variance, and we generally use the residual mean square. We also want an estimate of a baseline for calculating a hypothetical Effect Size. In the context of this question, we’ll use the means for unfilmed surfaces, as we are thinking about the potential for our treatments to increase recruitment.\n\nboxplot(spir~film, data=serpulid)\n\n\n\n\n\n\n\nboxplot(bugula~film, data=serpulid)\n\n\n\n\n\n\n\nspir.aov&lt;-aov(spir~film, data=serpulid)\nplot(spir.aov)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsummary(spir.aov)\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)\nfilm         3  6.507   2.169   1.678  0.198\nResiduals   24 31.022   1.293               \n\nbugula.aov&lt;-aov(bugula~film, data=serpulid)\nplot(bugula.aov)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsummary(bugula.aov)\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)\nfilm         3   22.8   7.587   0.338  0.798\nResiduals   24  538.3  22.429               \n\nspirmean&lt;-summarySE(data=serpulid,measurevar = \"spir\", groupvars = \"film\")\nspirmean\n\n  film N      spir        sd        se        ci\n1    F 7 0.5357143 1.0818442 0.4088987 1.0005390\n2   NL 7 0.5714286 0.9759001 0.3688556 0.9025570\n3   SL 7 1.1428571 1.0690450 0.4040610 0.9887017\n4   UL 7 1.7142857 1.3801311 0.5216405 1.2764084\n\nbugmean&lt;-summarySE(data=serpulid,measurevar = \"bugula\", groupvars = \"film\")\nbugmean\n\n  film N   bugula       sd       se       ci\n1    F 7 6.982143 5.520524 2.086562 5.105634\n2   NL 7 8.857143 4.740906 1.791894 4.384607\n3   SL 7 6.857143 4.740906 1.791894 4.384607\n4   UL 7 6.571429 3.779645 1.428571 3.495588\n\n\n\n\nPower calculations\nThere are two scenarios in Box 6.11. Both involve a doubling of settlement from the base treatment SL above. In the first scenario, one treatment is 6.86 and the others are 13.72. In the second scenario, treatment means are spaced evenly between 6.86 and 13.72.\n\nalphasq1&lt;-3*var(c(6.86, 13.72, 13.72,13.72))\nalphasq2&lt;-3*var(c(6.86, 9.14, 11.44,13.72))\nmsres=22.43\nn=7\np=4\nlambda1&lt;-n*alphasq1/msres\nlambda1\n\n[1] 11.01484\n\nlambda2&lt;-n*alphasq2/msres\nlambda2\n\n[1] 8.168685\n\nf1&lt;-sqrt(alphasq1/p/msres)\nf1\n\n[1] 0.6272059\n\nf2&lt;-sqrt(alphasq2/p/msres)\nf2\n\n[1] 0.5401285\n\n#For scenario 1, λ= 11.01 and Cohens *f* = 0.627\n#For scenario 2, λ= 8.16 and Cohens *f* = 0.54\n# scenario 1: power\npwr.anova.test(k=p,f=f1,sig.level=0.05, n=n)\n\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 4\n              n = 7\n              f = 0.6272059\n      sig.level = 0.05\n          power = 0.7298327\n\nNOTE: n is number in each group\n\n#scenario 2: power\npwr.anova.test(k=p,f=f2,sig.level=0.05, n=n)\n\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 4\n              n = 7\n              f = 0.5401285\n      sig.level = 0.05\n          power = 0.5860053\n\nNOTE: n is number in each group\n\n#scenario 1: required sample size\npwr.anova.test(k=p,f=f1,sig.level=0.05, power=0.8)\n\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 4\n              n = 7.976459\n              f = 0.6272059\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: n is number in each group\n\n#scenario 2: required sample size\npwr.anova.test(k=p,f=f2,sig.level=0.05, power=0.8)\n\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 4\n              n = 10.37355\n              f = 0.5401285\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: n is number in each group",
    "crumbs": [
      "Examples",
      "QK Box 6.7 & 6.11"
    ]
  },
  {
    "objectID": "examples/stokes.html",
    "href": "examples/stokes.html",
    "title": "QK Box 11.1",
    "section": "",
    "text": "Stokes et al. (2014) studied the neurotoxin tetrodotoxin (TTX) in flatworms. The between-plots factor was flatworm species (fixed with two groups: Bipalium adventitium and Bipalium kewense) with individual flatworms (plots) nested within species. The within-plots factor was body segment (fixed with three groups: head, anterior body, posterior body) and each segment represented a “sub-plot”. The response variable was the TTX concentration of tissue adjusted for weight. The main research questions were about the fixed effects of species, body segment and their interaction on TTX concentration, but the analyses also provide information about the variances associated with the random effects of individual within species and the random interaction between individuals within species and body segment.\nBipalium advenitium. Yale Peabody Museum, , via Wikimedia Commons\nBipalium kewense. Don Loarie , via Wikimedia Commons\nThe data are here\nStokes, A. N., Ducey, P. K., Neuman-Lee, L., Hanifin, C. T., French, S. S., Pfrender, M. E., Brodie, E. D., 3rd & Brodie, E. D., Jr. (2014). Confirmation and distribution of tetrodotoxin for the first time in terrestrial invertebrates: two terrestrial flatworm species (Bipalium adventitium and Bipalium kewense). PLoS One, 9, e100718.",
    "crumbs": [
      "Examples",
      "QK Box 11.1"
    ]
  },
  {
    "objectID": "examples/stokes.html#fit-full-model-with-logttxweight",
    "href": "examples/stokes.html#fit-full-model-with-logttxweight",
    "title": "QK Box 11.1",
    "section": "Fit full model with log(ttxweight)",
    "text": "Fit full model with log(ttxweight)\n\nstokes3.aov &lt;- aov(lttxweight~species*segment+Error(indiv), stokes)\nsummary(stokes3.aov)\n\n\nError: indiv\n          Df Sum Sq Mean Sq F value Pr(&gt;F)  \nspecies    1 0.8087  0.8087    6.17 0.0323 *\nResiduals 10 1.3107  0.1311                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nError: Within\n                Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nsegment          2 17.149   8.575  46.965 2.78e-08 ***\nspecies:segment  2  0.074   0.037   0.202    0.819    \nResiduals       20  3.652   0.183                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nUse ez for comparison with type 3 SS - same result as design is balanced\n\nezstokes &lt;- ezANOVA(data=stokes, dv=lttxweight, wid=indiv, within=segment, between=species, type=3)\nprint(ezstokes)\n\n$ANOVA\n           Effect DFn DFd          F            p p&lt;.05        ges\n2         species   1  10  6.1699328 3.232426e-02     * 0.14012958\n3         segment   2  20 46.9649253 2.779231e-08     * 0.77558408\n4 species:segment   2  20  0.2021994 8.185815e-01       0.01466111\n\n$`Mauchly's Test for Sphericity`\n           Effect        W         p p&lt;.05\n3         segment 0.719343 0.2270974      \n4 species:segment 0.719343 0.2270974      \n\n$`Sphericity Corrections`\n           Effect       GGe        p[GG] p[GG]&lt;.05       HFe        p[HF]\n3         segment 0.7808492 6.940839e-07         * 0.8993921 1.214771e-07\n4 species:segment 0.7808492 7.654586e-01           0.8993921 7.963230e-01\n  p[HF]&lt;.05\n3         *\n4          \n\n\n\nGet var components using OLS\nNote that these estimates treat B(A)*C as the residual for B(A) vc\n\nstokes2 &lt;- as.data.frame(stokes)\nstokes.vca &lt;- anovaMM(lttxweight~species/(indiv)+segment+species*segment, NegVC=TRUE, stokes2)\nstokes.vca\n\n\n\nANOVA-Type Estimation of Mixed Model:\n--------------------------------------\n\n    [Fixed Effects]\n\n                         int          speciesBadventitium \n                   -0.385414                     0.326190 \n             speciesBkewense                     segmentb \n                    0.000000                     0.272682 \n                    segmenth                     segmentp \n                    1.667431                     0.000000 \nspeciesBadventitium:segmentb     speciesBkewense:segmentb \n                    0.068888                     0.000000 \nspeciesBadventitium:segmenth     speciesBkewense:segmenth \n                   -0.148197                     0.000000 \nspeciesBadventitium:segmentp     speciesBkewense:segmentp \n                    0.000000                     0.000000 \n\n\n    [Variance Components]\n\n  Name          DF        SS       MS       VC       %Total     SD      \n1 total         29.367096                   0.165407 100        0.406702\n2 species:indiv 10        1.310664 0.131066 -0.01717 -10.380622 0       \n3 error         20        3.651543 0.182577 0.182577 110.380622 0.42729 \n  CV[%]     \n1 98.914014 \n2 0         \n3 103.921222\n\nMean: 0.411168 (N = 36) \n\nExperimental Design: balanced  |  Method: ANOVA\n\nVCAinference(stokes.vca, alpha=0.05, ci.method=\"satterthwaite\")\n\n\n\n\nInference from Mixed Model Fit\n------------------------------\n\n&gt; VCA Result:\n-------------\n\n    [Fixed Effects]\n\n                         int          speciesBadventitium \n                     -0.3854                       0.3262 \n             speciesBkewense                     segmentb \n                      0.0000                       0.2727 \n                    segmenth                     segmentp \n                      1.6674                       0.0000 \nspeciesBadventitium:segmentb     speciesBkewense:segmentb \n                      0.0689                       0.0000 \nspeciesBadventitium:segmenth     speciesBkewense:segmenth \n                     -0.1482                       0.0000 \nspeciesBadventitium:segmentp     speciesBkewense:segmentp \n                      0.0000                       0.0000 \n\n\n    [Variance Components]\n\n  Name          DF      SS     MS     VC      %Total   SD     CV[%]   \n1 total         29.3671               0.1654  100      0.4067 98.914  \n2 species:indiv 10      1.3107 0.1311 -0.0172 -10.3806 0      0       \n3 error         20      3.6515 0.1826 0.1826  110.3806 0.4273 103.9212\n\nMean: 0.4112 (N = 36) \n\nExperimental Design: balanced  |  Method: ANOVA\n\n\n&gt; VC:\n-----\n              Estimate      DF CI LCL CI UCL One-Sided LCL One-Sided UCL\ntotal           0.1654 29.3671 0.1052 0.2976        0.1130        0.2699\nspecies:indiv  -0.0172  0.7840                                          \nerror           0.1826 20.0000 0.1069 0.3807        0.1163        0.3365\n\n&gt; SD:\n-----\n              Estimate      DF CI LCL CI UCL One-Sided LCL One-Sided UCL\ntotal           0.4067 29.3671 0.3243 0.5456        0.3361        0.5195\nspecies:indiv   0.0000  0.7840                                          \nerror           0.4273 20.0000 0.3269 0.6170        0.3410        0.5801\n\n&gt; CV[%]:\n--------\n              Estimate      DF  CI LCL   CI UCL One-Sided LCL One-Sided UCL\ntotal          98.9140 29.3671 78.8754 132.6887       81.7401      126.3578\nspecies:indiv   0.0000  0.7840                                             \nerror         103.9212 20.0000 79.5059 150.0695       82.9244      141.0874\n\n\n95% Confidence Level  |  CIs for negative VCs excluded  \nSatterthwaite methodology used for computing CIs",
    "crumbs": [
      "Examples",
      "QK Box 11.1"
    ]
  },
  {
    "objectID": "examples/stokes.html#fit-mixed-effects-models-with-lmer",
    "href": "examples/stokes.html#fit-mixed-effects-models-with-lmer",
    "title": "QK Box 11.1",
    "section": "Fit mixed effects models with lmer",
    "text": "Fit mixed effects models with lmer\n\nstokes.lmer &lt;- lmer(lttxweight~species+segment+species*segment+(1|indiv), REML=TRUE, stokes)\n\nCheck residuals from lmer plot\n\nplot(stokes.lmer)\n\n\n\n\n\n\n\nsummary(stokes.lmer, ddf=\"Kenward-Roger\")\n\nLinear mixed model fit by REML. t-tests use Kenward-Roger's method [\nlmerModLmerTest]\nFormula: lttxweight ~ species + segment + species * segment + (1 | indiv)\n   Data: stokes\n\nREML criterion at convergence: 50.5\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.81144 -0.39960  0.08694  0.72676  1.72771 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n indiv    (Intercept) 0.0000   0.0000  \n Residual             0.1654   0.4067  \nNumber of obs: 36, groups:  indiv, 12\n\nFixed effects:\n                  Estimate Std. Error       df t value Pr(&gt;|t|)    \n(Intercept)        0.41117    0.06778 10.00000   6.066 0.000121 ***\nspecies1           0.14988    0.06778 10.00000   2.211 0.051464 .  \nsegment1           0.95985    0.09586 20.00000  10.013  3.1e-09 ***\nsegment2          -0.32636    0.09586 20.00000  -3.405 0.002812 ** \nspecies1:segment1 -0.06088    0.09586 20.00000  -0.635 0.532568    \nspecies1:segment2  0.04766    0.09586 20.00000   0.497 0.624467    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) specs1 sgmnt1 sgmnt2 spc1:1\nspecies1     0.000                            \nsegment1     0.000  0.000                     \nsegment2     0.000  0.000 -0.500              \nspcs1:sgmn1  0.000  0.000  0.000  0.000       \nspcs1:sgmn2  0.000  0.000  0.000  0.000 -0.500\noptimizer (nloptwrap) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\nanova(stokes.lmer, ddf=\"Kenward-Roger\")\n\nType III Analysis of Variance Table with Kenward-Roger's method\n                 Sum Sq Mean Sq NumDF DenDF F value    Pr(&gt;F)    \nspecies          0.8087  0.8087     1    10  4.8890   0.05146 .  \nsegment         17.1494  8.5747     2    20 51.8402 1.223e-08 ***\nspecies:segment  0.0738  0.0369     2    20  0.2232   0.80193    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nContrast head vs anterior and anterior vs posterior segments\n\nlibrary(emmeans)\nstokes.emm &lt;- emmeans(stokes.lmer, ~segment)\nstokes.con &lt;- contrast(stokes.emm, \"consec\")\nsummary(stokes.con, adjust=\"none\")\n\n contrast estimate    SE df t.ratio p.value\n b - h      -1.286 0.166 20  -7.747  &lt;.0001\n p - b      -0.307 0.166 20  -1.850  0.0792\n\nResults are averaged over the levels of: species \nDegrees-of-freedom method: kenward-roger \n\n\n\n\nGet variance components with CIs\n\nstokes.ci &lt;- confint.merMod(stokes.lmer)\nstokes.vc &lt;- (stokes.ci)^2\nprint(stokes.vc)\n\n                         2.5 %     97.5 %\n.sig01            0.0000000000 0.04989851\n.sigma            0.0897577507 0.22735990\n(Intercept)       0.0821288275 0.28703247\nspecies1          0.0006396067 0.07533009\nsegment1          0.6141142602 1.29058241\nsegment2          0.2525582467 0.02255047\nspecies1:segment1 0.0562032225 0.01329672\nspecies1:segment2 0.0165198438 0.05011064",
    "crumbs": [
      "Examples",
      "QK Box 11.1"
    ]
  },
  {
    "objectID": "slides/intro_qk_slides.html#course-overview",
    "href": "slides/intro_qk_slides.html#course-overview",
    "title": "Intro to QK2E",
    "section": "Course overview",
    "text": "Course overview\n1 or 2 presenters per week\nExpectations (everyone)\n\nRead the chapter\nWork through the suggested examples in R"
  },
  {
    "objectID": "slides/intro_qk_slides.html#expectations-presenter",
    "href": "slides/intro_qk_slides.html#expectations-presenter",
    "title": "Intro to QK2E",
    "section": "Expectations (presenter)",
    "text": "Expectations (presenter)\n\nlead a discussion of the reading\nprepare a group / break-out activity\nbe creative and focus on what you want, in the context of the chapter\nupload relevant materials to Canvas / Gdrive"
  },
  {
    "objectID": "slides/intro_qk_slides.html#qk2e",
    "href": "slides/intro_qk_slides.html#qk2e",
    "title": "Intro to QK2E",
    "section": "QK2E",
    "text": "QK2E"
  },
  {
    "objectID": "slides/intro_qk_slides.html#sign-up-to-lead-a-week",
    "href": "slides/intro_qk_slides.html#sign-up-to-lead-a-week",
    "title": "Intro to QK2E",
    "section": "Sign up to lead a week",
    "text": "Sign up to lead a week\nLink to Google sign up is on Canvas\n\n\n\nProgramming challenge (should you choose to accept it):\n\nIf you are going to use slides, create them in Quarto (.qmd) or Rmarkdown (.Rmd)\n\nTotally optional. In any case, please share your ppt slides, R code, other relevant materials with the group in the Gdrive."
  },
  {
    "objectID": "slides/intro_qk_slides.html#prerequisites",
    "href": "slides/intro_qk_slides.html#prerequisites",
    "title": "Intro to QK2E",
    "section": "Prerequisites",
    "text": "Prerequisites\nYou have some familiarity with R computing and statistics.\n\nDo you know what all this means?\n\nx &lt;- c(2, 4, 3, 6)\ny &lt;- c(5, 12, 4, 10, 2)\nt.test(x, y)\n\n\n    Welch Two Sample t-test\n\ndata:  x and y\nt = -1.3761, df = 5.4988, p-value = 0.2222\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -8.031728  2.331728\nsample estimates:\nmean of x mean of y \n     3.75      6.60"
  },
  {
    "objectID": "slides/intro_qk_slides.html#statistics-vs-parameters",
    "href": "slides/intro_qk_slides.html#statistics-vs-parameters",
    "title": "Intro to QK2E",
    "section": "Statistics vs parameters",
    "text": "Statistics vs parameters\n\n\nA statistic is\n\na numerical description of a sample\n\n\n\n\n\nA parameter is\n\na numerical attribute of a population\n\n\n\n\nOften, statistics are used to estimate parameters."
  },
  {
    "objectID": "slides/intro_qk_slides.html#the-two-heads-of-classical-statistics",
    "href": "slides/intro_qk_slides.html#the-two-heads-of-classical-statistics",
    "title": "Intro to QK2E",
    "section": "The two heads of classical statistics",
    "text": "The two heads of classical statistics\n\nestimating parameters, with uncertainty (confidence intervals)\nevaluating (in-)consistency with a particular situation (\\(p\\)-values)\n\n\n\nWhat do these data tell us about the world?\nHow strongly do we believe it?"
  },
  {
    "objectID": "slides/intro_qk_slides.html#lurking-behind-everything",
    "href": "slides/intro_qk_slides.html#lurking-behind-everything",
    "title": "Intro to QK2E",
    "section": "Lurking, behind everything:",
    "text": "Lurking, behind everything:\nis uncertainty, thanks to:\n\n\nactual differences of biological interest (process uncertainty)\n\n\n\n\nuninteresting differences due to sampling variation and measurement error (observation uncertainty)\n\n\n\nHow do we understand uncertainty, concretely and quantitatively?\n\n\n\nwith models."
  },
  {
    "objectID": "slides/intro_qk_slides.html#break",
    "href": "slides/intro_qk_slides.html#break",
    "title": "Intro to QK2E",
    "section": "Break",
    "text": "Break\nStand up! Stretch! Get a drink, use the restroom.\nThen, with a partner(s), go to a board and discuss the following:\n\nWhat is hypothesis testing?\nWhat is a p-value?"
  },
  {
    "objectID": "slides/intro_qk_slides.html#data-story",
    "href": "slides/intro_qk_slides.html#data-story",
    "title": "Intro to QK2E",
    "section": "Data story",
    "text": "Data story\nLow et al (2016) examined the effects of two different anesthetics on aspects of the physiology of the mouse. Twelve mice were anesthetized with isoflurane and eleven mice were anesthetized with alpha chloralose and blood CO2 levels were recorded after 120 minutes. The H0 was that there was no difference between the anesthetics in the mean blood CO2 level. This is an independent comparison because individual mice were only given one of the two anesthetics."
  },
  {
    "objectID": "slides/intro_qk_slides.html#r",
    "href": "slides/intro_qk_slides.html#r",
    "title": "Intro to QK2E",
    "section": "R",
    "text": "R\n\nlibrary(tidyverse)\nlibrary(car)\n\ntheme_set(theme_bw(base_size = 16) + \n            theme(panel.grid.minor = element_blank(), \n                  strip.background = element_blank()))"
  },
  {
    "objectID": "slides/intro_qk_slides.html#the-data",
    "href": "slides/intro_qk_slides.html#the-data",
    "title": "Intro to QK2E",
    "section": "The data",
    "text": "The data\nDescribe what is happening in these lines of code.\n\nlow &lt;- read.csv(\"data/lowco2.csv\")\n\n \n\n\nnames(low)\n\n[1] \"anesth\" \"co2\"   \n\n\n\n\n\n\n\ndim(low)\n\n[1] 23  2\n\n\n\n\n\n\n\nstr(low)\n\n'data.frame':   23 obs. of  2 variables:\n $ anesth: chr  \"iso\" \"iso\" \"iso\" \"iso\" ...\n $ co2   : int  43 35 50 39 56 54 39 51 49 54 ..."
  },
  {
    "objectID": "slides/intro_qk_slides.html#visualize-data",
    "href": "slides/intro_qk_slides.html#visualize-data",
    "title": "Intro to QK2E",
    "section": "Visualize data",
    "text": "Visualize data\n\nlow %&gt;% \n  ggplot(aes(anesth, co2)) + \n  geom_point(alpha = 0.5, size = 5) + \n  labs(x = \"Anesthetic\", y = \"CO2\") + \n  theme_bw(base_size = 24)"
  },
  {
    "objectID": "slides/intro_qk_slides.html#summarizing-data-point-estimates-and-variability",
    "href": "slides/intro_qk_slides.html#summarizing-data-point-estimates-and-variability",
    "title": "Intro to QK2E",
    "section": "Summarizing data: point estimates and variability",
    "text": "Summarizing data: point estimates and variability\n\nlow %&gt;%  \n  group_by(anesth) %&gt;% \n  summarise(n = n(), \n            mean = mean(co2),\n            median = median(co2),\n            sd = sd(co2), \n            variance = var(co2), \n            se = sd / sqrt(n)\n            )\n\n# A tibble: 2 × 7\n  anesth     n  mean median    sd variance    se\n  &lt;chr&gt;  &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n1 ac        11  70.9   64    20.2     408.  6.09\n2 iso       12  50     50.5  11.4     130.  3.29"
  },
  {
    "objectID": "slides/intro_qk_slides.html#confidence-intervals",
    "href": "slides/intro_qk_slides.html#confidence-intervals",
    "title": "Intro to QK2E",
    "section": "Confidence intervals",
    "text": "Confidence intervals\n\nlow %&gt;%  \n  group_by(anesth) %&gt;% \n  summarise(n = n(), \n            mean = mean(co2),\n            sd = sd(co2), \n            se = sd / sqrt(n), \n            CI_upper = mean + se * qt(p = 0.975, df = n-1), \n            CI_lower = mean + se * qt(p = 0.025, df = n-1), \n            CI = se * qt(p = 0.975, df = n-1), \n            upper = mean + CI, \n            lower = mean - CI\n            )\n\n# A tibble: 2 × 10\n  anesth     n  mean    sd    se CI_upper CI_lower    CI upper lower\n  &lt;chr&gt;  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 ac        11  70.9  20.2  6.09     84.5     57.3 13.6   84.5  57.3\n2 iso       12  50    11.4  3.29     57.2     42.8  7.24  57.2  42.8\n\n\nIn a frequentist world, parameters are fixed (but unknowable). Interpret the CI in this context."
  },
  {
    "objectID": "slides/intro_qk_slides.html#hypothesis-testing",
    "href": "slides/intro_qk_slides.html#hypothesis-testing",
    "title": "Intro to QK2E",
    "section": "Hypothesis testing",
    "text": "Hypothesis testing\nA. Construct a null hypothesis (HO)\nB. Derive a test statistic from the data\nC. Compare the obtained test statistic to one derived from values obtained under the HO."
  },
  {
    "objectID": "slides/intro_qk_slides.html#a-p-value-is",
    "href": "slides/intro_qk_slides.html#a-p-value-is",
    "title": "Intro to QK2E",
    "section": "A \\(p\\)-value is",
    "text": "A \\(p\\)-value is\n\n\nthe probability of seeing a result at least as surprising as what was observed in the data, if the null hypothesis is true.\n\n\n\nUsually, this means\n\na result - numerical value of a statistic\nsurprising - big\nnull hypothesis - the model we use to calculate the \\(p\\)-value\n\nwhich can all be defined to suit the situation."
  },
  {
    "objectID": "slides/intro_qk_slides.html#what-does-a-small-p-value-mean",
    "href": "slides/intro_qk_slides.html#what-does-a-small-p-value-mean",
    "title": "Intro to QK2E",
    "section": "What does a small \\(p\\)-value mean?",
    "text": "What does a small \\(p\\)-value mean?\n\nIf the null hypothesis was true, then you’d be really unlikely to see something like what you actually did.\n\n\n\n\nSo, either the “null hypothesis” is not a good description of reality or something surprising happened.\n\n\n\n\nHow useful this is depends on the null hypothesis."
  },
  {
    "objectID": "slides/intro_qk_slides.html#t-test-equal-variances",
    "href": "slides/intro_qk_slides.html#t-test-equal-variances",
    "title": "Intro to QK2E",
    "section": "T-test: equal variances",
    "text": "T-test: equal variances\n\nt.test(co2 ~ anesth, var.equal = TRUE, data = low)\n\n\n    Two Sample t-test\n\ndata:  co2 by anesth\nt = 3.0927, df = 21, p-value = 0.005515\nalternative hypothesis: true difference in means between group ac and group iso is not equal to 0\n95 percent confidence interval:\n  6.849172 34.969010\nsample estimates:\n mean in group ac mean in group iso \n         70.90909          50.00000 \n\n\n\n\nInterpret this result."
  },
  {
    "objectID": "slides/intro_qk_slides.html#your-turn",
    "href": "slides/intro_qk_slides.html#your-turn",
    "title": "Intro to QK2E",
    "section": "Your turn",
    "text": "Your turn\nWith a partner, go to the website, check out the source code, and work though the Chapter 2 notes together."
  },
  {
    "objectID": "slides/maximum_likelihood.html#what-is-likelihood",
    "href": "slides/maximum_likelihood.html#what-is-likelihood",
    "title": "Maximum likelihood",
    "section": "What is likelihood?",
    "text": "What is likelihood?\n\n\n\n\nLikelihood: (relative) measure of how well our observations fit a specific hypothesis / model"
  },
  {
    "objectID": "slides/maximum_likelihood.html#probability-distribution-functions-vs-likelihood-functions",
    "href": "slides/maximum_likelihood.html#probability-distribution-functions-vs-likelihood-functions",
    "title": "Maximum likelihood",
    "section": "Probability distribution functions vs likelihood functions",
    "text": "Probability distribution functions vs likelihood functions\n\nparameters\ndata\nsummation / integration of probabilities"
  },
  {
    "objectID": "slides/maximum_likelihood.html#probability-distribution-function",
    "href": "slides/maximum_likelihood.html#probability-distribution-function",
    "title": "Maximum likelihood",
    "section": "Probability distribution function",
    "text": "Probability distribution function\nknown parameter, variable data\n\nWe know that the oceans cover 70% of the earth’s surface. If we were to take 10 random GPS coordinates of the earth, what’s the probability of getting 5 ocean points?\n\np &lt;- 0.7\n\nWhat probability distribution should we use? Which function in R?\n\n\n\ndbinom(x = 5, size = 10, p = p)\n\n[1] 0.1029193"
  },
  {
    "objectID": "slides/maximum_likelihood.html#section",
    "href": "slides/maximum_likelihood.html#section",
    "title": "Maximum likelihood",
    "section": "",
    "text": "If we repeat this calculation for every possible outcome of ocean points (out of 10), we can visualize the probability distribution, for a known parameter (\\(p\\)).\n\nocean_grid &lt;- seq(0, 10, by = 1)\nocean_p &lt;- dbinom(x = ocean_grid, size = 10, p = p)\n\n\nIf we sum the ocean_p vector, what value should we get?\nReminder: this is a probability mass function.\n\n\n\nsum(ocean_p)\n\n[1] 1"
  },
  {
    "objectID": "slides/maximum_likelihood.html#likelihood-function",
    "href": "slides/maximum_likelihood.html#likelihood-function",
    "title": "Maximum likelihood",
    "section": "Likelihood function",
    "text": "Likelihood function\nunknown parameter, fixed data\n(the usual case)\n\nImagine you are martian observing Earth for the first time and you want to estimate the proportion of ocean on the planet using a bunch of random landings with your space probe.\nOut of 20 trials, you made 12 ocean landings."
  },
  {
    "objectID": "slides/maximum_likelihood.html#section-2",
    "href": "slides/maximum_likelihood.html#section-2",
    "title": "Maximum likelihood",
    "section": "",
    "text": "What is \\(\\text{Pr}(p = 0.7 | \\text{ocean} = 12, \\text{trials} = 20)\\)?\n\n\ndbinom(x = 12, size = 20, prob = 0.7)\n\n[1] 0.1143967\n\n\n\n\n   \nRepeat for many values of \\(p\\):\n\np_grid &lt;- seq(0, 1, by = 0.01)\np_lik &lt;- dbinom(x = 12, size = 20, prob = p_grid)\nplot(p_grid, p_lik, type = \"b\", \n     xlab = \"Proportion of ocean (hypothesis)\", \n     ylab = \"Pr(data | hypothesis)\", \n     main = \"Likelihood function\")"
  },
  {
    "objectID": "slides/maximum_likelihood.html#section-3",
    "href": "slides/maximum_likelihood.html#section-3",
    "title": "Maximum likelihood",
    "section": "",
    "text": "Will these probabilities sum to 1?\n\n\nsum(p_lik)\n\n[1] 4.761905"
  },
  {
    "objectID": "slides/maximum_likelihood.html#section-4",
    "href": "slides/maximum_likelihood.html#section-4",
    "title": "Maximum likelihood",
    "section": "",
    "text": "Which value of \\(p\\) corresponds to the maximum likelihood?\n\ndf &lt;- tibble(p_grid, p_lik)\nsummary(df)\n\n     p_grid         p_lik          \n Min.   :0.00   Min.   :0.000e+00  \n 1st Qu.:0.25   1st Qu.:2.979e-05  \n Median :0.50   Median :8.532e-03  \n Mean   :0.50   Mean   :4.715e-02  \n 3rd Qu.:0.75   3rd Qu.:9.113e-02  \n Max.   :1.00   Max.   :1.797e-01  \n\ndf[which.max(df$p_lik), ]\n\n# A tibble: 1 × 2\n  p_grid p_lik\n   &lt;dbl&gt; &lt;dbl&gt;\n1    0.6 0.180"
  },
  {
    "objectID": "slides/maximum_likelihood.html#maximum-likelihood",
    "href": "slides/maximum_likelihood.html#maximum-likelihood",
    "title": "Maximum likelihood",
    "section": "Maximum likelihood",
    "text": "Maximum likelihood\nFollowing slides are based on a lab by Maurice Goodman."
  },
  {
    "objectID": "slides/maximum_likelihood.html#outline",
    "href": "slides/maximum_likelihood.html#outline",
    "title": "Maximum likelihood",
    "section": "Outline",
    "text": "Outline\n\nsimulate data from a linear model using a known intercept and slope\nwrite a function to fit a linear regression using maximum-likelihood estimation,\ncompare estimated intercept and slope compare to the known values"
  },
  {
    "objectID": "slides/maximum_likelihood.html#linear-regression",
    "href": "slides/maximum_likelihood.html#linear-regression",
    "title": "Maximum likelihood",
    "section": "Linear regression",
    "text": "Linear regression\n\\[\n\\begin{aligned}\nY &= \\alpha + \\beta X + \\epsilon \\\\\n\\epsilon &\\sim \\text{Normal}(0, \\sigma^2)\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\nY &\\sim \\text{Normal}(\\mu, \\sigma^2) \\\\\n\\mu &= \\alpha + \\beta X\n\\end{aligned}\n\\]\nThree parameters:\n\n\\(\\alpha\\), the intercept\n\\(\\beta\\), the slope\n\\(\\sigma\\), the standard deviation of the residuals"
  },
  {
    "objectID": "slides/maximum_likelihood.html#simulate-data",
    "href": "slides/maximum_likelihood.html#simulate-data",
    "title": "Maximum likelihood",
    "section": "Simulate data",
    "text": "Simulate data\n\na &lt;- 1 ## intercept\nb &lt;- 2 ## slope\nsigma &lt;- 1 ## error standard deviation\nn &lt;- 50 ## sample size\n## simulate data\nset.seed(13)\nsim_data &lt;- data.frame(x = runif(n, min = -3, max = 3))\nsim_data$y &lt;- rnorm(n, mean = a + b*sim_data$x, sd = sigma)\n## Look at the data structure\nhead(sim_data)\n\n           x         y\n1  1.2619347  3.673224\n2 -1.5231762 -3.505669\n3 -0.6621933 -2.351430\n4 -2.4516980 -4.960354\n5  2.7723873  5.816631\n6 -2.9344000 -4.877011"
  },
  {
    "objectID": "slides/maximum_likelihood.html#section-6",
    "href": "slides/maximum_likelihood.html#section-6",
    "title": "Maximum likelihood",
    "section": "",
    "text": "Likelihood function:\n\\[\n\\mathcal{L}(\\theta) = f_X(x|\\theta) = \\prod_{i = 1}^n f_X(x_i )\n\\]\n\nLog-likelihood function:\n\\[\nl(\\theta) = \\ln (\\mathcal{L}(\\theta)) = \\sum_{i = 1}^n \\ln [f_X(x_i)]\n\\]\nWe want to maximize the log-likelihood of this function, which is equivalent to minimizing the negative log-likelihood."
  },
  {
    "objectID": "slides/maximum_likelihood.html#section-7",
    "href": "slides/maximum_likelihood.html#section-7",
    "title": "Maximum likelihood",
    "section": "",
    "text": "\\[\n\\begin{aligned}\nY &= \\alpha + \\beta X + \\epsilon \\\\\n\\epsilon &\\sim \\text{Normal}(0, \\sigma^2)\n\\end{aligned}\n\\]\nbecomes:\n\\[\nY \\sim \\text{Normal}(\\underbrace{\\alpha + \\beta X}_{\\text{mean}}, \\underbrace{\\sigma^2}_{\\text{variance}})\n\\]\nWe can express the negative log-likelihood function in R as:\n-sum(dnorm(y, mean = ..., sd = ..., log = TRUE))"
  },
  {
    "objectID": "slides/maximum_likelihood.html#r-function",
    "href": "slides/maximum_likelihood.html#r-function",
    "title": "Maximum likelihood",
    "section": "R function",
    "text": "R function\n\n## negative log-likelihood of y-values given \n## intercept, slope, and error sd\n## we pass the parameters as a vector in the order a, b, sigma\nlm_nll &lt;- function(par, x, y) {\n  a &lt;- par[\"a\"]; b &lt;- par[\"b\"]; sigma &lt;- exp(par[\"log_sigma\"])\n  -sum(dnorm(y, mean = a + b*x, sd = sigma, log = TRUE))\n}"
  },
  {
    "objectID": "slides/maximum_likelihood.html#optimize-the-function",
    "href": "slides/maximum_likelihood.html#optimize-the-function",
    "title": "Maximum likelihood",
    "section": "optimize the function",
    "text": "optimize the function\n\nfit &lt;- optim(\n  par = c(a = 0, b = 0, log_sigma = 1), \n  fn = lm_nll, \n  x = sim_data$x, \n  y = sim_data$y, \n  hessian = TRUE\n)"
  },
  {
    "objectID": "slides/maximum_likelihood.html#section-8",
    "href": "slides/maximum_likelihood.html#section-8",
    "title": "Maximum likelihood",
    "section": "",
    "text": "fit\n\n$par\n         a          b  log_sigma \n 0.8269008  2.0182844 -0.1061237 \n\n$value\n[1] 65.64576\n\n$counts\nfunction gradient \n     140       NA \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n$hessian\n                    a             b     log_sigma\na         61.82268539   4.243509988  -0.037625973\nb          4.24350999 176.478986962  -0.002715066\nlog_sigma -0.03762597  -0.002715066 100.020193344\n\n# Compare with lm\ncoef(lm(y ~ x, data = sim_data))\n\n(Intercept)           x \n  0.8265966   2.0182841"
  },
  {
    "objectID": "slides/maximum_likelihood.html#section-9",
    "href": "slides/maximum_likelihood.html#section-9",
    "title": "Maximum likelihood",
    "section": "",
    "text": "sim_data %&gt;% ggplot(aes(x, y)) + geom_point() + \n  geom_abline(intercept = a, slope = b, \n              color = \"red\") + # truth\n  geom_abline(intercept = fit$par[1], slope = fit$par[2], \n              color = \"blue\") # fit"
  },
  {
    "objectID": "slides/maximum_likelihood.html#likelihood-slice",
    "href": "slides/maximum_likelihood.html#likelihood-slice",
    "title": "Maximum likelihood",
    "section": "Likelihood slice",
    "text": "Likelihood slice\n\na_seq &lt;- seq(-2, 4, length.out = 100)\nb_seq &lt;- seq(-1, 5, length.out = 100)\n\n### data frame containing all combinations of the intercept and slope\npar_grid &lt;- expand.grid(\"a\" = a_seq, \"b\" = b_seq)\n\n### loop over data frame, store negative log likelihood\nfor (i in 1:nrow(par_grid)) {\n  par_grid$loglik[i] &lt;- lm_nll(\n    c(a = par_grid$a[i], b = par_grid$b[i], fit$par[3]), \n    x = sim_data$x, \n    y = sim_data$y\n  )\n}"
  },
  {
    "objectID": "slides/maximum_likelihood.html#likelihood-profile-for-the-slope",
    "href": "slides/maximum_likelihood.html#likelihood-profile-for-the-slope",
    "title": "Maximum likelihood",
    "section": "Likelihood profile for the slope",
    "text": "Likelihood profile for the slope\nModify our lm_log_lik function so that the slope is a separate argument.\n\nslope_proflik &lt;- function(pars, b, x, y) {\n  a &lt;- pars[\"a\"]; sigma &lt;- exp(pars[\"log_sigma\"]) ## only two pars in this function\n  -sum(dnorm(y, mean = a + b*x, sd = sigma, log = TRUE))\n}\n\nb_profile &lt;- data.frame(\n  b = seq(-1, 5, length.out = 100)  ## values of slope to loop over\n)"
  },
  {
    "objectID": "slides/maximum_likelihood.html#section-11",
    "href": "slides/maximum_likelihood.html#section-11",
    "title": "Maximum likelihood",
    "section": "",
    "text": "We’ll pass this to optim to just find the values of the intercept and sd\n\n## loop over values of the slope, store negative log-likelihood\nfor (i in 1:nrow(b_profile)) {\n  b_fit &lt;- optim(\n    par = c(a = 0, log_sigma = log(1)), # initial values for intercept and sd\n    fn = slope_proflik,  # objective function\n    b = b_profile$b[i], # fixed slope value\n    x = sim_data$x,   # x values\n    y = sim_data$y   # y values\n  )\n  b_profile$nll[i] &lt;- b_fit$value\n}"
  },
  {
    "objectID": "slides/maximum_likelihood.html#section-12",
    "href": "slides/maximum_likelihood.html#section-12",
    "title": "Maximum likelihood",
    "section": "",
    "text": "## plot slope profile\nb_profile %&gt;% \n  ggplot(aes(b, nll)) + \n  geom_line() + \n  labs(x = expression(beta), y = \"negative log-likelihood\")"
  },
  {
    "objectID": "slides/maximum_likelihood.html#standard-errors-fisher-information",
    "href": "slides/maximum_likelihood.html#standard-errors-fisher-information",
    "title": "Maximum likelihood",
    "section": "Standard errors: Fisher information",
    "text": "Standard errors: Fisher information\n\\[\n\\begin{aligned}\n\\mathcal{J}(\\hat{\\theta}) &= - \\frac{\\partial^2}{\\partial \\theta^2} \\ln [\\mathcal{L}(\\hat{\\theta})] \\\\\n\\text{Var}(\\hat{\\theta}) &= 1 / \\mathcal{J}(\\hat{\\theta})\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/maximum_likelihood.html#section-13",
    "href": "slides/maximum_likelihood.html#section-13",
    "title": "Maximum likelihood",
    "section": "",
    "text": "var_cov &lt;- solve(fit$hessian)\nvar_cov\n\n                      a             b    log_sigma\na          1.620204e-02 -3.895846e-04 6.084368e-06\nb         -3.895846e-04  5.675765e-03 7.514228e-09\nlog_sigma  6.084368e-06  7.514228e-09 9.997983e-03\n\n\nThe variances are on the diagonal, and we can get the standard errors by taking their square root:"
  },
  {
    "objectID": "slides/maximum_likelihood.html#section-14",
    "href": "slides/maximum_likelihood.html#section-14",
    "title": "Maximum likelihood",
    "section": "",
    "text": "fisher_se &lt;- sqrt(diag(var_cov))\nfisher_se\n\n         a          b  log_sigma \n0.12728722 0.07533767 0.09998992 \n\n\n\n\nLet’s compare these to the standard errors reported by lm:\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n0.8265966\n0.1299250\n6.362106\n1e-07\n\n\nx\n2.0182841\n0.0768989\n26.245943\n0e+00"
  },
  {
    "objectID": "slides/maximum_likelihood.html#confidence-intervals-normal-approximation",
    "href": "slides/maximum_likelihood.html#confidence-intervals-normal-approximation",
    "title": "Maximum likelihood",
    "section": "Confidence intervals: normal approximation",
    "text": "Confidence intervals: normal approximation\n\\[\n\\text{CI} = (\\hat{\\theta} - \\phi^{-1}(\\gamma / 2) \\times \\text{SE}(\\hat{\\theta}), \\; \\hat{\\theta} + \\phi^{-1}(1 - \\gamma / 2) \\times \\text{SE}(\\hat{\\theta}))\n\\]\nWhere \\(\\phi^{-1}\\) is the inverse cumulative distribution function of the standard Normal distribution."
  },
  {
    "objectID": "slides/maximum_likelihood.html#section-15",
    "href": "slides/maximum_likelihood.html#section-15",
    "title": "Maximum likelihood",
    "section": "",
    "text": "Equivalently:\n\\[\n\\text{CI} = (\\phi^{-1}_{\\hat{\\theta}, \\text{SE}(\\theta)}(\\gamma / 2), \\; \\phi^{-1}_{\\hat{\\theta}, \\text{SE}(\\theta)} (1-\\gamma/2))\n\\]\nWhere \\(\\phi^{-1}_{\\hat{\\theta}, \\text{SE}(\\theta)}\\) is the inverse cumulative distribution function for a Normal distribution with mean \\(\\hat{\\theta}\\) and standard deviation \\(\\text{SE}(\\theta)\\)."
  },
  {
    "objectID": "slides/maximum_likelihood.html#section-16",
    "href": "slides/maximum_likelihood.html#section-16",
    "title": "Maximum likelihood",
    "section": "",
    "text": "The Normal inverse CDF is returned by the qnorm function. We can obtain a confidence interval for our intercept \\(\\alpha\\) and slope \\(\\beta\\):\n\ngamma &lt;- 0.05 ## 95% confidence interval\nalpha_CI &lt;- qnorm(c(gamma/2, 1 - gamma/2), mean = fit$par[1], \n                  sd = fisher_se[1])\nbeta_CI &lt;- qnorm(c(gamma/2, 1 - gamma/2), mean = fit$par[2], \n                 sd = fisher_se[2])\ncbind(alpha_CI, beta_CI)\n\n      alpha_CI  beta_CI\n[1,] 0.5774225 1.870625\n[2,] 1.0763792 2.165944"
  },
  {
    "objectID": "slides/maximum_likelihood.html#making-it-a-function",
    "href": "slides/maximum_likelihood.html#making-it-a-function",
    "title": "Maximum likelihood",
    "section": "Making it a function",
    "text": "Making it a function\n\nlm_fit &lt;- function(x, y, SE = \"fisher\", init = c(\"a\" = 0, \"b\" = 0, \"log_sigma\" = 0), n_boot = 1000) {\n\n  if (!(SE %in% c(\"bootstrap\", \"fisher\"))) stop(\"SE options are bootstrap or fisher\")\n  if (length(x) != length(y)) stop(\"x and y lengths differ\")\n\n  ## Maximum-likelihood estimate\n  MLE &lt;- optim(\n    par = init, # initial values\n    fn = lm_nll,  # our negative log-likelihood function\n    x = x,   # x values\n    y = y,   # y values\n    hessian = (SE == \"fisher\") # only return Hessian if Fisher information used\n  )\n\n  ## Standard error using either fisher information or bootstrapping\n  if (SE == \"fisher\") {\n    var_cov &lt;- solve(MLE$hessian)\n    fit_se &lt;- sqrt(diag(var_cov))\n  } else {\n    n_obs &lt;- length(x) ## number of observations\n\n    ## initialize an empty matrix to store parameter estimates for each sample\n    par_samples &lt;- matrix(nrow = n_boot, ncol = 3)\n\n    ## recalculate MLE for each sample\n    for (i in 1:n_boot) {\n      samp &lt;- sample(1:n_obs, size = n_obs, replace = TRUE) # sampled observations\n      new_x &lt;- x[samp] # subset x to sampled observations\n      new_y &lt;- y[samp] # subset y to sampled observations\n\n      sample_fit &lt;- optim(\n        par = init, # initial values\n        fn = lm_nll, # log likelihood function\n        x = new_x, # sampled x-values\n        y = new_y # sampled y-values\n      )\n  \n    par_samples[i,] &lt;- sample_fit$par # store sample parameters\n  }\n  fit_se &lt;- apply(par_samples, MARGIN = 2, FUN = sd) # calculate column SDs\n }\n\n ## nicely format output\n data.frame(\n   coef = c(\"intercept\", \"slope\"), \n   estimate = MLE$par[1:2], \n   SE = fit_se[1:2]\n )\n\n}"
  },
  {
    "objectID": "slides/maximum_likelihood.html#lab-exercise-1",
    "href": "slides/maximum_likelihood.html#lab-exercise-1",
    "title": "Maximum likelihood",
    "section": "Lab exercise 1",
    "text": "Lab exercise 1\n\na &lt;- 1 ## intercept\nb &lt;- 2 ## slope\nsigma &lt;- 1 ## error standard deviation\nn &lt;- 50 ## sample size\n\n## simulate data\nsim_data &lt;- tibble(\n  x = runif(n, min = -3, max = 3), ## x values\n  y = rnorm(n, mean = a + b*x, sd = sigma) ## regression equation\n)\n\n## plot data\nsim_data %&gt;% ggplot(aes(x, y)) + geom_point() + geom_smooth(method = \"lm\")\n\n## run regression\nlm_fit(sim_data$x, sim_data$y, SE = \"fisher\")\n\n       coef  estimate         SE\na intercept 0.9358099 0.13386046\nb     slope 2.0485483 0.07308945"
  }
]